# QLoRA (Quantized Low-Rank Adaptation) Configuration
# =====================================================
# Configuration for QLoRA training following methodologies from:
# - Dettmers et al. (2023): "QLoRA: Efficient Finetuning of Quantized LLMs"
# - Hu et al. (2022): "LoRA: Low-Rank Adaptation of Large Language Models"
# - Dettmers et al. (2022): "8-bit Optimizers via Block-wise Quantization"
#
# Mathematical Foundation:
# QLoRA combines 4-bit quantization with LoRA adapters:
# W = W_0^{NF4} + BA where W_0 is quantized to 4-bit NormalFloat
# B ∈ R^{d×r}, A ∈ R^{r×k}, with r << min(d,k)
#
# Memory reduction: O(dk) → O(4dk/32 + r(d+k))
# Approximately 65% memory reduction for large models
#
# Author: Võ Hải Dũng
# License: MIT

name: qlora_training
type: efficient_qlora
description: "4-bit quantized training with LoRA adapters for extreme memory efficiency"

# Base configuration inheritance
extends: lora_peft.yaml

# QLoRA Specific Configuration
qlora:
  enabled: true
  
  # Quantization configuration
  quantization:
    # Quantization bit width
    bits: 4  # 4-bit quantization
    
    # Quantization type
    quant_type: nf4  # Options: nf4, fp4, int4
    
    # NormalFloat4 (NF4) configuration
    nf4:
      # Double quantization (quantize quantization constants)
      double_quant: true
      double_quant_bits: 8  # Quantize constants to 8-bit
      
      # Compute dtype for dequantization
      compute_dtype: float16  # Options: float16, bfloat16, float32
      
      # Block size for quantization
      block_size: 64  # Quantization block size
      
      # Nested quantization
      nested_quant: true
      nested_quant_bits: 8
    
    # FP4 configuration (alternative to NF4)
    fp4:
      enabled: false
      e2m1: true  # 2-bit exponent, 1-bit mantissa
      
    # INT4 configuration
    int4:
      enabled: false
      symmetric: false  # Asymmetric quantization
      per_channel: true
  
  # Storage format
  storage:
    # Model storage
    store_in_4bit: true
    
    # Optimizer state storage
    optimizer_bits: 32  # Store optimizer states in 32-bit
    
    # Gradient storage
    gradient_bits: 16
    
    # Activation storage
    activation_bits: 16
    
    # Memory mapping
    memory_map: true
    max_memory: null  # Auto-detect available memory

# LoRA Configuration (inherited and modified)
lora:
  # Rank configuration
  r: 64  # Higher rank for QLoRA (compensates for quantization)
  
  # Scaling factor
  alpha: 16  # LoRA alpha
  scaling: 0.25  # alpha/r scaling
  
  # Dropout
  dropout: 0.05  # Lower dropout for QLoRA
  
  # Target modules
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    
  # Module-specific ranks
  module_ranks:
    q_proj: 64
    k_proj: 64
    v_proj: 64
    o_proj: 32
    gate_proj: 32
    up_proj: 32
    down_proj: 32
  
  # Initialization
  init_lora_weights: true
  lora_init_method: kaiming  # Options: kaiming, xavier, normal, zeros

# Paged Optimizers
paged_optimizer:
  enabled: true
  
  # Paged AdamW configuration
  optimizer: paged_adamw_32bit  # Options: paged_adamw_32bit, paged_adamw_8bit, paged_lion_32bit
  
  # Page size
  page_size: 512  # Memory page size
  
  # CPU offloading
  cpu_offload: true
  offload_threshold: 0.9  # Offload when GPU memory > 90%
  
  # Optimizer hyperparameters
  hyperparameters:
    lr: 2e-4  # Higher LR for LoRA parameters
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 0.01
    
  # 8-bit optimizer configuration
  eight_bit:
    enabled: false
    block_wise: true
    percentile_clipping: 100
    
  # Lion optimizer (more memory efficient)
  lion:
    enabled: false
    beta1: 0.9
    beta2: 0.99

# Gradient Checkpointing
gradient_checkpointing:
  enabled: true
  
  # Checkpointing strategy
  strategy: selective  # Options: full, selective, custom
  
  # Selective checkpointing
  selective:
    checkpoint_ratio: 0.5  # Checkpoint 50% of layers
    checkpoint_method: uniform  # Options: uniform, importance, memory
    
  # Custom checkpointing
  custom:
    checkpoint_layers: [4, 8, 12, 16, 20, 24, 28]
    
  # Memory-computation tradeoff
  use_reentrant: false  # Non-reentrant for better memory
  
  # Gradient accumulation with checkpointing
  gradient_accumulation_dtype: float32

# Mixed Precision Training
mixed_precision:
  enabled: true
  
  # Precision configuration
  param_dtype: float16  # Options: float16, bfloat16
  
  # Loss scaling
  loss_scale: dynamic
  initial_loss_scale: 2048
  
  # Gradient scaling
  grad_scaling: true
  
  # FP16 optimizer state
  fp16_optimizer_states: false  # Keep optimizer states in FP32
  
  # BFloat16 configuration (for A100/H100)
  bf16:
    enabled: false
    full_bf16: false  # Full BF16 training

# Memory Optimization
memory_optimization:
  # Flash Attention
  flash_attention:
    enabled: true
    version: 2  # Flash Attention 2
    causal: false  # Non-causal for classification
    window_size: null  # Full attention
    
  # Memory efficient attention
  memory_efficient_attention:
    enabled: false  # Use if Flash Attention unavailable
    chunk_size: 512
    
  # Activation checkpointing
  activation_checkpointing:
    enabled: true
    preserve_rng_state: true
    
  # CPU offloading
  offload_to_cpu:
    enabled: true
    offload_params: false  # Keep params on GPU
    offload_buffers: true
    offload_optimizer: true
    
  # Memory monitoring
  monitor_memory: true
  memory_logging_step: 100
  max_memory_reserved: 0.95  # Max 95% GPU memory

# Training Configuration
training:
  # Batch size (can be larger with QLoRA)
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  
  # Gradient accumulation
  gradient_accumulation_steps: 4
  
  # Learning rate
  learning_rate: 2e-4  # Higher for LoRA parameters
  
  # Scheduler
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  
  # Training epochs
  num_train_epochs: 3
  
  # Max steps (alternative to epochs)
  max_steps: -1
  
  # Optimization
  optim: paged_adamw_32bit
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  
  # Weight decay
  weight_decay: 0.01
  
  # Gradient clipping
  max_grad_norm: 0.3
  
  # Training precision
  fp16: true
  bf16: false
  
  # Logging
  logging_steps: 10
  logging_first_step: true
  
  # Evaluation
  evaluation_strategy: steps
  eval_steps: 100
  
  # Saving
  save_strategy: steps
  save_steps: 500
  save_total_limit: 3
  
  # Early stopping
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false

# Model Loading
model_loading:
  # Load in 4bit
  load_in_4bit: true
  
  # Device map
  device_map: auto  # Automatic device mapping
  
  # Max memory per GPU
  max_memory: null  # Auto-detect
  
  # Model revision
  revision: main
  
  # Trust remote code
  trust_remote_code: false
  
  # Torch dtype
  torch_dtype: auto  # Auto-detect optimal dtype

# Data Configuration
data:
  # Dataset configuration
  max_seq_length: 512
  
  # Padding
  pad_to_max_length: false
  padding: longest
  
  # Truncation
  truncation: true
  
  # Label smoothing
  label_smoothing_factor: 0.0
  
  # Data collator
  data_collator: default
  
  # Preprocessing
  preprocessing_num_workers: 4
  
  # Caching
  keep_in_memory: false
  cache_dir: ./cache/qlora

# Evaluation Configuration
evaluation:
  # Metrics
  compute_metrics: true
  
  # Evaluation batch size
  per_device_eval_batch_size: 8
  
  # Evaluation accumulation
  eval_accumulation_steps: null
  
  # Prediction loss only
  prediction_loss_only: false
  
  # Generate during eval
  generate_during_eval: false
  generation_max_length: 128
  generation_num_beams: 1

# Inference Optimization
inference:
  # Merge LoRA weights
  merge_and_unload: true
  
  # Save merged model
  save_merged: true
  merged_model_path: ./models/qlora_merged
  
  # Quantization after training
  post_training_quantization:
    enabled: false
    bits: 8
    method: dynamic  # Options: dynamic, static

# Hardware Requirements
hardware_requirements:
  # Minimum GPU memory (GB)
  min_gpu_memory: 16  # Can train 7B model on 16GB
  
  # Recommended GPU memory
  recommended_gpu_memory: 24
  
  # Supported GPUs
  supported_gpus:
    - V100
    - A100
    - A6000
    - RTX_3090
    - RTX_4090
    
  # CUDA version
  min_cuda_version: "11.8"
  
  # PyTorch version
  min_pytorch_version: "2.0.0"

# Debugging and Monitoring
debugging:
  # Gradient monitoring
  log_gradients: false
  gradient_histogram_freq: 100
  
  # Weight monitoring
  log_weights: false
  weight_histogram_freq: 100
  
  # Memory profiling
  profile_memory: true
  memory_snapshot_freq: 500
  
  # Quantization analysis
  analyze_quantization_error: true
  log_quantization_stats: true
  
  # LoRA analysis
  analyze_lora_ranks: true
  log_effective_rank: true

# Experimental Features
experimental:
  # Adaptive rank
  adaptive_rank:
    enabled: false
    min_rank: 8
    max_rank: 128
    
  # Mixed quantization
  mixed_quantization:
    enabled: false
    layer_bits: [4, 4, 8, 8, 4, 4]  # Per-layer quantization
    
  # Quantization-aware training
  qat:
    enabled: false
    start_epoch: 2
    
  # Sparse LoRA
  sparse_lora:
    enabled: false
    sparsity: 0.5

# Notes
notes: |
  QLoRA Configuration for AG News Text Classification:
  
  Key features:
  - 4-bit NormalFloat quantization with double quantization
  - LoRA rank 64 for quality preservation
  - Paged optimizers for memory efficiency
  - Flash Attention 2 for speed
  - Gradient checkpointing for memory savings
  
  Memory savings:
  - 7B model: 28GB → 6GB VRAM
  - 13B model: 52GB → 10GB VRAM
  - 30B model: 120GB → 20GB VRAM
  
  Performance expectations:
  - 95% of full fine-tuning performance
  - 65% memory reduction
  - 2-3x slower than full fine-tuning
  - Enables large model training on consumer GPUs
  
  Best practices:
  - Use higher LoRA rank (64-128) to compensate for quantization
  - Enable gradient checkpointing for additional memory savings
  - Use paged optimizers to handle optimizer states
  - Monitor quantization error during training
  
  Common issues:
  - NaN losses: Reduce learning rate or disable mixed precision
  - OOM errors: Reduce batch size or enable more aggressive checkpointing
  - Slow training: Disable gradient checkpointing if memory allows
  - Quality degradation: Increase LoRA rank or use higher bit quantization
