# Base Training Configuration
# Standard training setup for AG News models

name: base_training
type: standard
description: "Base training configuration with best practices"

# General training settings
general:
  seed: 42
  deterministic: true
  num_train_epochs: 10
  max_steps: -1  # -1 means use num_train_epochs
  early_stopping: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  
  # Gradient accumulation
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  
  # Mixed precision
  fp16: true
  fp16_opt_level: O1
  fp16_backend: auto
  bf16: false  # For A100 GPUs
  
  # Distributed training
  local_rank: -1
  ddp_backend: nccl
  ddp_find_unused_parameters: false
  
  # DeepSpeed
  deepspeed: null  # Path to DeepSpeed config

# Batch configuration
batch:
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 64
  
  # Dynamic batching
  auto_find_batch_size: false
  
  # DataLoader settings
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  dataloader_drop_last: false
  dataloader_persistent_workers: true

# Optimizer configuration
optimizer:
  name: adamw
  learning_rate: 2e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # Layer-wise learning rate decay
  layer_decay: 0.95
  
  # Bias correction
  correct_bias: true

# Learning rate scheduler
scheduler:
  name: cosine
  warmup_ratio: 0.1
  warmup_steps: 0  # If > 0, overrides warmup_ratio
  
  # Scheduler-specific params
  num_cycles: 0.5  # For cosine
  power: 1.0  # For polynomial
  
  # Learning rate range
  lr_end: 1e-7

# Loss configuration
loss:
  name: cross_entropy
  label_smoothing_factor: 0.1
  
  # Class weights for imbalanced data
  use_class_weights: false
  class_weights: auto  # Or specify [1.0, 1.0, 1.0, 1.0]
  
  # Focal loss parameters
  focal_loss_gamma: 2.0
  focal_loss_alpha: 0.25

# Regularization
regularization:
  dropout_rate: 0.1
  attention_dropout_rate: 0.1
  hidden_dropout_rate: 0.1
  
  # Weight decay
  weight_decay: 0.01
  weight_decay_exclude: ["bias", "LayerNorm", "layer_norm"]
  
  # Stochastic depth
  stochastic_depth_rate: 0.0
  
  # Mixup/CutMix
  mixup_alpha: 0.0
  cutmix_alpha: 0.0

# Evaluation settings
evaluation:
  evaluation_strategy: steps  # Options: no, steps, epoch
  eval_steps: 500
  eval_delay: 0
  eval_accumulation_steps: null
  
  # Metrics
  metric_for_best_model: f1_macro
  greater_is_better: true
  
  # Prediction settings
  prediction_loss_only: false
  
  # Evaluation during training
  do_eval: true
  do_predict: true

# Checkpointing
checkpointing:
  save_strategy: steps  # Options: no, steps, epoch
  save_steps: 500
  save_total_limit: 3
  save_on_each_node: false
  
  # Best model
  load_best_model_at_end: true
  save_best_only: true
  
  # Resume training
  resume_from_checkpoint: null  # Path to checkpoint
  
  # Checkpoint management
  save_safetensors: true
  save_optimizer: true
  save_scheduler: true

# Logging and monitoring
logging:
  logging_dir: ./outputs/logs
  logging_strategy: steps
  logging_first_step: true
  logging_steps: 50
  logging_nan_inf_filter: true
  
  # Experiment tracking
  report_to: ["tensorboard", "wandb"]
  
  # WandB settings
  wandb_project: ag-news-training
  wandb_entity: null
  wandb_run_name: null
  wandb_tags: ["training", "standard"]
  
  # TensorBoard
  tensorboard_dir: ./outputs/logs/tensorboard
  
  # Metrics logging
  log_level: info
  log_level_replica: warning
  
  # System metrics
  log_gpu_memory: true
  log_learning_rate: true

# Data configuration
data:
  # Data paths
  train_file: ./data/processed/train.csv
  validation_file: ./data/processed/validation.csv
  test_file: ./data/processed/test.csv
  
  # Processing
  max_seq_length: 512
  preprocessing_num_workers: 4
  overwrite_cache: false
  
  # Sampling
  max_train_samples: null
  max_eval_samples: null
  max_predict_samples: null
  
  # Data collator
  pad_to_max_length: true
  pad_to_multiple_of: 8
  label_pad_token_id: -100

# Advanced training strategies
advanced:
  # Gradient accumulation
  gradient_accumulation_steps: 1
  eval_accumulation_steps: null
  
  # Training resume
  ignore_data_skip: false
  
  # Push to hub
  push_to_hub: false
  hub_model_id: null
  hub_strategy: every_save
  hub_token: null
  
  # Ray integration
  ray_scope: last
  
  # Skip memory metrics
  skip_memory_metrics: true
  
  # Torch compile
  torch_compile: false
  torch_compile_backend: inductor
  torch_compile_mode: default

# Hardware settings
hardware:
  no_cuda: false
  use_mps_device: false  # For Apple Silicon
  use_cpu: false
  
  # Device placement
  device: cuda
  n_gpu: 1
  
  # Memory management
  per_device_train_batch_size: 32
  auto_find_batch_size: false
  full_determinism: false
  
  # Performance
  tf32: true  # For Ampere GPUs
  dataloader_prefetch_factor: 2

# Callbacks
callbacks:
  - name: EarlyStoppingCallback
    patience: 3
    threshold: 0.001
    
  - name: ModelCheckpointCallback
    save_best: true
    
  - name: TensorBoardCallback
    log_dir: ./outputs/logs/tensorboard
    
  - name: WandbCallback
    project: ag-news
    
  - name: ProgressBarCallback
    
  - name: PrinterCallback

# Environment variables
env_vars:
  TOKENIZERS_PARALLELISM: false
  CUDA_LAUNCH_BLOCKING: 0
  WANDB_DISABLED: false
  HF_DATASETS_CACHE: ./.cache/datasets

# Notes
notes: |
  Base training configuration optimized for:
  - Single GPU training (can scale to multi-GPU)
  - Mixed precision training (FP16)
  - Gradient accumulation for larger effective batch sizes
  - Early stopping to prevent overfitting
  - Comprehensive logging and monitoring
  
  Recommended adjustments:
  - Increase batch size for larger GPUs
  - Enable gradient checkpointing for large models
  - Use BF16 for A100 GPUs
  - Adjust learning rate based on batch size
