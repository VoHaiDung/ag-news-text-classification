# Knowledge Distillation Configuration
# =====================================
# Configuration for knowledge distillation following methodologies from:
# - Hinton et al. (2015): "Distilling the Knowledge in a Neural Network"
# - Romero et al. (2015): "FitNets: Hints for Thin Deep Nets"
# - Zagoruyko & Komodakis (2017): "Paying More Attention to Attention"
# - Furlanello et al. (2018): "Born-Again Neural Networks"
# - Xie et al. (2020): "Self-training with Noisy Student improves ImageNet classification"
#
# Mathematical Foundation:
# Distillation loss: L_KD = α*L_CE(y, p_student) + (1-α)*T²*KL(p_teacher/T, p_student/T)
# where T is temperature, α is balance parameter, KL is Kullback-Leibler divergence
#
# Feature matching: L_feature = ||f_student - f_teacher||²
# Attention transfer: L_AT = ||A_student - A_teacher||_F
#
# Author: Võ Hải Dũng
# License: MIT

name: knowledge_distillation
type: advanced_distillation
description: "Knowledge distillation from ensemble teacher to efficient student"

# Base configuration inheritance
inherit: training/standard/base_training.yaml

# Teacher Model Configuration
teacher:
  # Single teacher model configuration
  single_teacher:
    enabled: false
    model_path: "outputs/models/fine_tuned/deberta_v3_xlarge_best.pt"
    model_config: "configs/models/single/deberta_v3_xlarge.yaml"
    
  # Ensemble teacher configuration (superior performance)
  ensemble_teacher:
    enabled: true
    ensemble_config: "configs/models/ensemble/voting_ensemble.yaml"
    models:
      - path: "outputs/models/fine_tuned/deberta_v3_xlarge_best.pt"
        weight: 0.4  # DeBERTa has highest individual performance
      - path: "outputs/models/fine_tuned/roberta_large_best.pt"
        weight: 0.3  # RoBERTa for robustness
      - path: "outputs/models/fine_tuned/xlnet_large_best.pt"
        weight: 0.3  # XLNet for permutation invariance
    
  # Teacher inference settings
  teacher_batch_size: 32  # Smaller batch for large teacher
  use_teacher_cache: true  # Cache predictions for efficiency
  cache_dir: "./.cache/teacher_predictions"
  
  # Teacher augmentation for robust distillation
  teacher_augmentation:
    enabled: true
    num_augmentations: 3  # Multiple teacher predictions
    temperature_range: [1.0, 5.0]  # Temperature augmentation

# Student Model Configuration
student:
  model_name: "distilroberta-base"  # Efficient student architecture
  model_config: "configs/models/efficient/distilroberta_base.yaml"
  
  # Student architecture specifications
  hidden_size: 768  # Reduced hidden dimension
  num_hidden_layers: 6  # Fewer layers than teacher
  num_attention_heads: 12  # Maintain attention heads
  intermediate_size: 3072  # Smaller FFN
  
  # Initialization from teacher weights
  initialize_from_teacher: true
  initialization_strategy: "layer_mapping"  # Options: layer_mapping, truncation, interpolation
  layer_mapping:  # Map student layers to teacher layers
    0: 0   # First layer
    1: 4   # Skip 3 teacher layers
    2: 8   # Skip 3 teacher layers
    3: 12  # Middle teacher layer
    4: 16  # Skip 3 teacher layers
    5: 20  # Near-final teacher layer

# Distillation Configuration
distillation:
  # Loss balancing parameters
  alpha: 0.7  # Weight for distillation loss (0.7 distillation, 0.3 hard targets)
  temperature: 4.0  # Softening factor for probability distributions
  
  # Loss components
  use_soft_targets: true  # Distill soft probabilities
  use_hard_targets: true  # Also train on ground truth
  use_feature_distillation: true  # Match intermediate representations
  use_attention_distillation: true  # Transfer attention patterns
  
  # Feature distillation configuration
  feature_distillation:
    layers: [2, 4, 5]  # Student layers to distill
    teacher_layers: [8, 16, 23]  # Corresponding teacher layers
    feature_loss_weight: 0.1  # Feature matching weight
    use_mse_loss: true  # Mean squared error for features
    normalize_features: true  # L2 normalization before matching
    
  # Attention distillation configuration
  attention_distillation:
    attention_loss_weight: 0.1  # Attention transfer weight
    use_all_heads: false  # Selective head distillation
    selected_heads: [0, 5, 11]  # Important attention heads
    
  # Relational knowledge distillation (RKD)
  relational_kd:
    enabled: true
    angle_loss_weight: 0.05  # Angle-wise relation
    distance_loss_weight: 0.05  # Distance-wise relation
    
  # Progressive distillation strategy
  progressive_distillation:
    enabled: true
    stages: 3  # Number of progressive stages
    stage_epochs: [3, 3, 4]  # Epochs per stage
    temperature_schedule: [6.0, 4.0, 2.0]  # Decreasing temperature
    alpha_schedule: [0.9, 0.7, 0.5]  # Decreasing distillation weight

# Training Configuration
training:
  # Training duration
  num_train_epochs: 15  # Extended training for distillation
  
  # Learning rate settings
  learning_rate: 5e-5  # Higher LR for student
  warmup_ratio: 0.1  # 10% warmup steps
  
  # Batch configuration
  per_device_train_batch_size: 64  # Larger batch for student
  gradient_accumulation_steps: 1
  
  # Optimizer configuration
  optimizer: adamw
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Learning rate schedule
  lr_scheduler_type: cosine  # Cosine annealing
  
  # Mixed precision training
  fp16: true  # Enable mixed precision
  fp16_opt_level: O2  # Optimization level

# Data Configuration for Distillation
data:
  # Original training data
  use_original_data: true
  original_data_weight: 0.5  # 50% original data
  
  # Teacher-augmented data
  use_teacher_augmented: true
  teacher_augmented_weight: 0.3  # 30% augmented data
  
  # Pseudo-labeled data
  use_pseudo_labeled: true
  pseudo_labeled_weight: 0.2  # 20% pseudo labels
  pseudo_label_threshold: 0.95  # High confidence threshold
  
  # Data mixing strategy
  mixing_strategy: "weighted"  # Options: weighted, interleave, curriculum

# Advanced Distillation Techniques
advanced_techniques:
  # Born-Again Networks (BAN)
  born_again:
    enabled: false
    generations: 3  # Number of self-distillation iterations
    
  # Self-distillation
  self_distillation:
    enabled: true
    num_iterations: 2  # Self-teaching iterations
    
  # Online distillation
  online_distillation:
    enabled: false
    co_teaching: true  # Mutual learning
    
  # Noisy Student training
  noisy_student:
    enabled: true
    noise_type: "dropout"  # Noise injection method
    noise_level: 0.1  # Noise magnitude
    iterations: 2  # Noisy student iterations
    
  # Adversarial distillation
  adversarial_distillation:
    enabled: false
    adversarial_weight: 0.1  # Adversarial loss weight
    
  # Contrastive distillation
  contrastive_distillation:
    enabled: true
    temperature: 0.07  # Contrastive temperature
    queue_size: 65536  # Negative sample queue

# Curriculum Learning for Distillation
curriculum:
  enabled: true
  
  # Data curriculum strategy
  data_curriculum:
    strategy: "easy_to_hard"  # Start with easy samples
    difficulty_metric: "teacher_confidence"  # Use teacher confidence
    stages: 4  # Number of curriculum stages
    
  # Temperature curriculum
  temperature_curriculum:
    strategy: "high_to_low"  # Anneal temperature
    initial_temperature: 8.0  # Start with high temperature
    final_temperature: 1.0  # End with low temperature
    
  # Loss weight curriculum
  alpha_curriculum:
    strategy: "distillation_to_task"  # Shift focus to task
    initial_alpha: 0.9  # Start with distillation focus
    final_alpha: 0.3  # End with task focus

# Evaluation Configuration
evaluation:
  # Teacher comparison
  compare_with_teacher: true  # Compare student vs teacher
  
  # Distillation-specific metrics
  track_distillation_loss: true  # Monitor distillation loss
  track_feature_similarity: true  # Feature alignment metric
  track_attention_similarity: true  # Attention alignment metric
  
  # Performance metrics
  metrics:
    - accuracy  # Classification accuracy
    - f1_macro  # Macro F1 score
    - inference_time  # Speed measurement
    - model_size  # Model size in MB
    - compression_ratio  # Teacher/Student size ratio
    
  # Efficiency measurements
  measure_speedup: true  # Inference speedup
  measure_memory_reduction: true  # Memory savings

# Model Compression Post-Distillation
compression:
  # Quantization settings
  quantize_after_distillation: true
  quantization_method: "dynamic"  # Dynamic quantization
  quantization_bits: 8  # INT8 quantization
  
  # Pruning settings
  prune_after_distillation: false
  pruning_sparsity: 0.5  # 50% sparsity
  
  # Model export
  export_onnx: true  # Export to ONNX
  optimize_onnx: true  # Optimize ONNX graph

# Hardware Optimization
hardware:
  # Device placement
  teacher_device: "cuda:0"  # Teacher on primary GPU
  student_device: "cuda:1"  # Student on secondary GPU
  
  # Memory optimization
  teacher_gradient_checkpointing: true  # Save teacher memory
  student_gradient_checkpointing: false  # Student is smaller
  
  # Cache management
  clear_teacher_cache_frequency: 100  # Clear cache periodically
  offload_teacher_to_cpu: false  # Keep teacher on GPU

# Monitoring and Logging
monitoring:
  # Prediction tracking
  log_teacher_predictions: true  # Log teacher outputs
  log_student_predictions: true  # Log student outputs
  log_prediction_differences: true  # Track disagreements
  
  # Visualization settings
  plot_loss_curves: true  # Loss progression plots
  plot_feature_similarity: true  # Feature alignment plots
  plot_attention_maps: false  # Attention visualization
  
  # Checkpointing strategy
  save_best_student: true  # Save best student model
  save_teacher_cache: true  # Cache teacher predictions

# Expected Results
expected_results:
  student_accuracy: 0.945  # ~2% drop from teacher
  teacher_accuracy: 0.965  # Teacher ensemble accuracy
  compression_ratio: 3.5  # 3.5x smaller model
  speedup: 2.8  # 2.8x faster inference

# Post-Distillation Fine-tuning
post_distillation:
  # Student fine-tuning
  finetune_student: true  # Additional fine-tuning
  finetune_epochs: 2  # Fine-tuning duration
  finetune_lr: 1e-5  # Lower learning rate
  
  # Student-Teacher ensemble
  create_student_teacher_ensemble: false
  ensemble_weight_student: 0.3  # Student contribution
  ensemble_weight_teacher: 0.7  # Teacher contribution

# Notes
notes: |
  Knowledge Distillation Configuration for AG News Text Classification:
  
  Key insights:
  1. Ensemble teacher provides richer knowledge than single teacher
  2. Multi-level distillation (logits + features + attention) improves transfer
  3. Progressive distillation with curriculum prevents overfitting
  4. Temperature annealing helps balance soft/hard targets
  
  Recommended settings:
  - For maximum compression: Use smaller student, higher temperature
  - For best accuracy: Use larger student, feature distillation
  - For fastest training: Cache teacher predictions
  - For production: Quantize after distillation
  
  Performance expectations:
  - Student retains 98% of teacher accuracy
  - 3-4x model compression ratio
  - 2-3x inference speedup
  - Better calibration than training from scratch
  
  Common issues:
  - Student underfitting: Increase alpha, reduce temperature
  - Feature mismatch: Adjust layer mapping or use projection
  - Memory issues: Enable gradient checkpointing, cache predictions
  - Poor generalization: Add noise, use data augmentation
  
  Computational considerations:
  - Memory: 2x requirement for teacher + student
  - Time: 1.5x training time vs single model
  - Storage: Cache can be large for big datasets

# References
references:
  - hinton_distillation: "https://arxiv.org/abs/1503.02531"
  - fitnets: "https://arxiv.org/abs/1412.5403"
  - attention_transfer: "https://arxiv.org/abs/1612.03928"
  - born_again: "https://arxiv.org/abs/1805.04770"
  - noisy_student: "https://arxiv.org/abs/1911.04252"
  - rkd: "https://arxiv.org/abs/1904.05068"
  - feature_distillation: "https://arxiv.org/abs/1507.00448"
