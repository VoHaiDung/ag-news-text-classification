# Adversarial Training Configuration
# ===================================
# Configuration for adversarial training following methodologies from:
# - Madry et al. (2018): "Towards Deep Learning Models Resistant to Adversarial Attacks"
# - Zhu et al. (2020): "FreeLB: Enhanced Adversarial Training for Natural Language Understanding"
# - Jiang et al. (2020): "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models"
# - Goodfellow et al. (2015): "Explaining and Harnessing Adversarial Examples"
#
# Mathematical Foundation:
# Adversarial loss: L_adv = max_||δ||≤ε L(f(x + δ), y)
# where δ is perturbation, ε is perturbation budget, f is model, L is loss
#
# Min-max optimization: min_θ max_||δ||≤ε E[(x,y)] L(f_θ(x + δ), y)
#
# Author: Võ Hải Dũng
# License: MIT

name: adversarial_training
type: advanced_adversarial
description: "Adversarial training for robust model against perturbations"

# Base configuration inheritance
inherit: training/standard/base_training.yaml

# Adversarial Training Configuration
adversarial:
  enabled: true
  
  # Primary adversarial method
  method: freelb  # Options: fgm, pgd, freelb, smart
  
  # Global attack parameters
  epsilon: 1.0  # Maximum perturbation magnitude
  alpha: 0.3  # Step size for iterative attacks
  attack_steps: 3  # Number of attack iterations
  
  # FGM (Fast Gradient Method) settings
  fgm:
    enabled: false
    epsilon: 1.0  # Perturbation magnitude
    normalize: true  # Normalize gradients
    
  # PGD (Projected Gradient Descent) settings
  pgd:
    enabled: false
    epsilon: 1.0  # Maximum L∞ perturbation
    alpha: 0.3  # Step size
    steps: 3  # Number of PGD iterations
    random_start: true  # Random initialization within epsilon ball
    
  # FreeLB (Free Large-Batch) settings
  freelb:
    enabled: true
    adv_steps: 3  # Number of adversarial steps
    adv_lr: 1e-1  # Learning rate for adversarial perturbations
    adv_init_mag: 1e-1  # Initial perturbation magnitude
    adv_max_norm: 1.0  # Maximum perturbation norm
    gradient_accumulation_steps: 3  # Accumulate gradients
    
  # SMART (Smooth Adversarial Regularization) settings
  smart:
    enabled: false
    epsilon: 1e-2  # Perturbation bound
    step_size: 1e-3  # Optimization step size
    noise_var: 1e-5  # Noise variance for smoothness
    norm_p: inf  # Norm type: 2 or inf
    k: 1  # Number of perturbations
    alpha: 0.01  # Regularization coefficient
    
  # Adversarial weight scheduling
  adversarial_warmup: true  # Gradually increase adversarial strength
  warmup_steps: 1000  # Number of warmup steps
  adversarial_weight: 0.5  # Weight of adversarial loss
  
  # Target layers for perturbation
  perturb_layers: ["embeddings", "encoder.layer.0", "encoder.layer.11", "encoder.layer.23"]
  
  # Gradient masking prevention
  gradient_masking_check: true  # Check for gradient masking
  
  # Virtual Adversarial Training (VAT)
  use_vat: false  # Enable VAT for semi-supervised learning
  vat_epsilon: 1e-2  # VAT perturbation magnitude
  vat_xi: 1e-6  # Small random noise for finite difference
  vat_iterations: 1  # Power iteration for approximating dominant eigenvector

# Training Modifications for Adversarial Training
training:
  # Extended training for robustness
  num_train_epochs: 12  # More epochs for adversarial robustness
  
  # Learning rate configuration
  learning_rate: 1e-5  # Smaller LR for stability
  
  # Gradient accumulation (required for FreeLB)
  gradient_accumulation_steps: 3
  
  # Evaluation frequency
  eval_steps: 300  # More frequent evaluation
  
  # Mixed precision settings
  fp16: false  # Disable for better gradient quality
  
  # Gradient clipping
  max_grad_norm: 0.5  # Stronger clipping for stability

# Loss Configuration
loss:
  # Adversarial loss weight
  adversarial_loss_weight: 0.5  # Balance between clean and adversarial loss
  
  # KL divergence for consistency
  use_kl_divergence: true  # Enforce prediction consistency
  kl_weight: 0.1  # KL divergence weight
  
  # Symmetric KL divergence
  symmetric_kl: true  # Use symmetric KL for better stability

# Regularization Enhancements
regularization:
  # Dropout configuration for robustness
  dropout_rate: 0.2  # General dropout
  attention_dropout_rate: 0.15  # Attention layer dropout
  hidden_dropout_rate: 0.2  # Hidden layer dropout
  
  # R-Drop regularization
  r_drop:
    enabled: true
    alpha: 0.5  # R-Drop coefficient
    
  # Token cutoff regularization
  token_cutoff:
    enabled: false
    cutoff_length: 128  # Maximum sequence length after cutoff
    
  # Feature cutoff regularization
  feature_cutoff:
    enabled: false
    cutoff_prob: 0.1  # Probability of feature cutoff

# Data Augmentation (Complementary to Adversarial)
augmentation:
  # Text-level augmentation techniques
  synonym_replacement: true  # Replace with synonyms
  random_insertion: true  # Insert random words
  random_swap: true  # Swap word positions
  random_deletion: true  # Delete random words
  
  # Embedding-level augmentation
  embedding_dropout: 0.1  # Dropout on embeddings
  embedding_noise: 0.05  # Gaussian noise on embeddings

# Evaluation Settings
evaluation:
  # Robustness evaluation flag
  evaluate_robustness: true
  
  # Attack evaluation configuration
  attack_evaluation:
    enabled: true
    methods: ["textfooler", "bert-attack", "pwws"]  # Attack methods to test
    
  # Perturbation analysis
  perturbation_analysis: true
  perturbation_budget: [0.1, 0.2, 0.5, 1.0]  # Different epsilon values
  
  # Certified robustness
  certified_robustness: false
  certification_radius: 0.1  # Radius for certification

# Model-Specific Adversarial Settings
model_specific:
  # DeBERTa-specific configuration
  deberta:
    perturb_attention: true  # Perturb attention weights
    perturb_position_embeddings: true  # Perturb position embeddings
    disentangled_attention_perturbation: true  # DeBERTa-specific
    
  # RoBERTa-specific configuration
  roberta:
    perturb_token_type_embeddings: false  # RoBERTa doesn't use token types
    layer_wise_perturbation: true  # Apply perturbations layer-wise
    
  # XLNet-specific configuration
  xlnet:
    perturb_memory: true  # Perturb memory stream
    two_stream_perturbation: true  # Perturb both streams

# Advanced Adversarial Techniques
advanced_techniques:
  # Multi-perturbation training
  multi_perturbation:
    enabled: true
    perturbations: ["fgm", "pgd"]  # Multiple perturbation types
    combination: "random"  # Options: random, sequential, weighted
    
  # Curriculum adversarial training
  curriculum:
    enabled: true
    schedule: "linear"  # Options: linear, exponential, step
    initial_epsilon: 0.1  # Starting perturbation
    final_epsilon: 1.0  # Final perturbation
    
  # Adversarial distillation
  adversarial_distillation:
    enabled: false
    teacher_model: "outputs/models/fine_tuned/deberta_best.pt"
    temperature: 3.0  # Distillation temperature
    
  # Ensemble adversarial training
  ensemble_adversarial:
    enabled: false
    models: ["deberta", "roberta"]  # Models for ensemble
    aggregate_gradients: true  # Aggregate adversarial gradients

# Monitoring and Logging
monitoring:
  # Adversarial metrics tracking
  track_adversarial_loss: true  # Track adversarial loss
  track_perturbation_norm: true  # Track perturbation magnitudes
  track_gradient_norm: true  # Track gradient norms
  
  # Visualization settings
  visualize_perturbations: true  # Visualize perturbations
  visualize_attention_changes: true  # Visualize attention changes
  
  # Logging frequency
  adversarial_logging_steps: 100  # Log every N steps

# Hardware Optimization
hardware:
  # Memory optimization
  gradient_checkpointing: true  # Use gradient checkpointing
  
  # Batch size management
  adversarial_batch_size_reduction: 2  # Reduce batch size for adversarial steps
  
  # CPU offloading
  offload_perturbations: false  # Offload perturbations to CPU

# Checkpointing Configuration
checkpointing:
  # Adversarial checkpoint settings
  save_adversarial_checkpoints: true  # Save adversarial checkpoints
  adversarial_checkpoint_dir: "outputs/models/adversarial"
  
  # Best model tracking
  save_best_adversarial: true  # Save best adversarial model
  adversarial_metric: "robustness_score"  # Metric for best model

# Post-Training Configuration
post_training:
  # Adversarial fine-tuning
  adversarial_finetune: true  # Additional adversarial fine-tuning
  finetune_epochs: 2  # Number of fine-tuning epochs
  finetune_epsilon: 0.5  # Fine-tuning perturbation budget
  
  # Robustness certification
  certify_robustness: false  # Certify model robustness
  certification_samples: 1000  # Samples for certification

# Expected Results
expected_results:
  clean_accuracy: 0.958  # Expected clean accuracy
  adversarial_accuracy: 0.925  # Expected adversarial accuracy
  robustness_improvement: 0.15  # Expected robustness gain

# Notes
notes: |
  Adversarial Training Configuration for AG News Text Classification:
  
  Key insights:
  1. FreeLB provides best trade-off between robustness and efficiency
  2. Larger epsilon generally improves robustness but hurts clean accuracy
  3. Gradient accumulation crucial for FreeLB effectiveness
  4. Adversarial warmup prevents training instability
  
  Recommended settings:
  - For maximum robustness: Use PGD with large epsilon
  - For efficiency: Use FGM or SMART
  - For best trade-off: Use FreeLB with moderate epsilon
  
  Performance expectations:
  - 2-3% drop in clean accuracy
  - 15-20% improvement against adversarial attacks
  - Better generalization to out-of-distribution data
  - Improved calibration and uncertainty estimates
  
  Common issues:
  - Training instability: Reduce learning rate or epsilon
  - Gradient explosion: Increase gradient clipping
  - Poor clean accuracy: Reduce adversarial weight
  - Memory issues: Enable gradient checkpointing
  
  Mathematical insights:
  - Adversarial training solves a min-max optimization problem
  - FreeLB approximates K-step PGD within single forward-backward pass
  - SMART adds smoothness regularization to adversarial objective

# References
references:
  - freelb: "https://arxiv.org/abs/1909.11764"
  - pgd: "https://arxiv.org/abs/1706.06083"
  - smart: "https://arxiv.org/abs/1911.03437"
  - adversarial_training: "https://arxiv.org/abs/1412.6572"
  - fgm: "https://arxiv.org/abs/1605.07725"
  - vat: "https://arxiv.org/abs/1704.03976"
