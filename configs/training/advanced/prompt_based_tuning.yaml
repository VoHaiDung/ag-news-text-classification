# Prompt-Based Tuning Configuration
# ==================================
# Configuration for prompt-based learning following methodologies from:
# - Liu et al. (2021): "Pre-train, Prompt, and Predict: A Systematic Survey"
# - Lester et al. (2021): "The Power of Scale for Parameter-Efficient Prompt Tuning"
# - Li & Liang (2021): "Prefix-Tuning: Optimizing Continuous Prompts"
# - Schick & Schütze (2021): "Exploiting Cloze Questions for Few Shot Text Classification"
#
# Mathematical Foundation:
# Prompt-based learning: P(y|x) = P(M(T(x)))
# where T is template function, M is verbalizer mapping
#
# Soft prompts: Optimize continuous embeddings P ∈ R^{k×d}
# minimizing L = -log P(y|[P; x])
#
# Author: Võ Hải Dũng
# License: MIT

name: prompt_based_tuning
type: advanced_prompt
description: "Parameter-efficient prompt-based tuning for text classification"

# Base configuration inheritance
extends: ../standard/base_training.yaml

# Prompt Configuration
prompt:
  enabled: true
  
  # Prompt type
  prompt_type: soft  # Options: hard, soft, mixed, continuous, discrete
  
  # Template configuration
  template:
    # Template pattern
    pattern: "This is a [MASK] news article: {text}"  # For AG News
    
    # Multiple templates
    use_multiple_templates: true
    templates:
      - "Category: [MASK]. Article: {text}"
      - "{text} This article is about [MASK]."
      - "Topic: [MASK]. Content: {text}"
      - "This is a [MASK] news article: {text}"
    
    # Template selection
    template_selection: ensemble  # Options: single, random, ensemble, learned
    
    # Template optimization
    optimize_template: false
    template_search_space: null

# Soft Prompt Configuration
soft_prompt:
  enabled: true
  
  # Prompt initialization
  initialization:
    method: random  # Options: random, vocab, task_specific, pretrained
    
    # Random initialization
    random:
      distribution: normal  # Options: normal, uniform
      mean: 0.0
      std: 0.02
      
    # Vocabulary initialization
    vocab:
      tokens: ["news", "article", "category", "topic", "about"]
      
    # Task-specific initialization
    task_specific:
      use_label_words: true
      label_words:
        world: ["world", "international", "global"]
        sports: ["sports", "game", "athletic"]
        business: ["business", "economy", "finance"]
        tech: ["technology", "science", "innovation"]
        
    # Pretrained initialization
    pretrained:
      model: null  # Path to pretrained soft prompts
      
  # Prompt dimensions
  num_tokens: 20  # Number of soft prompt tokens
  embedding_dim: 768  # Dimension of prompt embeddings
  
  # Prompt position
  position: prefix  # Options: prefix, suffix, mixed, middle
  
  # Reparameterization
  reparameterization:
    enabled: false
    method: mlp  # Options: mlp, lstm, attention
    hidden_dim: 512
    
  # Deep prompting (prompts at multiple layers)
  deep_prompting:
    enabled: false
    layers: [0, 6, 11]  # Layers to insert prompts
    shared_prompts: false  # Share prompts across layers

# Prefix Tuning
prefix_tuning:
  enabled: false
  
  # Prefix configuration
  prefix_length: 10
  
  # Prefix dimensions
  num_layers: 12  # Number of transformer layers
  num_heads: 12  # Number of attention heads
  
  # Prefix projection
  use_projection: true
  projection_dim: 512
  
  # Reparameterization
  reparameterize: true
  bottleneck_dim: 256
  
  # Layer-specific prefixes
  layer_specific: false
  shared_across_layers: true

# P-Tuning Configuration
p_tuning:
  enabled: false
  
  # Prompt encoder
  encoder_type: lstm  # Options: lstm, mlp, transformer
  
  # LSTM encoder
  lstm:
    hidden_dim: 512
    num_layers: 2
    bidirectional: true
    
  # MLP encoder
  mlp:
    hidden_dims: [512, 768]
    activation: relu
    dropout: 0.1
    
  # Pattern configuration
  pattern_length: 20
  anchor_tokens: true  # Use anchor tokens between prompts

# LoRA with Prompting
lora_prompting:
  enabled: false
  
  # LoRA configuration
  r: 8  # Rank
  alpha: 16  # Scaling factor
  dropout: 0.1
  
  # Target modules
  target_modules: ["q_proj", "v_proj"]
  
  # Combination with prompts
  combine_method: parallel  # Options: parallel, sequential, mixed

# Verbalizer Configuration
verbalizer:
  # Verbalizer type
  type: manual  # Options: manual, automatic, soft, prototypical
  
  # Manual verbalizer (label words)
  manual:
    label_words:
      0: ["World", "International", "Global", "Foreign"]
      1: ["Sports", "Game", "Athletic", "Competition"]
      2: ["Business", "Economy", "Finance", "Market"]
      3: ["Technology", "Science", "Tech", "Innovation"]
      
  # Automatic verbalizer search
  automatic:
    enabled: false
    search_method: gradient  # Options: gradient, reinforcement, evolutionary
    num_candidates: 100
    selection_metric: accuracy
    
  # Soft verbalizer
  soft:
    enabled: false
    trainable: true
    initialization: uniform
    
  # Prototypical verbalizer
  prototypical:
    enabled: false
    use_class_prototypes: true
    prototype_aggregation: mean

# Prompt Ensemble
ensemble:
  enabled: false
  
  # Ensemble method
  method: voting  # Options: voting, averaging, attention, learned
  
  # Voting ensemble
  voting:
    type: soft  # Options: soft, hard
    weights: uniform  # Options: uniform, learned, performance
    
  # Attention-based ensemble
  attention:
    hidden_dim: 256
    num_heads: 4
    
  # Learned ensemble
  learned:
    meta_learner: mlp
    hidden_dims: [256, 128]

# Training Configuration
training:
  # Frozen model
  freeze_model: true  # Only train prompts
  freeze_layers: all  # Options: all, list of layers
  
  # Learning rates
  prompt_learning_rate: 1e-3  # Higher LR for prompts
  model_learning_rate: 1e-5  # Lower LR for model (if not frozen)
  
  # Optimizer
  prompt_optimizer: adamw
  model_optimizer: adamw
  
  # Scheduler
  prompt_scheduler: cosine
  warmup_ratio: 0.1
  
  # Regularization
  prompt_dropout: 0.1
  prompt_weight_decay: 0.01
  
  # Gradient clipping
  gradient_clip_val: 1.0
  
  # Training epochs
  num_epochs: 20  # Fewer epochs needed
  
  # Batch size
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1

# Few-shot Configuration
few_shot:
  enabled: false
  
  # Shot configuration
  num_shots: 16  # 16-shot learning
  
  # Sampling strategy
  sampling: balanced  # Options: balanced, random, stratified
  
  # Support set usage
  use_support_set: true
  support_in_context: false  # In-context learning style
  
  # Data augmentation for few-shot
  augment_support: true
  augmentation_factor: 2

# Prompt Optimization
optimization:
  # Automatic prompt length selection
  auto_select_length: false
  length_range: [5, 50]
  
  # Prompt pruning
  pruning:
    enabled: false
    method: magnitude  # Options: magnitude, gradient, attention
    sparsity: 0.5
    
  # Prompt distillation
  distillation:
    enabled: false
    teacher_prompts: null  # Path to teacher prompts
    temperature: 3.0
    
  # Multi-task prompt sharing
  multi_task_sharing:
    enabled: false
    shared_prompt_ratio: 0.5
    task_specific_ratio: 0.5

# Continuous Prompt Search
prompt_search:
  enabled: false
  
  # Search method
  method: gradient  # Options: gradient, reinforcement, evolutionary, bayesian
  
  # Gradient-based search
  gradient:
    learning_rate: 1e-2
    num_steps: 100
    
  # Reinforcement learning search
  reinforcement:
    algorithm: reinforce
    reward: validation_accuracy
    
  # Evolutionary search
  evolutionary:
    population_size: 50
    num_generations: 20
    mutation_rate: 0.1
    
  # Bayesian optimization
  bayesian:
    acquisition_function: ei  # Expected improvement
    num_iterations: 50

# Advanced Prompting Techniques
advanced:
  # Chain-of-thought prompting
  chain_of_thought:
    enabled: false
    cot_trigger: "Let's think step by step:"
    
  # Instruction prompting
  instruction:
    enabled: false
    instruction_template: "Classify the following text into categories."
    
  # Demonstration examples
  demonstrations:
    enabled: false
    num_demos: 3
    demo_selection: similar  # Options: random, similar, diverse
    
  # Prompt chaining
  chaining:
    enabled: false
    chain_length: 2
    intermediate_tasks: []

# Evaluation
evaluation:
  # Prompt analysis
  analyze_prompts: true
  visualize_attention: true
  
  # Ablation studies
  ablation:
    without_prompts: true
    random_prompts: true
    different_lengths: [5, 10, 20, 50]
    
  # Transferability
  test_transfer: false
  transfer_tasks: []
  
  # Robustness
  test_robustness: false
  perturbation_types: ["synonym", "paraphrase", "noise"]

# Memory and Efficiency
efficiency:
  # Parameter efficiency
  compute_parameter_efficiency: true
  
  # Memory optimization
  gradient_checkpointing: false
  mixed_precision: true
  
  # Inference optimization
  cache_prompts: true
  compile_prompts: false

# Debugging
debugging:
  # Prompt visualization
  save_prompt_embeddings: true
  track_prompt_evolution: true
  
  # Gradient analysis
  monitor_prompt_gradients: true
  detect_dead_prompts: true
  
  # Performance tracking
  log_per_template_performance: true
  log_verbalizer_distribution: true

# Notes
notes: |
  Prompt-Based Tuning Configuration for AG News Text Classification:
  
  Template design for AG News Text Classification:
  - "This is a [MASK] news article: {text}"
  - "Category: [MASK]. Article: {text}"
  - "{text} This article is about [MASK]."
  
  Verbalizer mapping:
  - World → ["World", "International", "Global"]
  - Sports → ["Sports", "Athletic", "Game"]
  - Business → ["Business", "Economy", "Finance"]
  - Tech → ["Technology", "Science", "Innovation"]
  
  Recommended settings:
  - Soft prompts: 20 tokens, prefix position
  - Learning rate: 1e-3 (higher than fine-tuning)
  - Frozen model for parameter efficiency
  - Ensemble multiple templates for robustness
  
  Expected performance:
  - 90%+ accuracy with full data
  - 80%+ accuracy with 16-shot
  - 10-100x fewer parameters than fine-tuning
  - 2-3x faster training
  
  Common issues:
  - Prompt collapse: Reduce learning rate
  - Poor initialization: Try vocab initialization
  - Overfitting: Add dropout, reduce prompt length
  - Instability: Use gradient clipping
