# Instruction Tuning Configuration
# =================================
# Configuration for instruction-based fine-tuning following methodologies from:
# - Wei et al. (2022): "Finetuned Language Models Are Zero-Shot Learners"
# - Sanh et al. (2022): "Multitask Prompted Training Enables Zero-Shot Task Generalization"
# - Wang et al. (2022): "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
# - Ouyang et al. (2022): "Training language models to follow instructions with human feedback"
#
# Mathematical Foundation:
# Instruction tuning objective: L = -E[log P(y|instruction, x, θ)]
# where instruction provides explicit task description
#
# RLHF objective: J(θ) = E[r(y) - β KL(π_θ||π_ref)]
# where r is reward model, β controls KL penalty
#
# Author: Võ Hải Dũng
# License: MIT

name: instruction_tuning
type: advanced_instruction
description: "Instruction-based fine-tuning for improved task generalization and zero-shot capabilities"

# Base configuration inheritance  
extends: ../standard/base_training.yaml

# Instruction Tuning Configuration
instruction_tuning:
  enabled: true
  
  # Model type
  model_type: encoder_decoder  # Options: encoder_decoder, decoder_only, encoder_only
  base_model: t5-base  # Options: t5, flan-t5, gpt, llama, etc.
  
  # Instruction format
  instruction_format:
    # Template structure
    template: |
      Task: {task_description}
      Input: {input_text}
      Output:
    
    # Alternative templates
    templates:
      - "Instruction: {task_description}\nText: {input_text}\nAnswer:"
      - "{task_description}\n\n{input_text}\n\nResponse:"
      - "Q: {task_description}\nContext: {input_text}\nA:"
    
    # Template selection
    template_selection: random  # Options: fixed, random, learned
    
    # System prompt (for chat models)
    system_prompt: "You are a helpful assistant for text classification tasks."

# Task Instructions
task_instructions:
  # AG News classification instruction
  ag_news:
    task_name: "AG News Text Classification"
    description: "Classify the given news article into one of four categories: World, Sports, Business, or Science/Technology."
    
    # Detailed instructions
    detailed: |
      Given a news article, determine which category it belongs to:
      1. World: International news, politics, conflicts
      2. Sports: Athletic events, games, competitions
      3. Business: Economy, finance, companies, markets
      4. Science/Technology: Scientific discoveries, tech news, innovations
      
      Read the article carefully and select the most appropriate category.
    
    # Short instruction
    short: "Classify this news article as World, Sports, Business, or Science/Tech."
    
    # Examples in instruction
    include_examples: true
    num_examples: 2
    example_selection: diverse  # Options: random, similar, diverse

# Multi-Task Instruction Tuning
multi_task:
  enabled: true
  
  # Task mixture
  tasks:
    - name: ag_news_classification
      weight: 0.3
      instruction: "Classify the news article"
      
    - name: summarization
      weight: 0.2
      instruction: "Summarize the main points"
      
    - name: sentiment_analysis
      weight: 0.2
      instruction: "Determine the sentiment"
      
    - name: keyword_extraction
      weight: 0.15
      instruction: "Extract key topics"
      
    - name: question_answering
      weight: 0.15
      instruction: "Answer questions about the text"
  
  # Task sampling
  sampling_strategy: weighted  # Options: weighted, uniform, curriculum
  tasks_per_batch: 1  # Mix tasks in batch

# Chain-of-Thought (CoT)
chain_of_thought:
  enabled: true
  
  # CoT trigger
  trigger: "Let's think step by step:"
  
  # CoT format
  format: |
    {instruction}
    {trigger}
    1. First, I'll identify the main topic...
    2. Key indicators suggest...
    3. Therefore, the category is...
    Answer: {answer}
  
  # Auto-generate CoT
  auto_cot:
    enabled: false
    generation_method: zero_shot  # Options: zero_shot, few_shot, self_consistency
    
  # Self-consistency
  self_consistency:
    enabled: false
    num_samples: 5
    aggregation: majority  # Options: majority, weighted, confidence

# Demonstration Examples
demonstrations:
  # In-context learning
  in_context:
    enabled: true
    num_demos: 3
    
    # Demo selection
    selection_strategy: diverse  # Options: random, similar, diverse, manual
    
    # Demo format
    format: |
      Example {i}:
      Input: {input}
      Output: {output}
    
    # Demo pool
    use_training_data: true
    demo_pool_size: 100
    
  # Dynamic demonstrations
  dynamic:
    enabled: false
    retrieve_similar: true
    similarity_metric: cosine
    top_k: 5

# Self-Instruct
self_instruct:
  enabled: false
  
  # Instruction generation
  generation:
    model: gpt-3.5-turbo
    num_instructions: 1000
    diversity_threshold: 0.7
    
  # Instruction filtering
  filtering:
    min_length: 10
    max_length: 500
    quality_threshold: 0.8
    
  # Instruction augmentation
  augmentation:
    paraphrase: true
    back_translation: false
    synonym_replacement: true

# Reinforcement Learning from Human Feedback (RLHF)
rlhf:
  enabled: false
  
  # Reward model
  reward_model:
    type: learned  # Options: learned, rule_based, hybrid
    checkpoint: null  # Path to reward model
    
  # PPO configuration
  ppo:
    learning_rate: 1e-5
    batch_size: 128
    epochs: 4
    clip_range: 0.2
    value_loss_coef: 0.5
    
  # KL penalty
  kl_penalty:
    coefficient: 0.02
    target: 6.0
    
  # Reward shaping
  reward_shaping:
    use_auxiliary_rewards: true
    auxiliary_weights:
      fluency: 0.1
      relevance: 0.2
      correctness: 0.7

# Constitutional AI
constitutional_ai:
  enabled: false
  
  # Principles
  principles:
    - "Be helpful and harmless"
    - "Provide accurate information"
    - "Be concise and clear"
    
  # Critique and revision
  critique:
    enabled: true
    num_revisions: 2
    
  # Red teaming
  red_teaming:
    enabled: false
    attack_types: ["jailbreak", "prompt_injection"]

# Training Configuration
training:
  # Model configuration
  max_length: 512
  max_target_length: 128
  
  # Training parameters
  num_epochs: 5
  learning_rate: 5e-5
  warmup_steps: 500
  
  # Batch configuration
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  
  # Optimization
  optimizer: adamw
  weight_decay: 0.01
  gradient_clipping: 1.0
  
  # Mixed precision
  fp16: true
  bf16: false  # For newer GPUs
  
  # Instruction-specific
  instruction_loss_weight: 1.0
  response_loss_only: true  # Only compute loss on response tokens

# Generation Configuration
generation:
  # Decoding strategy
  do_sample: true
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  
  # Length control
  max_new_tokens: 128
  min_length: 10
  
  # Penalties
  repetition_penalty: 1.2
  length_penalty: 1.0
  
  # Beam search
  num_beams: 1  # 1 for sampling
  early_stopping: true
  
  # Special tokens
  pad_token_id: 0
  eos_token_id: 1
  
  # Constraints
  no_repeat_ngram_size: 3
  force_words: null

# Evaluation
evaluation:
  # Metrics
  metrics:
    - accuracy
    - f1_score
    - bleu
    - rouge
    - exact_match
    
  # Instruction following
  instruction_following:
    enabled: true
    criteria:
      - correctness
      - completeness
      - relevance
      - format_compliance
      
  # Zero-shot evaluation
  zero_shot:
    enabled: true
    unseen_tasks: []
    
  # Human evaluation
  human_eval:
    enabled: false
    criteria:
      - helpfulness
      - accuracy
      - clarity

# Data Processing
data_processing:
  # Instruction augmentation
  augment_instructions: true
  augmentation_methods:
    - paraphrase
    - simplify
    - elaborate
    
  # Response processing
  clean_responses: true
  normalize_responses: true
  
  # Format validation
  validate_format: true
  fix_formatting: true

# Efficiency Optimizations
efficiency:
  # LoRA for instruction tuning
  use_lora: false
  lora_r: 8
  lora_alpha: 16
  
  # Quantization
  quantization:
    enabled: false
    bits: 8
    
  # Flash attention
  use_flash_attention: false
  
  # Gradient checkpointing
  gradient_checkpointing: true

# Safety and Alignment
safety:
  # Content filtering
  filter_toxic: true
  filter_biased: true
  
  # Output safety
  safety_checks: true
  fallback_response: "I cannot provide that information."
  
  # Alignment techniques
  alignment:
    method: constitutional  # Options: constitutional, rlhf, dpo
    strength: medium

# Debugging and Analysis
debugging:
  # Log examples
  log_sample_predictions: true
  num_samples_to_log: 10
  
  # Instruction analysis
  analyze_instruction_distribution: true
  track_instruction_difficulty: true
  
  # Performance breakdown
  per_instruction_metrics: true
  error_analysis: true
  
  # Attention analysis
  save_attention_weights: false
  visualize_attention: false

# Notes
notes: |
  Instruction Tuning Configuration for AG News Text Classification:
  
  Instruction templates:
  1. "Classify this news article into World, Sports, Business, or Tech."
  2. "What category does this news article belong to?"
  3. "Determine if this is about World/Sports/Business/Technology."
  
  Key strategies:
  - Multi-task instruction tuning for generalization
  - Chain-of-thought for reasoning
  - In-context learning with demonstrations
  - RLHF for alignment (optional)
  
  Expected improvements:
  - Better zero-shot performance
  - Improved instruction following
  - More robust to prompt variations
  - Better generalization to new tasks
  
  Training tips:
  - Start with supervised fine-tuning
  - Add RLHF for further alignment
  - Use diverse instruction templates
  - Include reasoning in responses
  
  Common issues:
  - Overfitting to instruction format: Use template variety
  - Poor zero-shot: Increase task diversity
  - Hallucination: Add factuality rewards
  - Format non-compliance: Strengthen format supervision
