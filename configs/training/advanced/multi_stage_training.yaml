# Multi-Stage Progressive Training Configuration
# ==============================================
# Configuration for multi-stage training following methodologies from:
# - Bengio et al. (2009): "Curriculum Learning"
# - Gong et al. (2019): "Efficient Training of BERT by Progressively Stacking"
# - Zhang & He (2020): "Accelerating Training of Transformer-Based Language Models"
# - Gu et al. (2021): "Progressive Multi-Stage Learning for Discriminative Tracking"
#
# Mathematical Foundation:
# Stage k objective: L_k = L_task(θ_k) + λ_k * R_k(θ_k, θ_{k-1})
# where R_k is regularization to previous stage, λ_k is stage-specific weight
#
# Progressive unfreezing: θ_k = {θ_frozen ∪ θ_trainable_k}
# where trainable parameters increase with stages
#
# Author: Võ Hải Dũng
# License: MIT

name: multi_stage_training
type: advanced_progressive
description: "Multi-stage progressive training for efficient and stable learning"

# Base configuration inheritance
extends: ../standard/base_training.yaml

# Multi-Stage Configuration
multi_stage:
  enabled: true
  
  # Number of stages
  num_stages: 4
  
  # Stage transition
  transition:
    type: smooth  # Options: abrupt, smooth, overlapping
    overlap_epochs: 1  # For overlapping transition
    
    # Smooth transition
    smoothing_factor: 0.9  # EMA between stages
    warmup_epochs: 1  # Warmup after transition
    
  # Global scheduling
  total_epochs: 20
  stage_allocation: adaptive  # Options: equal, adaptive, manual
  
  # Stage dependencies
  dependencies:
    sequential: true  # Stages must complete in order
    skip_failed: false  # Skip failed stages
    
  # Checkpointing
  checkpoint_all_stages: true
  keep_best_stage_only: false

# Stage Definitions
stages:
  # Stage 1: Feature Extraction
  - name: feature_extraction
    stage_id: 1
    epochs: 5
    description: "Train shallow layers for basic feature extraction"
    
    # Model configuration
    model:
      # Layer freezing
      freeze_layers:
        - embeddings
        - encoder.layer.0
        - encoder.layer.1
        - encoder.layer.2
        - encoder.layer.3
        - encoder.layer.4
        - encoder.layer.5
        
      # Unfrozen layers
      trainable_layers:
        - encoder.layer.6
        - encoder.layer.7
        - encoder.layer.8
        - encoder.layer.9
        - encoder.layer.10
        - encoder.layer.11
        - classifier
        
      # Dropout modification
      dropout_rate: 0.1
      attention_dropout: 0.1
      
    # Training configuration
    training:
      learning_rate: 5e-5
      batch_size: 64
      gradient_accumulation_steps: 1
      
      # Optimizer
      optimizer: adamw
      weight_decay: 0.01
      
      # Scheduler
      scheduler: cosine
      warmup_ratio: 0.2
      
    # Loss configuration
    loss:
      type: cross_entropy
      label_smoothing: 0.1
      
    # Data configuration
    data:
      subset_ratio: 0.5  # Use 50% of data
      sampling_strategy: random
      augmentation: light  # Light augmentation
      
    # Success criteria
    success_criteria:
      min_accuracy: 0.7
      min_f1: 0.65
      early_stopping_patience: 3
  
  # Stage 2: Deep Feature Learning
  - name: deep_features
    stage_id: 2
    epochs: 5
    description: "Unfreeze middle layers for deeper feature learning"
    
    model:
      # Progressive unfreezing
      freeze_layers:
        - embeddings
        - encoder.layer.0
        - encoder.layer.1
        
      trainable_layers:
        - encoder.layer.2
        - encoder.layer.3
        - encoder.layer.4
        - encoder.layer.5
        - encoder.layer.6
        - encoder.layer.7
        - encoder.layer.8
        - encoder.layer.9
        - encoder.layer.10
        - encoder.layer.11
        - classifier
        
      dropout_rate: 0.2
      attention_dropout: 0.15
      
    training:
      learning_rate: 3e-5
      batch_size: 48
      gradient_accumulation_steps: 2
      
      # Layer-wise learning rates
      layer_wise_lr:
        enabled: true
        decay_factor: 0.9  # Each layer gets 0.9x LR of layer above
        
    loss:
      type: focal_loss
      gamma: 2.0
      alpha: 0.25
      
    data:
      subset_ratio: 0.75
      sampling_strategy: stratified
      augmentation: medium
      
    # Knowledge retention
    knowledge_retention:
      enabled: true
      method: ewc  # Elastic Weight Consolidation
      lambda: 0.5
      
    success_criteria:
      min_accuracy: 0.8
      min_f1: 0.75
      improvement_threshold: 0.05
  
  # Stage 3: Full Model Fine-tuning
  - name: full_fine_tuning
    stage_id: 3
    epochs: 7
    description: "Fine-tune entire model with all data"
    
    model:
      # Unfreeze all layers
      freeze_layers: []
      trainable_layers: all
      
      dropout_rate: 0.3
      attention_dropout: 0.2
      
      # Stochastic depth
      stochastic_depth:
        enabled: true
        drop_rate: 0.1
        
    training:
      learning_rate: 1e-5
      batch_size: 32
      gradient_accumulation_steps: 4
      
      # Advanced optimizer
      optimizer: lamb  # Layer-wise Adaptive Moments
      weight_decay: 0.05
      
      # Scheduler with restarts
      scheduler: cosine_with_restarts
      num_restarts: 2
      
    loss:
      type: cross_entropy
      label_smoothing: 0.15
      
      # Mixup augmentation
      mixup:
        enabled: true
        alpha: 0.2
        
    data:
      subset_ratio: 1.0  # Full dataset
      sampling_strategy: balanced
      augmentation: heavy
      
      # Hard example mining
      hard_example_mining:
        enabled: true
        ratio: 0.3
        
    # Regularization
    regularization:
      l2_weight: 0.01
      gradient_penalty: 0.001
      
    success_criteria:
      min_accuracy: 0.85
      min_f1: 0.82
  
  # Stage 4: Optimization and Compression
  - name: optimization
    stage_id: 4
    epochs: 3
    description: "Optimize for inference with distillation and pruning"
    
    model:
      trainable_layers: all
      
      # Reduced capacity
      dropout_rate: 0.1
      attention_dropout: 0.05
      
    training:
      learning_rate: 5e-6
      batch_size: 128
      
      # Optimization for inference
      optimize_for_inference: true
      
    # Knowledge distillation from best checkpoint
    distillation:
      enabled: true
      teacher_checkpoint: stage_3_best
      temperature: 3.0
      alpha: 0.7
      
    # Model compression
    compression:
      pruning:
        enabled: true
        sparsity: 0.3
        structured: true
        
      quantization:
        enabled: false
        bits: 8
        
    data:
      subset_ratio: 0.5
      focus_on_errors: true  # Focus on misclassified samples
      
    success_criteria:
      maintain_accuracy: 0.98  # Maintain 98% of previous accuracy
      inference_speedup: 1.5  # 1.5x faster inference

# Progressive Strategies
progressive_strategies:
  # Layer-wise training
  layer_wise:
    enabled: true
    direction: bottom_up  # Options: bottom_up, top_down, mixed
    layers_per_stage: 3
    
  # Gradual unfreezing
  gradual_unfreezing:
    enabled: true
    unfreezing_schedule: exponential  # Options: linear, exponential, manual
    
  # Progressive resizing (for vision models)
  progressive_resizing:
    enabled: false
    initial_size: 224
    final_size: 384
    
  # Progressive augmentation
  progressive_augmentation:
    enabled: true
    initial_strength: 0.1
    final_strength: 0.5
    
  # Progressive dropout
  progressive_dropout:
    enabled: true
    initial_rate: 0.0
    final_rate: 0.3

# Stage Transitions
transitions:
  # Transition strategies
  strategy: gradual  # Options: abrupt, gradual, adaptive
  
  # Knowledge transfer
  knowledge_transfer:
    enabled: true
    method: soft_targets  # Options: soft_targets, feature_matching, attention_transfer
    
  # Learning rate reset
  lr_reset:
    enabled: true
    reset_factor: 2.0  # Multiply LR by this factor
    
  # Optimizer state
  reset_optimizer: false
  reset_scheduler: true
  
  # Validation between stages
  validate_transition: true
  rollback_on_failure: true

# Adaptive Staging
adaptive:
  enabled: true
  
  # Dynamic stage allocation
  allocation_metric: validation_improvement
  min_epochs_per_stage: 2
  max_epochs_per_stage: 10
  
  # Early stage completion
  early_completion:
    enabled: true
    patience: 3
    min_improvement: 0.001
    
  # Stage skipping
  skip_conditions:
    accuracy_threshold: 0.9
    convergence_check: true
    
  # Dynamic stage insertion
  insert_stages:
    enabled: false
    trigger: plateau
    inserted_stage_type: regularization

# Resource Management
resources:
  # Memory management across stages
  memory:
    gradient_checkpointing_stages: [3, 4]
    clear_cache_between_stages: true
    
  # Compute allocation
  compute:
    gpu_allocation:
      stage_1: 1
      stage_2: 1
      stage_3: 2  # Use 2 GPUs for stage 3
      stage_4: 1
      
  # Data loading
  data_loading:
    num_workers_per_stage: [2, 4, 4, 2]
    prefetch_factor: 2

# Monitoring and Logging
monitoring:
  # Stage-specific metrics
  track_stage_metrics: true
  
  # Comparison across stages
  compare_stages: true
  comparison_metrics:
    - accuracy
    - f1_score
    - loss
    - learning_rate
    - gradient_norm
    
  # Visualization
  plot_stage_progression: true
  plot_layer_gradients: true
  
  # Checkpointing
  save_all_stage_checkpoints: true
  save_transition_states: true

# Recovery and Fault Tolerance
recovery:
  # Stage failure handling
  on_stage_failure: retry  # Options: retry, skip, abort
  max_retries: 2
  
  # Checkpoint recovery
  auto_recover: true
  recovery_strategy: last_successful_stage
  
  # Partial training
  allow_partial_completion: true
  min_completed_stages: 2

# Experimental Features
experimental:
  # Neural Architecture Search per stage
  nas_per_stage:
    enabled: false
    search_space: darts
    
  # Meta-learning for stage optimization
  meta_stage_optimization:
    enabled: false
    optimization_metric: total_training_time
    
  # Automated stage discovery
  auto_stage_discovery:
    enabled: false
    clustering_method: gradient_similarity
    num_clusters: 4

# Notes
notes: |
  Multi-Stage Training Configuration for AG News Text Classification:
  
  Stage progression:
  1. Feature extraction: Train top layers only (5 epochs)
  2. Deep features: Unfreeze middle layers (5 epochs)
  3. Full fine-tuning: Train entire model (7 epochs)
  4. Optimization: Compress and optimize (3 epochs)
  
  Key benefits:
  - Faster convergence (30-40% time reduction)
  - Better generalization (2-3% accuracy improvement)
  - More stable training (reduced variance)
  - Memory efficient (gradual parameter increase)
  
  Best practices:
  - Start with high learning rate, decay per stage
  - Increase regularization progressively
  - Use knowledge retention between stages
  - Monitor stage transitions carefully
  
  Common patterns:
  - Bottom-up unfreezing for transfer learning
  - Top-down for domain adaptation
  - Mixed for balanced learning
  
  Troubleshooting:
  - Stage failure: Check success criteria
  - No improvement: Adjust learning rates
  - Overfitting: Increase regularization
  - Memory issues: Use gradient checkpointing
