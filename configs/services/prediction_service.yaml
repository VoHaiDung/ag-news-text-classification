# Prediction Service Configuration for AG News Text Classification
# ================================================================================
# This configuration file defines settings for the prediction service including
# model loading, inference parameters, caching strategies, and performance tuning.
#
# References:
#   - Serving Machine Learning Models: A Guide to Architecture, Stream Processing, and APIs (2019)
#   - High Performance Machine Learning Inference at Scale (Google, 2020)
#
# Author: Võ Hải Dũng
# License: MIT

# Service Metadata
service:
  name: "prediction-service"
  version: "1.0.0"
  description: "Text classification prediction service for AG News"
  namespace: "agnews"
  
# Model Configuration
models:
  # Default model settings
  default_model: "deberta-v3-base"
  
  # Model loading configuration
  loading:
    # Model registry path
    registry_path: "${MODEL_REGISTRY_PATH:/models}"
    
    # Preload models at startup
    preload_models:
      - "deberta-v3-base"
      - "roberta-large"
      - "ensemble-voting"
    
    # Lazy loading for other models
    lazy_loading: true
    
    # Model cache settings
    cache:
      max_models_in_memory: 5
      ttl_seconds: 3600
      eviction_policy: "LRU"  # LRU, LFU, FIFO
    
  # Model versioning
  versioning:
    strategy: "semantic"  # semantic, timestamp, incremental
    default_version: "latest"
    allow_version_pinning: true
    
  # Model optimization
  optimization:
    # ONNX conversion
    use_onnx: false
    onnx_opset_version: 14
    
    # Quantization
    quantization:
      enabled: false
      type: "dynamic"  # dynamic, static
      backend: "qnnpack"
      
    # TensorRT optimization (for NVIDIA GPUs)
    tensorrt:
      enabled: false
      precision: "fp16"  # fp32, fp16, int8
      
# Inference Configuration
inference:
  # Batch processing
  batching:
    enabled: true
    max_batch_size: 32
    batch_timeout_ms: 50
    dynamic_batching: true
    
  # Threading configuration
  threading:
    num_threads: 4
    thread_pool_type: "fixed"  # fixed, cached, scheduled
    
  # GPU configuration
  gpu:
    enabled: true
    device_id: 0
    memory_fraction: 0.8
    allow_growth: true
    
  # Timeout settings
  timeouts:
    inference_timeout_ms: 5000
    model_load_timeout_ms: 30000
    
  # Text preprocessing
  preprocessing:
    max_sequence_length: 512
    truncation_strategy: "longest_first"  # longest_first, only_first, only_second
    padding_strategy: "max_length"  # max_length, longest, do_not_pad
    
  # Post-processing
  postprocessing:
    # Confidence calibration
    calibration:
      enabled: true
      method: "platt"  # platt, isotonic, temperature
      
    # Threshold optimization
    threshold_optimization:
      enabled: false
      method: "f1"  # f1, precision, recall
      
    # Output formatting
    output_format:
      include_all_scores: false
      top_k: 3
      min_confidence: 0.01
      
# Caching Configuration
caching:
  # Prediction cache
  prediction_cache:
    enabled: true
    backend: "redis"  # redis, memcached, memory
    
    # Redis configuration
    redis:
      host: "${REDIS_HOST:localhost}"
      port: 6379
      db: 0
      password: "${REDIS_PASSWORD:}"
      
    # Cache key generation
    key_generation:
      include_model_version: true
      hash_algorithm: "xxhash"  # xxhash, md5, sha256
      
    # Cache settings
    ttl_seconds: 300
    max_entries: 10000
    
  # Feature cache
  feature_cache:
    enabled: true
    ttl_seconds: 600
    max_size_mb: 100
    
# Performance Configuration
performance:
  # Request handling
  request_handling:
    max_concurrent_requests: 100
    queue_size: 1000
    rejection_policy: "abort"  # abort, wait, redirect
    
  # Circuit breaker
  circuit_breaker:
    enabled: true
    failure_threshold: 5
    recovery_timeout_seconds: 60
    half_open_requests: 3
    
  # Rate limiting
  rate_limiting:
    enabled: true
    default_rate: 1000
    default_burst: 2000
    per_client_limits:
      enabled: true
      
  # Resource limits
  resource_limits:
    max_memory_mb: 4096
    max_cpu_percent: 80
    
# Monitoring Configuration
monitoring:
  # Metrics collection
  metrics:
    enabled: true
    collection_interval_seconds: 10
    
    # Metrics to collect
    collect:
      - "inference_latency"
      - "model_load_time"
      - "cache_hit_rate"
      - "error_rate"
      - "throughput"
      
  # Logging
  logging:
    level: "INFO"
    format: "json"
    include_request_body: false
    include_response_body: false
    
  # Tracing
  tracing:
    enabled: true
    sampling_rate: 0.1
    exporter: "jaeger"
    
# High Availability Configuration
high_availability:
  # Replication
  replication:
    enabled: false
    replicas: 3
    
  # Health checks
  health_checks:
    liveness_probe:
      enabled: true
      path: "/health/live"
      interval_seconds: 30
      
    readiness_probe:
      enabled: true
      path: "/health/ready"
      interval_seconds: 10
      
  # Failover
  failover:
    enabled: false
    strategy: "active-passive"  # active-passive, active-active
    
# Security Configuration
security:
  # Authentication
  authentication:
    enabled: true
    type: "jwt"  # jwt, api_key, oauth2
    
  # Authorization
  authorization:
    enabled: true
    rbac_enabled: true
    
  # Encryption
  encryption:
    tls_enabled: false
    encrypt_cache: false
    
# Feature Flags
features:
  # Experimental features
  experimental:
    prompt_based_classification: false
    multi_modal_support: false
    streaming_inference: false
    
  # A/B testing
  ab_testing:
    enabled: false
    experiments: []
