# Phase 4: Final SOTA Configuration
# Combining all techniques for maximum performance

name: phase4_final_sota
type: experiment
description: "Final push for 96%+ accuracy with all optimizations"

# Inheritance chain
inherit:
  - experiments/sota_attempts/phase3_dapt.yaml
  - training/advanced/adversarial_training.yaml
  - training/advanced/knowledge_distillation.yaml

# Experiment metadata
metadata:
  phase: 4
  target_accuracy: 0.965
  target_f1_macro: 0.964
  expected_duration_hours: 72
  gpu_requirements: "8x A100 80GB"
  
  tags:
    - sota
    - final
    - production
    - ensemble
    - multi-stage

# Multi-stage training pipeline
training_pipeline:
  # Stage 1: Domain-Adaptive Pretraining
  stage1_dapt:
    enabled: true
    duration_hours: 24
    
    corpus:
      - source: "news_crawl"
        size: 10000000
      - source: "cc_news"
        size: 5000000
      - source: "reddit_news"
        size: 2000000
        
    mlm_probability: 0.15
    learning_rate: 5e-5
    batch_size: 256
    gradient_accumulation: 8
    
    models:
      - deberta-v3-xlarge
      - roberta-large
      - xlnet-large
      
  # Stage 2: Task-Adaptive Pretraining
  stage2_tapt:
    enabled: true
    duration_hours: 12
    
    use_augmented_data: true
    augmentation_factor: 3
    learning_rate: 2e-5
    
  # Stage 3: Supervised Fine-tuning
  stage3_fine_tuning:
    enabled: true
    duration_hours: 18
    
    # Progressive fine-tuning
    progressive:
      - subset: 0.25
        epochs: 3
        lr: 5e-5
      - subset: 0.5
        epochs: 3
        lr: 3e-5
      - subset: 1.0
        epochs: 4
        lr: 2e-5
        
    # Multi-task learning
    auxiliary_tasks:
      - task: "domain_classification"
        weight: 0.1
      - task: "sentiment_analysis"
        weight: 0.1
        
  # Stage 4: Adversarial Training
  stage4_adversarial:
    enabled: true
    duration_hours: 6
    
    methods: ["freelb", "smart"]
    epsilon: 1.0
    alpha: 0.5
    steps: 3
    
  # Stage 5: Ensemble Training
  stage5_ensemble:
    enabled: true
    duration_hours: 12
    
    ensemble_method: "stacking"
    meta_learner: "xgboost"
    
    # Knowledge distillation from ensemble
    distill_to_single: true
    student_model: "deberta-v3-large"
    distillation_epochs: 5

# Model configurations
models:
  # Primary model: DeBERTa-v3-xlarge
  primary:
    name: "microsoft/deberta-v3-xlarge"
    config_override:
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      classifier_dropout: 0.2
      
    # Custom modifications
    modifications:
      - type: "multi_sample_dropout"
        num_samples: 5
      - type: "mixout"
        probability: 0.1
      - type: "layer_wise_attention"
        
  # Secondary models for ensemble
  secondary:
    - name: "roberta-large"
      weight: 0.25
    - name: "xlnet-large-cased"
      weight: 0.20
    - name: "electra-large-discriminator"
      weight: 0.10
    - name: "albert-xxlarge-v2"
      weight: 0.05

# Advanced training techniques
advanced_techniques:
  # Curriculum learning
  curriculum:
    enabled: true
    strategy: "combined"
    
    difficulty_scoring:
      - metric: "length"
        weight: 0.2
      - metric: "perplexity"
        weight: 0.3
      - metric: "model_confidence"
        weight: 0.5
        
    schedule:
      - phase: "easy"
        epochs: 2
        percentage: 0.3
      - phase: "medium"
        epochs: 3
        percentage: 0.5
      - phase: "hard"
        epochs: 5
        percentage: 1.0
        
  # Pseudo-labeling
  pseudo_labeling:
    enabled: true
    
    unlabeled_data:
      - source: "news_crawl_2024"
        size: 100000
        
    confidence_threshold: 0.95
    agreement_threshold: 0.9
    
    iterative_rounds: 3
    
  # Contrastive learning
  contrastive:
    enabled: true
    
    temperature: 0.07
    projection_dim: 128
    loss_weight: 0.1
    
    augmentation_strategies:
      - "dropout"
      - "token_cutoff"
      - "feature_cutoff"
      
  # Sharpness-Aware Minimization (SAM)
  sam:
    enabled: true
    rho: 0.05
    adaptive: true
    
  # Stochastic Weight Averaging (SWA)
  swa:
    enabled: true
    start_epoch: 6
    lr: 1e-5
    anneal_epochs: 2

# Data configuration
data:
  # Training data sources
  train_sources:
    - name: "ag_news_train"
      weight: 1.0
    - name: "ag_news_augmented"
      weight: 0.5
    - name: "external_news"
      weight: 0.3
    - name: "pseudo_labeled"
      weight: 0.2
      
  # Augmentation pipeline
  augmentation_pipeline:
    - method: "back_translation"
      probability: 0.3
      languages: ["de", "fr", "es", "zh", "ja"]
    - method: "paraphrase"
      probability: 0.3
      model: "tuner007/pegasus_paraphrase"
    - method: "token_replacement"
      probability: 0.2
    - method: "mixup"
      probability: 0.2
      alpha: 0.2
      
  # Preprocessing
  preprocessing:
    max_length: 512
    sliding_window: true
    stride: 128
    
  # Sampling strategy
  sampling:
    strategy: "weighted"
    class_weights: "balanced"
    
    # Dynamic sampling
    dynamic_sampling: true
    update_frequency: 500

# Optimization configuration
optimization:
  # Optimizer
  optimizer:
    name: "adamw"
    learning_rate: 2e-5
    weight_decay: 0.01
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1e-8
    
    # Layer-wise learning rate decay
    layer_decay: 0.95
    
    # Differential learning rates
    differential_lr:
      embeddings: 1e-5
      encoder: 2e-5
      classifier: 5e-5
      
  # Learning rate schedule
  scheduler:
    name: "cosine_with_restarts"
    warmup_ratio: 0.1
    num_cycles: 2
    
  # Gradient management
  gradient:
    max_norm: 1.0
    accumulation_steps: 4
    
    # Gradient surgery
    gradient_surgery: true
    
    # Gradient noise
    add_noise: true
    noise_gamma: 0.55

# Hardware optimization
hardware:
  # Distributed training
  distributed:
    strategy: "ddp"
    backend: "nccl"
    num_nodes: 2
    gpus_per_node: 4
    
  # Mixed precision
  mixed_precision:
    enabled: true
    opt_level: "O2"
    loss_scale: "dynamic"
    
  # Memory optimization
  memory:
    gradient_checkpointing: true
    cpu_offload: false
    optimizer_offload: false
    
    # ZeRO optimization
    zero_optimization:
      stage: 2
      
  # Compilation (PyTorch 2.0+)
  compile:
    enabled: true
    mode: "max-autotune"
    backend: "inductor"

# Evaluation strategy
evaluation:
  # Metrics
  metrics:
    - accuracy
    - f1_macro
    - f1_micro
    - f1_weighted
    - precision_macro
    - recall_macro
    - matthews_corrcoef
    - cohen_kappa
    
  # Evaluation frequency
  eval_steps: 100
  save_steps: 200
  
  # Best model selection
  metric_for_best_model: "f1_macro"
  greater_is_better: true
  
  # Test-time augmentation
  tta:
    enabled: true
    num_augmentations: 5
    aggregation: "mean"
    
  # Ensemble evaluation
  ensemble_eval:
    enabled: true
    voting: "soft"
    optimize_weights: true
    
  # Robustness evaluation
  robustness:
    adversarial_attacks: ["textfooler", "bert-attack"]
    contrast_sets: true
    out_of_distribution: true

# Post-processing
post_processing:
  # Confidence calibration
  calibration:
    method: "temperature_scaling"
    validation_split: 0.1
    
  # Threshold optimization
  threshold_optimization:
    enabled: true
    metric: "f1_macro"
    search_space: [0.3, 0.7]
    
  # Output ensemble
  ensemble_predictions:
    method: "weighted_average"
    weights: "optimized"

# Ablation studies
ablation:
  components_to_test:
    - "dapt"
    - "adversarial"
    - "ensemble"
    - "curriculum"
    - "pseudo_labeling"
    - "contrastive"
    
  metrics_to_track:
    - "accuracy_delta"
    - "training_time"
    - "inference_time"

# Expected results
expected_results:
  accuracy: 0.966
  f1_macro: 0.965
  precision: 0.965
  recall: 0.965
  
  # Per-class performance
  class_accuracy:
    World: 0.97
    Sports: 0.98
    Business: 0.95
    Sci_Tech: 0.94
    
  # Robustness
  adversarial_accuracy: 0.93
  contrast_set_accuracy: 0.92
  
  # Efficiency
  inference_time_ms: 25
  model_size_mb: 1500

# Notes
notes: |
  Phase 4 Final SOTA Configuration:
  
  1. Multi-Stage Training Pipeline:
     - DAPT on 17M news articles (24h)
     - TAPT on augmented AG News (12h)
     - Progressive fine-tuning (18h)
     - Adversarial training (6h)
     - Ensemble optimization (12h)
     
  2. Model Architecture:
     - Primary: DeBERTa-v3-xlarge with modifications
     - Ensemble: 5 models with optimized weights
     - Student: Distilled DeBERTa-v3-large
     
  3. Advanced Techniques:
     - Curriculum learning with multiple metrics
     - SAM + SWA for better generalization
     - Contrastive learning for representations
     - Multi-task learning with auxiliary tasks
     
  4. Data Strategy:
     - 3x augmentation through multiple methods
     - Pseudo-labeling on 100K unlabeled samples
     - Dynamic sampling based on difficulty
     
  5. Expected Performance:
     - Accuracy: 96.6%
     - F1-macro: 96.5%
     - Robust to adversarial attacks
     - Fast inference (25ms)
     
  6. Computational Requirements:
     - 72 hours total training
     - 8x A100 80GB GPUs
     - ~640 GPU-hours
     
  7. Key Innovations:
     - Combined DAPT + TAPT + Fine-tuning
     - Multi-method augmentation pipeline
     - Progressive training with curriculum
     - Ensemble distillation for efficiency

references:
  - dapt: "https://arxiv.org/abs/2004.10964"
  - sam: "https://arxiv.org/abs/2010.01412"
  - swa: "https://arxiv.org/abs/1803.05407"
  - curriculum: "https://arxiv.org/abs/0904.3095"
  - contrastive: "https://arxiv.org/abs/2002.05709"
