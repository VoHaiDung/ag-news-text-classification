# Model Size Ablation Study Configuration
# ========================================
#
# This configuration conducts systematic ablation studies on model size impact,
# following methodology from:
# - Kaplan et al. (2020): "Scaling Laws for Neural Language Models"
# - Hoffmann et al. (2022): "Training Compute-Optimal Large Language Models"
# - Tay et al. (2022): "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"
#
# Author: Võ Hải Dũng
# License: MIT

name: model_size_ablation
type: experiment
subtype: ablation
description: "Systematic study of model size impact on AG News classification performance"

# Experimental methodology
methodology:
  # Ablation design
  ablation_design:
    variable: "model_size"
    control_variables:
      - training_data
      - hyperparameters
      - evaluation_protocol
    
  # Statistical rigor
  statistical_analysis:
    num_seeds: 5
    significance_test: "paired_t_test"
    confidence_level: 0.95
    
  # Scaling analysis
  scaling_analysis:
    fit_power_law: true
    compute_efficiency_frontier: true

# Model size configurations
model_sizes:
  # BERT family
  bert_family:
    bert_tiny:
      model_name: "google/bert_uncased_L-2_H-128_A-2"
      num_parameters: 4.4e6
      hidden_size: 128
      num_layers: 2
      num_heads: 2
      
    bert_mini:
      model_name: "google/bert_uncased_L-4_H-256_A-4"
      num_parameters: 11.3e6
      hidden_size: 256
      num_layers: 4
      num_heads: 4
      
    bert_small:
      model_name: "google/bert_uncased_L-4_H-512_A-8"
      num_parameters: 29.1e6
      hidden_size: 512
      num_layers: 4
      num_heads: 8
      
    bert_medium:
      model_name: "google/bert_uncased_L-8_H-512_A-8"
      num_parameters: 41.7e6
      hidden_size: 512
      num_layers: 8
      num_heads: 8
      
    bert_base:
      model_name: "bert-base-uncased"
      num_parameters: 110e6
      hidden_size: 768
      num_layers: 12
      num_heads: 12
      
    bert_large:
      model_name: "bert-large-uncased"
      num_parameters: 340e6
      hidden_size: 1024
      num_layers: 24
      num_heads: 16
      
  # RoBERTa family
  roberta_family:
    distilroberta:
      model_name: "distilroberta-base"
      num_parameters: 82e6
      hidden_size: 768
      num_layers: 6
      num_heads: 12
      
    roberta_base:
      model_name: "roberta-base"
      num_parameters: 125e6
      hidden_size: 768
      num_layers: 12
      num_heads: 12
      
    roberta_large:
      model_name: "roberta-large"
      num_parameters: 355e6
      hidden_size: 1024
      num_layers: 24
      num_heads: 16
      
  # DeBERTa family
  deberta_family:
    deberta_v3_small:
      model_name: "microsoft/deberta-v3-small"
      num_parameters: 44e6
      hidden_size: 768
      num_layers: 6
      num_heads: 12
      
    deberta_v3_base:
      model_name: "microsoft/deberta-v3-base"
      num_parameters: 86e6
      hidden_size: 768
      num_layers: 12
      num_heads: 12
      
    deberta_v3_large:
      model_name: "microsoft/deberta-v3-large"
      num_parameters: 304e6
      hidden_size: 1024
      num_layers: 24
      num_heads: 16
      
    deberta_v3_xlarge:
      model_name: "microsoft/deberta-v3-xlarge"
      num_parameters: 750e6
      hidden_size: 1536
      num_layers: 48
      num_heads: 24

# Training configuration
training:
  # Fixed hyperparameters across all sizes
  fixed_params:
    num_epochs: 10
    warmup_ratio: 0.1
    weight_decay: 0.01
    max_length: 256
    
  # Size-dependent parameters
  size_dependent:
    # Batch size scaling
    batch_size:
      tiny: 128
      mini: 64
      small: 32
      medium: 24
      base: 16
      large: 8
      xlarge: 4
      
    # Learning rate scaling
    learning_rate:
      tiny: 5e-5
      mini: 4e-5
      small: 3e-5
      medium: 2e-5
      base: 2e-5
      large: 1e-5
      xlarge: 1e-5
      
    # Gradient accumulation
    gradient_accumulation:
      tiny: 1
      mini: 1
      small: 2
      medium: 2
      base: 4
      large: 8
      xlarge: 16

# Metrics to track
metrics:
  # Performance metrics
  performance:
    - accuracy
    - f1_macro
    - precision
    - recall
    - loss
    
  # Efficiency metrics
  efficiency:
    - training_time
    - inference_time
    - memory_usage
    - flops
    - parameters_count
    
  # Scaling metrics
  scaling:
    - performance_per_parameter
    - performance_per_flop
    - performance_per_training_hour
    - pareto_efficiency

# Ablation experiments
experiments:
  # Experiment 1: Full model comparison
  full_comparison:
    models: "all"
    training_data: "full"
    
    expected_results:
      bert_tiny: 0.88
      bert_base: 0.94
      bert_large: 0.945
      roberta_base: 0.945
      roberta_large: 0.95
      deberta_v3_base: 0.95
      deberta_v3_xlarge: 0.964
      
  # Experiment 2: Efficiency frontier
  efficiency_frontier:
    models: ["tiny", "small", "base", "large"]
    
    metrics_to_optimize:
      - accuracy_vs_parameters
      - accuracy_vs_inference_time
      - accuracy_vs_training_cost
      
  # Experiment 3: Layer ablation
  layer_ablation:
    base_model: "bert-base"
    
    layer_configs:
      - num_layers: 1
      - num_layers: 3
      - num_layers: 6
      - num_layers: 9
      - num_layers: 12
      
  # Experiment 4: Hidden size ablation
  hidden_size_ablation:
    base_model: "bert-base"
    
    hidden_sizes:
      - 128
      - 256
      - 384
      - 512
      - 768
      
  # Experiment 5: Attention head ablation
  attention_head_ablation:
    base_model: "bert-base"
    
    num_heads:
      - 1
      - 2
      - 4
      - 8
      - 12
      - 16

# Analysis configuration
analysis:
  # Scaling law fitting
  scaling_laws:
    # Fit power law: L = α * N^β
    fit_performance_vs_parameters: true
    fit_loss_vs_parameters: true
    
    # Chinchilla scaling
    compute_optimal_size_for_budget: true
    
  # Statistical analysis
  statistical:
    # Correlation analysis
    correlations:
      - parameters_vs_accuracy
      - layers_vs_accuracy
      - hidden_size_vs_accuracy
      
    # Significance testing
    pairwise_comparisons: true
    
  # Visualization
  plots:
    - accuracy_vs_parameters
    - efficiency_frontier
    - scaling_curves
    - layer_importance
    - pareto_frontier

# Resource tracking
resources:
  # GPU memory tracking
  track_gpu_memory: true
  
  # Training cost estimation
  estimate_costs:
    gpu_hour_cost: 1.0  # USD per hour
    
  # Carbon footprint
  track_carbon: true

# Expected insights
expected_insights:
  # Performance scaling
  performance:
    - "Logarithmic improvement with model size"
    - "Diminishing returns beyond 350M parameters"
    - "DeBERTa most parameter-efficient"
    
  # Efficiency insights
  efficiency:
    - "Sweet spot at base size (110-125M params)"
    - "Large models not worth 2x compute for 0.5% gain"
    - "Distilled models best for production"
    
  # Architecture insights
  architecture:
    - "Depth more important than width"
    - "12 layers sufficient for AG News"
    - "Attention heads show redundancy beyond 8"

# Timeline
timeline:
  total_hours: 48
  
  phases:
    - name: "Small models (<50M)"
      hours: 6
      
    - name: "Base models (50-150M)"
      hours: 12
      
    - name: "Large models (150-400M)"
      hours: 18
      
    - name: "XLarge models (>400M)"
      hours: 8
      
    - name: "Analysis"
      hours: 4

# Notes
notes: |
  Model Size Ablation Study
  
  Objectives:
  1. Understand performance scaling with model size
  2. Find optimal size-performance trade-off
  3. Analyze component importance (layers, heads, hidden size)
  
  Key Findings Expected:
  - Performance scales as O(log N) with parameters
  - 90% of performance achievable with 10% of parameters
  - Base models (110-125M) optimal for production
  - Larger models show diminishing returns
  
  Practical Recommendations:
  - Use DistilRoBERTa for real-time (82M params, 94.3%)
  - Use RoBERTa-base for balanced (125M params, 94.5%)
  - Use DeBERTa-v3-large only if 0.5% matters (304M params, 95.8%)

references:
  - scaling_laws: "https://arxiv.org/abs/2001.08361"
  - chinchilla: "https://arxiv.org/abs/2203.15556"
  - efficient_transformers: "https://arxiv.org/abs/2109.02846"
