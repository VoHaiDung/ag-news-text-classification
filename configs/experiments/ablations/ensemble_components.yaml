# Ensemble Components Ablation Study Configuration
# ================================================
#
# This configuration analyzes the contribution of individual components in ensemble models,
# following methodologies from:
# - Dietterich (2000): "Ensemble Methods in Machine Learning"
# - Sagi & Rokach (2018): "Ensemble Learning: A Survey"
# - Dong et al. (2020): "A Survey on Ensemble Learning"
#
# Author: Võ Hải Dũng
# License: MIT

name: ensemble_components_ablation
type: experiment
subtype: ablation
description: "Systematic ablation of ensemble components to understand their contributions"

# Experimental methodology
methodology:
  # Ablation strategy
  ablation_strategy:
    approach: "leave_one_out"
    baseline: "full_ensemble"
    granularity: ["model_level", "technique_level", "layer_level"]
    
  # Statistical rigor
  statistical_rigor:
    num_seeds: 5
    bootstrap_samples: 1000
    significance_testing: true
    effect_size_measurement: true
    
  # Component interaction analysis
  interaction_analysis:
    measure_pairwise: true
    measure_higher_order: true
    synergy_metrics: ["shapley_value", "interaction_index"]

# Ensemble configurations
ensemble_configurations:
  # Full ensemble (baseline)
  full_ensemble:
    name: "complete_ensemble"
    type: "heterogeneous"
    
    components:
      # Transformer models
      deberta_v3:
        model: "microsoft/deberta-v3-large"
        weight: 0.25
        role: "primary"
        
      roberta:
        model: "roberta-large"
        weight: 0.20
        role: "primary"
        
      xlnet:
        model: "xlnet-large-cased"
        weight: 0.15
        role: "complementary"
        
      electra:
        model: "google/electra-large-discriminator"
        weight: 0.15
        role: "complementary"
        
      # Specialized models
      longformer:
        model: "allenai/longformer-large-4096"
        weight: 0.10
        role: "long_context"
        
      # Efficient models
      distilbert:
        model: "distilbert-base-uncased"
        weight: 0.10
        role: "fast_inference"
        
      # Domain-adapted
      news_bert:
        model: "domain_adapted_bert"
        weight: 0.05
        role: "domain_specific"
        
  # Voting ensemble variants
  voting_ensemble:
    soft_voting:
      name: "soft_voting"
      aggregation: "weighted_average"
      
    hard_voting:
      name: "hard_voting"
      aggregation: "majority"
      
    rank_voting:
      name: "rank_voting"
      aggregation: "borda_count"
      
  # Stacking ensemble variants
  stacking_ensemble:
    linear_meta:
      name: "linear_stacking"
      meta_learner: "logistic_regression"
      
    nonlinear_meta:
      name: "nonlinear_stacking"
      meta_learner: "xgboost"
      
    neural_meta:
      name: "neural_stacking"
      meta_learner: "feedforward_nn"
      
  # Blending ensemble
  blending_ensemble:
    static_blending:
      name: "static_blend"
      blend_ratio: "fixed"
      
    dynamic_blending:
      name: "dynamic_blend"
      blend_ratio: "learned"

# Ablation experiments
ablation_experiments:
  # Model-level ablations
  model_level:
    description: "Remove one model at a time"
    
    ablations:
      - remove: "deberta_v3"
        name: "without_deberta"
        expected_impact: -0.015
        
      - remove: "roberta"
        name: "without_roberta"
        expected_impact: -0.012
        
      - remove: "xlnet"
        name: "without_xlnet"
        expected_impact: -0.008
        
      - remove: "electra"
        name: "without_electra"
        expected_impact: -0.007
        
      - remove: "longformer"
        name: "without_longformer"
        expected_impact: -0.004
        
      - remove: "distilbert"
        name: "without_distilbert"
        expected_impact: -0.003
        
      - remove: "news_bert"
        name: "without_domain"
        expected_impact: -0.002
        
  # Architecture-level ablations
  architecture_level:
    description: "Remove architectural components"
    
    ablations:
      - remove: "attention_pooling"
        name: "without_attention_pool"
        
      - remove: "multi_head_ensemble"
        name: "without_multi_head"
        
      - remove: "hierarchical_aggregation"
        name: "without_hierarchy"
        
      - remove: "uncertainty_weighting"
        name: "without_uncertainty"
        
  # Technique-level ablations
  technique_level:
    description: "Remove ensemble techniques"
    
    ablations:
      - remove: "weighted_averaging"
        replace_with: "simple_averaging"
        
      - remove: "meta_learning"
        replace_with: "direct_voting"
        
      - remove: "cross_validation"
        replace_with: "single_split"
        
      - remove: "bootstrap_aggregation"
        replace_with: "single_training"
        
  # Feature-level ablations
  feature_level:
    description: "Remove feature components"
    
    ablations:
      - remove: "model_confidence"
        name: "without_confidence"
        
      - remove: "model_uncertainty"
        name: "without_uncertainty"
        
      - remove: "model_agreement"
        name: "without_agreement"
        
      - remove: "diversity_features"
        name: "without_diversity"

# Component interaction analysis
interaction_analysis:
  # Pairwise interactions
  pairwise_interactions:
    measure_all_pairs: true
    
    key_pairs:
      - [deberta_v3, roberta]
      - [xlnet, electra]
      - [transformer, domain_adapted]
      
  # Synergy analysis
  synergy_analysis:
    # Shapley values for contribution
    shapley_values:
      compute: true
      sampling_method: "monte_carlo"
      num_samples: 1000
      
    # Interaction indices
    interaction_indices:
      compute: true
      order: 3
      
  # Diversity analysis
  diversity_analysis:
    metrics:
      - disagreement_measure
      - q_statistic
      - correlation_coefficient
      - double_fault_measure
      - entropy_measure

# Performance analysis
performance_analysis:
  # Metrics to track
  metrics:
    primary:
      - accuracy
      - f1_macro
      
    secondary:
      - precision
      - recall
      - auc_roc
      
    ensemble_specific:
      - ensemble_diversity
      - component_agreement
      - prediction_confidence
      - uncertainty_quantification
      
  # Efficiency analysis
  efficiency:
    - inference_time
    - memory_usage
    - model_size
    - throughput
    
  # Robustness analysis
  robustness:
    - performance_variance
    - worst_case_accuracy
    - adversarial_robustness
    - out_of_distribution

# Optimization experiments
optimization_experiments:
  # Weight optimization
  weight_optimization:
    description: "Find optimal component weights"
    
    methods:
      - grid_search
      - bayesian_optimization
      - evolutionary_algorithm
      
    constraints:
      sum_to_one: true
      non_negative: true
      
  # Component selection
  component_selection:
    description: "Select optimal subset of models"
    
    methods:
      - greedy_forward
      - greedy_backward
      - genetic_algorithm
      
    criteria:
      - performance
      - efficiency
      - diversity
      
  # Architecture search
  architecture_search:
    description: "Find optimal ensemble architecture"
    
    search_space:
      - aggregation_method
      - meta_learner_type
      - layer_connections

# Expected results
expected_results:
  # Component importance ranking
  importance_ranking:
    1: "deberta_v3 (25% contribution)"
    2: "roberta (20% contribution)"
    3: "xlnet (12% contribution)"
    4: "electra (10% contribution)"
    5: "longformer (8% contribution)"
    6: "distilbert (5% contribution)"
    7: "news_bert (3% contribution)"
    
  # Synergy effects
  synergy_effects:
    positive:
      - "deberta + roberta: +2% over sum"
      - "transformers + domain: +1.5% over sum"
      
    negative:
      - "xlnet + electra: -0.5% redundancy"
      
  # Optimal configurations
  optimal_configs:
    performance_only:
      models: ["deberta_v3", "roberta", "xlnet", "electra"]
      accuracy: 0.962
      
    balanced:
      models: ["deberta_v3", "roberta", "distilbert"]
      accuracy: 0.958
      inference_time: "50ms"
      
    efficient:
      models: ["distilbert", "news_bert"]
      accuracy: 0.945
      inference_time: "20ms"

# Visualization and reporting
visualization:
  # Plots to generate
  plots:
    - ablation_impact_chart
    - component_contribution_pie
    - interaction_heatmap
    - diversity_correlation_matrix
    - performance_efficiency_frontier
    
  # Tables to generate
  tables:
    - component_ablation_results
    - shapley_values_table
    - optimal_weights_table
    - synergy_effects_table

# Resource requirements
resources:
  compute:
    gpu_hours: 72
    num_gpus: 4
    gpu_type: "V100"
    
  storage:
    model_checkpoints: "100GB"
    predictions_cache: "20GB"
    analysis_results: "5GB"

# Timeline
timeline:
  total_days: 7
  
  phases:
    - name: "Setup and baseline"
      days: 1
      
    - name: "Model-level ablations"
      days: 2
      
    - name: "Technique-level ablations"
      days: 2
      
    - name: "Interaction analysis"
      days: 1
      
    - name: "Optimization and reporting"
      days: 1

# Notes
notes: |
  Ensemble Components Ablation Study
  
  Objectives:
  1. Quantify individual model contributions
  2. Identify synergistic combinations
  3. Find minimal effective ensemble
  4. Optimize component weights
  
  Key Questions:
  - Which models are essential vs redundant?
  - What interactions exist between models?
  - Can we achieve 95% performance with 50% models?
  - What's the performance-efficiency tradeoff?
  
  Expected Findings:
  - Top 3 models provide 85% of ensemble benefit
  - Strong synergy between complementary architectures
  - Domain-adapted models provide unique value
  - Optimal ensemble has 3-4 diverse models
  
  Practical Implications:
  - Production ensemble can be simplified
  - Focus resources on top contributors
  - Maintain diversity over redundancy
  - Consider efficiency constraints

references:
  - ensemble_methods: "https://link.springer.com/chapter/10.1007/3-540-45014-9_1"
  - ensemble_survey: "https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1249"
  - diversity_measures: "https://link.springer.com/article/10.1007/s10994-006-7364-1"
