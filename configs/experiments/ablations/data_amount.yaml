# Data Amount Ablation Study Configuration
# =========================================
#
# This configuration studies the impact of training data size on model performance,
# following methodology from:
# - Banko & Brill (2001): "Scaling to Very Very Large Corpora for Natural Language Disambiguation"
# - Sun et al. (2017): "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"
# - Kaplan et al. (2020): "Scaling Laws for Neural Language Models"
#
# Author: Võ Hải Dũng
# License: MIT

name: data_amount_ablation
type: experiment
subtype: ablation
description: "Study impact of training data size on AG News classification performance"

# Experimental methodology
methodology:
  # Ablation design
  ablation_design:
    variable: "training_data_size"
    sampling_strategy: "stratified"
    maintain_class_balance: true
    
  # Statistical rigor
  statistical_analysis:
    num_seeds: 5
    num_sampling_iterations: 3
    confidence_intervals: true
    
  # Learning curve analysis
  learning_curve:
    fit_power_law: true
    extrapolate_performance: true

# Data size configurations
data_sizes:
  # Absolute numbers
  absolute_sizes:
    - size: 10
      name: "10_examples"
      per_class: 2.5
      
    - size: 50
      name: "50_examples"
      per_class: 12.5
      
    - size: 100
      name: "100_examples"
      per_class: 25
      
    - size: 500
      name: "500_examples"
      per_class: 125
      
    - size: 1000
      name: "1k_examples"
      per_class: 250
      
    - size: 5000
      name: "5k_examples"
      per_class: 1250
      
    - size: 10000
      name: "10k_examples"
      per_class: 2500
      
    - size: 20000
      name: "20k_examples"
      per_class: 5000
      
    - size: 50000
      name: "50k_examples"
      per_class: 12500
      
    - size: 96000
      name: "full_train"
      per_class: 24000
      
  # Percentage-based sizes
  percentage_sizes:
    - percentage: 0.01
      name: "1_percent"
      
    - percentage: 0.05
      name: "5_percent"
      
    - percentage: 0.1
      name: "10_percent"
      
    - percentage: 0.25
      name: "25_percent"
      
    - percentage: 0.5
      name: "50_percent"
      
    - percentage: 0.75
      name: "75_percent"
      
    - percentage: 1.0
      name: "100_percent"

# Models to test
models:
  # Classical ML baseline
  logistic_regression:
    name: "LogisticRegression"
    type: "classical"
    
    features: "tfidf"
    max_features: 10000
    
  # Neural network
  text_cnn:
    name: "TextCNN"
    type: "neural"
    
    embedding_dim: 300
    filter_sizes: [3, 4, 5]
    num_filters: 100
    
  # Small transformer
  distilbert:
    name: "distilbert-base"
    type: "transformer"
    
    num_parameters: 66e6
    max_length: 256
    
  # Base transformer
  roberta_base:
    name: "roberta-base"
    type: "transformer"
    
    num_parameters: 125e6
    max_length: 256
    
  # Large transformer
  deberta_large:
    name: "microsoft/deberta-v3-large"
    type: "transformer"
    
    num_parameters: 304e6
    max_length: 256

# Training configuration
training:
  # Fixed across all data sizes
  fixed_params:
    num_epochs: 20
    early_stopping_patience: 3
    validation_size: 10000  # Fixed validation
    test_size: 10000  # Fixed test
    
  # Data-size dependent
  size_dependent:
    # Batch size (smaller for small data)
    batch_size:
      10: 2
      50: 4
      100: 8
      500: 16
      1000: 32
      default: 64
      
    # Learning rate (higher for small data)
    learning_rate:
      10: 5e-5
      50: 3e-5
      100: 2e-5
      default: 1e-5
      
    # Regularization (stronger for small data)
    dropout:
      10: 0.5
      50: 0.4
      100: 0.3
      500: 0.2
      default: 0.1

# Experiments
experiments:
  # Experiment 1: Learning curves
  learning_curves:
    description: "Performance vs data size for each model"
    
    expected_results:
      logistic_regression:
        100: 0.75
        1000: 0.85
        10000: 0.90
        full: 0.918
        
      text_cnn:
        100: 0.70
        1000: 0.83
        10000: 0.89
        full: 0.915
        
      distilbert:
        100: 0.78
        1000: 0.88
        10000: 0.92
        full: 0.935
        
      roberta_base:
        100: 0.80
        1000: 0.90
        10000: 0.93
        full: 0.945
        
      deberta_large:
        100: 0.82
        1000: 0.91
        10000: 0.94
        full: 0.958
        
  # Experiment 2: Sample efficiency
  sample_efficiency:
    description: "Data needed to reach performance thresholds"
    
    thresholds:
      - accuracy: 0.85
        logistic_regression: 1000
        text_cnn: 2000
        distilbert: 500
        roberta_base: 300
        
      - accuracy: 0.90
        logistic_regression: 10000
        text_cnn: 15000
        distilbert: 3000
        roberta_base: 1000
        
      - accuracy: 0.93
        logistic_regression: "not_achievable"
        text_cnn: "not_achievable"
        distilbert: 20000
        roberta_base: 10000
        
  # Experiment 3: Few-shot performance
  few_shot:
    description: "Performance with very limited data"
    
    shots_per_class: [1, 5, 10, 20, 50]
    
    expected_results:
      1_shot:
        logistic_regression: 0.35
        roberta_base: 0.55
        
      5_shot:
        logistic_regression: 0.60
        roberta_base: 0.75
        
      10_shot:
        logistic_regression: 0.70
        roberta_base: 0.82
        
  # Experiment 4: Class imbalance
  class_imbalance:
    description: "Impact of imbalanced data"
    
    imbalance_ratios:
      - ratio: [1, 1, 1, 1]  # Balanced
      - ratio: [2, 1, 1, 1]  # Mild imbalance
      - ratio: [5, 1, 1, 1]  # Strong imbalance
      - ratio: [10, 1, 1, 1]  # Extreme imbalance

# Analysis
analysis:
  # Learning curve fitting
  curve_fitting:
    # Power law: accuracy = a - b * n^(-c)
    models:
      - power_law
      - logarithmic
      - exponential
      
    # Extrapolation
    extrapolate_to: [200000, 500000, 1000000]
    
  # Statistical tests
  statistical_tests:
    # Variance analysis
    anova: true
    
    # Pairwise comparisons
    tukey_hsd: true
    
    # Correlation analysis
    spearman_correlation: true
    
  # Visualizations
  plots:
    - learning_curves_all_models
    - sample_efficiency_comparison
    - few_shot_performance
    - extrapolation_predictions
    - variance_by_data_size

# Metrics
metrics:
  # Performance metrics
  performance:
    - accuracy
    - f1_macro
    - precision
    - recall
    
  # Efficiency metrics
  efficiency:
    - samples_to_convergence
    - training_time_per_sample
    - data_efficiency_score
    
  # Stability metrics
  stability:
    - variance_across_seeds
    - confidence_interval_width
    - worst_case_performance

# Expected insights
insights:
  # Data scaling insights
  scaling:
    - "Transformers benefit more from data than classical ML"
    - "Diminishing returns after 50k examples"
    - "Log-linear relationship between data and performance"
    
  # Model comparison
  comparison:
    - "Classical ML better with <100 examples"
    - "Transformers superior with >1000 examples"
    - "Larger models more data-hungry"
    
  # Practical recommendations
  recommendations:
    - "Minimum 1000 examples for transformers"
    - "5000 examples reach 90% of full performance"
    - "Use classical ML for <100 examples"

# Timeline
timeline:
  total_hours: 36
  
  phases:
    - name: "Small data experiments (<1k)"
      hours: 8
      
    - name: "Medium data experiments (1k-10k)"
      hours: 12
      
    - name: "Large data experiments (>10k)"
      hours: 12
      
    - name: "Analysis and curve fitting"
      hours: 4

# Notes
notes: |
  Data Amount Ablation Study
  
  Objectives:
  1. Understand data requirements for different model types
  2. Establish learning curves and scaling laws
  3. Find minimum data for acceptable performance
  
  Key Questions:
  - How much data needed for 90% of max performance?
  - Which models are most data-efficient?
  - What's the ROI of collecting more data?
  
  Expected Findings:
  - Classical ML plateaus at ~10k examples
  - Transformers continue improving with more data
  - 5k examples achieve 90% of full performance
  - Few-shot transformers outperform classical ML
  
  Practical Guidelines:
  - <100 samples: Use classical ML
  - 100-1000: Use DistilBERT
  - 1000-10000: Use BERT/RoBERTa base
  - >10000: Use large models

references:
  - data_effectiveness: "https://aclanthology.org/P01-1005/"
  - deep_learning_data: "https://arxiv.org/abs/1707.02968"
  - scaling_laws: "https://arxiv.org/abs/2001.08361"
