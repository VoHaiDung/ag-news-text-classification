# Stacking Ensemble with XGBoost Meta-Learner Configuration
# Advanced stacking ensemble for AG News Text Classification

name: stacking_xgboost
type: ensemble
method: stacking

# Base models (Level 0)
base_models:
  - name: deberta_v3_xlarge
    config_path: models/single/deberta_v3_xlarge.yaml
    checkpoint: outputs/models/fine_tuned/deberta_v3_xlarge_best.pt
    use_probabilities: true
    use_hidden_states: true
    hidden_layer: -2  # Second to last layer
    
  - name: roberta_large
    config_path: models/single/roberta_large.yaml
    checkpoint: outputs/models/fine_tuned/roberta_large_best.pt
    use_probabilities: true
    use_hidden_states: false
    
  - name: xlnet_large
    config_path: models/single/xlnet_large.yaml
    checkpoint: outputs/models/fine_tuned/xlnet_large_best.pt
    use_probabilities: true
    use_hidden_states: false
    
  - name: longformer_large
    config_path: models/single/longformer_large.yaml
    checkpoint: outputs/models/fine_tuned/longformer_large_best.pt
    use_probabilities: true
    use_hidden_states: false
    
  - name: t5_large
    config_path: models/single/t5_large.yaml
    checkpoint: outputs/models/fine_tuned/t5_large_best.pt
    use_probabilities: true
    use_hidden_states: false

# Meta-learner configuration (Level 1)
meta_learner:
  type: xgboost
  
  # XGBoost parameters
  xgboost_params:
    # General parameters
    booster: gbtree
    verbosity: 1
    nthread: -1
    
    # Tree parameters
    max_depth: 8
    min_child_weight: 3
    gamma: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    colsample_bylevel: 0.8
    colsample_bynode: 0.8
    
    # Learning parameters
    learning_rate: 0.05
    n_estimators: 500
    
    # Regularization
    reg_alpha: 0.1
    reg_lambda: 1.0
    
    # Objective
    objective: multi:softprob
    num_class: 4
    eval_metric: mlogloss
    
    # Advanced parameters
    tree_method: hist
    grow_policy: lossguide
    max_leaves: 31
    max_bin: 256
    
    # GPU acceleration
    device: cuda
    gpu_id: 0
    
  # Feature engineering for meta-learner
  feature_engineering:
    # Use raw probabilities
    use_probabilities: true
    
    # Statistical features
    add_statistical_features: true
    statistical_features:
      - max_probability
      - min_probability
      - std_probability
      - entropy
      - gini_impurity
      - prediction_margin
    
    # Model agreement features
    add_agreement_features: true
    agreement_features:
      - unanimous_agreement
      - majority_agreement
      - disagreement_count
      - normalized_disagreement
    
    # Confidence features
    add_confidence_features: true
    confidence_features:
      - mean_confidence
      - confidence_variance
      - confidence_entropy
      
    # Class-specific features
    add_class_features: true
    per_class_features:
      - mean_score
      - max_score
      - score_variance
    
    # Hidden state features (if available)
    use_hidden_states: true
    hidden_state_aggregation: mean
    hidden_state_dimension_reduction:
      method: pca
      n_components: 50
  
  # Cross-validation for meta-learner training
  cross_validation:
    strategy: stratified_kfold
    n_folds: 5
    shuffle: true
    random_state: 42
    
    # Out-of-fold predictions
    save_oof_predictions: true
    blend_oof_predictions: true

# Stacking strategy
stacking_strategy:
  # Multi-level stacking
  use_multi_level: false
  num_levels: 2
  
  # Blending holdout
  use_blending_holdout: true
  blending_holdout_size: 0.15
  
  # Cross-validation stacking
  cv_stacking: true
  cv_folds: 5
  
  # Feature selection
  feature_selection:
    enabled: true
    method: mutual_information
    max_features: 50
    
  # Model selection for base models
  base_model_selection:
    method: greedy  # Options: all, greedy, genetic
    min_models: 3
    max_models: 5
    selection_metric: validation_f1

# Training configuration
training:
  # Base model training
  train_base_models: false  # Use pre-trained models
  
  # Meta-learner training
  meta_learner_training:
    early_stopping_rounds: 50
    validation_fraction: 0.2
    
    # Hyperparameter optimization
    optimize_hyperparameters: true
    optimization_method: optuna
    n_trials: 100
    
    # Sample weights
    use_sample_weights: false
    class_weight: balanced
    
  # Ensemble training
  ensemble_iterations: 3
  progressive_training: true

# Optimization
optimization:
  # Feature importance analysis
  analyze_feature_importance: true
  importance_type: gain  # Options: gain, weight, cover, total_gain, total_cover
  
  # Model compression
  compress_meta_learner: false
  compression_method: quantization
  
  # Inference optimization
  optimize_inference: true
  cache_base_predictions: true
  parallel_base_inference: true

# Inference configuration
inference:
  # Batch processing
  batch_size: 64
  max_batch_size: 256
  
  # Prediction strategy
  prediction_strategy: standard  # Options: standard, cascade, dynamic
  
  # Cascade inference (early stopping)
  cascade:
    enabled: false
    confidence_threshold: 0.95
    min_models: 2
  
  # Dynamic model selection
  dynamic_selection:
    enabled: false
    selection_method: uncertainty
    uncertainty_threshold: 0.3
  
  # Post-processing
  calibrate_predictions: true
  calibration_method: isotonic

# Evaluation
evaluation:
  # Standard metrics
  metrics:
    - accuracy
    - f1_macro
    - f1_weighted
    - precision_macro
    - recall_macro
    - matthews_corrcoef
    - roc_auc_ovr
    - log_loss
  
  primary_metric: f1_macro
  
  # Stacking-specific evaluation
  evaluate_base_models: true
  evaluate_meta_learner: true
  evaluate_feature_importance: true
  
  # Cross-validation evaluation
  cv_evaluation: true
  cv_aggregation: mean
  
  # Ablation studies
  ablation_study:
    enabled: true
    remove_one_model: true
    feature_ablation: true

# Hardware requirements
hardware_requirements:
  # Base models requirements
  base_models_gpu_memory_gb: 48
  base_models_ram_gb: 64
  
  # Meta-learner requirements
  meta_learner_gpu_memory_gb: 8
  meta_learner_ram_gb: 16
  
  # Total requirements
  total_gpu_memory_gb: 56
  total_ram_gb: 80
  
  # Parallelization
  num_gpus: 2
  num_cpu_cores: 16

# Deployment
deployment:
  # Model serving
  serving_method: sequential  # Options: sequential, parallel, distributed
  
  # Optimization for deployment
  optimize_for_latency: true
  optimize_for_throughput: false
  
  # Model versioning
  version_base_models: true
  version_meta_learner: true
  
  # Monitoring
  monitor_base_predictions: true
  monitor_meta_predictions: true
  monitor_feature_drift: true

# Model card
model_card:
  description: "Stacking ensemble with XGBoost meta-learner for AG News Text Classification"
  intended_use: "High-accuracy classification with interpretable meta-features"
  architecture: "Two-level stacking: 5 transformer base models + XGBoost meta-learner"
  eval_results:
    accuracy: 0.971
    f1_macro: 0.970
    inference_time_ms: 95
    base_models: 5
    meta_features: 50
  advantages:
    - "Superior performance through meta-learning"
    - "Feature importance interpretability"
    - "Flexible base model combination"
    - "Efficient XGBoost meta-learner"
  limitations:
    - "Requires training meta-learner"
    - "Complex feature engineering"
    - "Higher memory requirements"

# Notes
notes: |
  Stacking ensemble with XGBoost meta-learner configuration:
  
  Architecture:
  - Level 0: 5 diverse transformer models (DeBERTa, RoBERTa, XLNet, Longformer, T5)
  - Level 1: XGBoost classifier trained on base model outputs and meta-features
  
  Key features:
  - Rich meta-feature engineering (50+ features)
  - Cross-validation for robust meta-learner training
  - GPU-accelerated XGBoost training
  - Feature importance analysis for interpretability
  
  Expected performance:
  - Accuracy: ~97.1%
  - F1-macro: ~97.0%
  - Inference: ~95ms per sample (with caching)
  
  Training strategy:
  1. Pre-train base models independently
  2. Generate out-of-fold predictions via cross-validation
  3. Engineer meta-features from base predictions
  4. Train XGBoost on meta-features
  5. Optimize hyperparameters with Optuna

references:
  - stacking: "https://arxiv.org/abs/1802.02407"
  - xgboost: "https://arxiv.org/abs/1603.02754"
  - meta_learning: "https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
