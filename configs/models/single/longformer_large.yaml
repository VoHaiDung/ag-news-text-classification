# Longformer-Large Model Configuration
# Efficient attention mechanism for long documents in AG News Classification

name: longformer_large
type: transformer
architecture: longformer

# Model identifiers
model:
  pretrained_model_name: allenai/longformer-large-4096
  model_revision: main
  cache_dir: ./.cache/models
  use_auth_token: false

# Architecture parameters
architecture_params:
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 4096
  type_vocab_size: 1
  initializer_range: 0.02
  layer_norm_eps: 1e-5
  # Longformer specific
  attention_window: [512] * 24  # Local attention window per layer
  attention_mode: sliding_chunks
  attention_dilation: [1] * 24
  autoregressive: false
  
# Task-specific configuration
task:
  num_labels: 4
  problem_type: single_label_classification
  id2label:
    0: World
    1: Sports
    2: Business
    3: Sci/Tech
  label2id:
    World: 0
    Sports: 1
    Business: 2
    Sci/Tech: 3

# Classification head
classifier:
  type: longformer_classification_head
  hidden_dropout_prob: 0.1
  classifier_dropout: 0.1
  use_pooler: false
  pooling_strategy: global_attention  # Use global attention tokens
  
  # Multi-layer head
  num_hidden_layers: 2
  hidden_sizes: [512, 128]
  activation: gelu
  use_batch_norm: false
  use_layer_norm: true

# Fine-tuning configuration
fine_tuning:
  # Layer freezing
  freeze_embeddings: false
  freeze_encoder: false
  freeze_encoder_layers: 0
  
  # Differential learning rates
  use_differential_lr: true
  layer_lr_decay: 0.95
  
  # Global attention configuration
  global_attention_indices: [0]  # CLS token gets global attention
  
  # Gradual unfreezing
  gradual_unfreezing: false
  unfreezing_schedule: [0, 6, 12, 18]

# Training hyperparameters
training:
  learning_rate: 3e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Batch settings (smaller due to memory constraints)
  train_batch_size: 8
  eval_batch_size: 16
  gradient_accumulation_steps: 8
  
  # Schedule
  num_train_epochs: 6
  warmup_ratio: 0.1
  warmup_steps: 500
  
  # Learning rate schedule
  lr_scheduler_type: polynomial
  lr_scheduler_kwargs:
    power: 1.0
    lr_end: 1e-7
  
  # Optimization
  fp16: true
  fp16_opt_level: O2
  gradient_checkpointing: true  # Important for long sequences
  
  # Regularization
  label_smoothing_factor: 0.05
  dropout_rate: 0.1
  weight_dropout: 0.0
  attention_dropout: 0.1

# Data processing
data_processing:
  max_length: 2048  # Can handle longer sequences
  truncation: true
  padding: max_length
  pad_to_multiple_of: 512  # Efficient for attention windows
  return_attention_mask: true
  return_token_type_ids: false
  
  # Special tokens
  add_special_tokens: true
  sep_token: "</s>"
  cls_token: "<s>"
  pad_token: "<pad>"
  mask_token: "<mask>"

# Advanced techniques
advanced:
  # Sliding window attention
  sliding_window:
    enabled: true
    window_size: 512
    window_overlap: 128
  
  # Hierarchical processing
  hierarchical_processing:
    enabled: false
    chunk_size: 512
    aggregate_method: mean
  
  # Adversarial training
  adversarial_training:
    enabled: false  # Memory intensive with long sequences
    method: fgm
    epsilon: 0.5
  
  # Ensemble distillation readiness
  distillation_ready: true
  temperature: 3.0

# Optimization techniques
optimization:
  # Quantization
  quantization:
    enabled: false
    method: dynamic
    bits: 8
  
  # Attention optimization
  attention_optimization:
    use_lsh_attention: false
    use_linformer: false
    use_performer: false
  
  # Compilation
  compile_model: false  # Custom attention not always compatible

# Model outputs
outputs:
  output_hidden_states: true
  output_attentions: false  # Memory intensive for long sequences
  use_cache: true
  
  # Save settings
  save_safetensors: true
  save_torch: true
  save_onnx: false

# Evaluation metrics
evaluation:
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - f1_macro
    - f1_micro
    - f1_weighted
    - matthews_corrcoef
  
  primary_metric: f1_macro
  greater_is_better: true
  
  # Evaluation settings
  eval_accumulation_steps: 5
  eval_delay: 0
  per_device_eval_batch_size: 16

# Inference configuration
inference:
  # Optimization
  use_fast_tokenizer: true
  use_cache: true
  torch_compile: false
  
  # Batching
  max_batch_size: 32
  dynamic_batching: true
  
  # Long document handling
  sliding_inference: true
  chunk_overlap: 128
  
  # Post-processing
  apply_softmax: true
  return_all_scores: false
  top_k: 1

# Hardware requirements
hardware_requirements:
  min_gpu_memory_gb: 24
  recommended_gpu_memory_gb: 40
  min_ram_gb: 32
  recommended_ram_gb: 64
  
  # Supported GPUs
  supported_gpus:
    - A100
    - A6000
    - V100-32GB
    - RTX-4090-24GB

# Experiment tracking
tracking:
  track_gradients: false
  track_weights: false
  log_frequency: 25
  
  # Tags for tracking
  tags:
    - longformer
    - large
    - long-context
    - efficient-attention

# Model card information
model_card:
  description: "Longformer-Large with efficient attention for long AG News articles"
  intended_use: "Classification of long news articles up to 2048 tokens"
  training_data: "AG News dataset with focus on longer articles"
  eval_results:
    accuracy: 0.961
    f1_macro: 0.960
    inference_time_ms: 35
  limitations: "Higher memory requirements for very long sequences"
  ethical_considerations: "Efficient for long documents but requires significant GPU memory"

# Notes and references
notes: |
  Longformer uses sliding window attention with global attention for efficiency.
  Key features:
  - Linear scaling with sequence length (vs quadratic for standard attention)
  - Global attention for task-specific tokens
  - Suitable for documents up to 4096 tokens
  
  Expected performance:
  - Accuracy: ~96.1%
  - F1-macro: ~96.0%
  - Inference: ~35ms per sample on A100 (for 2048 tokens)

references:
  - paper: "https://arxiv.org/abs/2004.05150"
  - github: "https://github.com/allenai/longformer"
  - huggingface: "https://huggingface.co/allenai/longformer-large-4096"
