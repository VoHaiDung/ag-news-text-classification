# GPT-2-Large Model Configuration
# Generative pre-trained transformer adapted for AG News Classification

name: gpt2_large
type: transformer
architecture: gpt2

# Model identifiers
model:
  pretrained_model_name: gpt2-large
  model_revision: main
  cache_dir: ./.cache/models
  use_auth_token: false

# Architecture parameters
architecture_params:
  hidden_size: 1280
  num_hidden_layers: 36
  num_attention_heads: 20
  intermediate_size: 5120
  hidden_act: gelu_new
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 1024
  initializer_range: 0.02
  layer_norm_eps: 1e-5
  scale_attn_weights: true
  use_cache: true
  
# Task-specific configuration
task:
  num_labels: 4
  problem_type: single_label_classification
  id2label:
    0: World
    1: Sports
    2: Business
    3: Sci/Tech
  label2id:
    World: 0
    Sports: 1
    Business: 2
    Sci/Tech: 3

# Classification head
classifier:
  type: gpt2_classification_head
  hidden_dropout_prob: 0.2
  classifier_dropout: 0.2
  use_pooler: false
  pooling_strategy: last_token  # GPT-2 uses last token for classification
  
  # Multi-layer head
  num_hidden_layers: 2
  hidden_sizes: [640, 256]
  activation: gelu_new
  use_batch_norm: false
  use_layer_norm: true
  
  # GPT-2 specific
  pad_token_id: 50256  # Using eos_token as pad_token

# Fine-tuning configuration
fine_tuning:
  # Layer freezing
  freeze_embeddings: false
  freeze_encoder: false
  freeze_encoder_layers: 12  # Freeze first 12 layers
  
  # Differential learning rates
  use_differential_lr: true
  layer_lr_decay: 0.9
  
  # Gradual unfreezing
  gradual_unfreezing: true
  unfreezing_schedule: [0, 9, 18, 27]

# Training hyperparameters
training:
  learning_rate: 5e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Batch settings
  train_batch_size: 16
  eval_batch_size: 32
  gradient_accumulation_steps: 4
  
  # Schedule
  num_train_epochs: 8
  warmup_ratio: 0.1
  warmup_steps: 500
  
  # Learning rate schedule
  lr_scheduler_type: cosine
  lr_scheduler_kwargs:
    num_cycles: 0.5
    last_epoch: -1
  
  # Optimization
  fp16: true
  fp16_opt_level: O1
  gradient_checkpointing: true
  
  # Regularization
  label_smoothing_factor: 0.1
  dropout_rate: 0.1
  weight_dropout: 0.0
  attention_dropout: 0.1

# Data processing
data_processing:
  max_length: 1024
  truncation: true
  padding: left  # GPT-2 uses left padding
  pad_to_multiple_of: 8
  return_attention_mask: true
  return_token_type_ids: false
  
  # Special tokens
  add_special_tokens: true
  bos_token: "<|startoftext|>"
  eos_token: "<|endoftext|>"
  pad_token: "<|endoftext|>"  # GPT-2 doesn't have pad token
  unk_token: "<|endoftext|>"
  
  # Prefix for classification
  task_prefix: "Classify the following news article:\n"

# Advanced techniques
advanced:
  # Prompt-based classification
  prompt_classification:
    enabled: true
    prompt_template: "This news article belongs to the category:"
    label_words:
      World: ["world", "international", "global"]
      Sports: ["sports", "game", "match"]
      Business: ["business", "economy", "finance"]
      Sci/Tech: ["technology", "science", "tech"]
  
  # Generation-based classification
  generation_classification:
    enabled: false
    max_new_tokens: 10
    temperature: 0.7
    top_p: 0.9
  
  # Adversarial training
  adversarial_training:
    enabled: false
    method: fgm
    epsilon: 0.5
  
  # Ensemble distillation readiness
  distillation_ready: true
  temperature: 2.0

# Optimization techniques
optimization:
  # Quantization
  quantization:
    enabled: false
    method: dynamic
    bits: 8
  
  # Pruning
  pruning:
    enabled: false
    method: magnitude
    sparsity: 0.2
  
  # Compilation
  compile_model: true
  compile_mode: default

# Model outputs
outputs:
  output_hidden_states: true
  output_attentions: false
  use_cache: true
  
  # Save settings
  save_safetensors: true
  save_torch: true
  save_onnx: false

# Evaluation metrics
evaluation:
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - f1_macro
    - f1_micro
    - f1_weighted
    - matthews_corrcoef
    - perplexity  # GPT-2 specific
  
  primary_metric: f1_macro
  greater_is_better: true
  
  # Evaluation settings
  eval_accumulation_steps: 10
  eval_delay: 0
  per_device_eval_batch_size: 32

# Inference configuration
inference:
  # Optimization
  use_fast_tokenizer: true
  use_cache: true
  torch_compile: true
  
  # Batching
  max_batch_size: 64
  dynamic_batching: true
  
  # Generation settings (if using generation-based classification)
  generation_config:
    do_sample: false
    max_new_tokens: 5
    temperature: 1.0
    top_p: 1.0
  
  # Post-processing
  apply_softmax: true
  return_all_scores: false
  top_k: 1

# Hardware requirements
hardware_requirements:
  min_gpu_memory_gb: 16
  recommended_gpu_memory_gb: 24
  min_ram_gb: 16
  recommended_ram_gb: 32
  
  # Supported GPUs
  supported_gpus:
    - A100
    - V100
    - RTX-3090
    - RTX-4090
    - T4

# Experiment tracking
tracking:
  track_gradients: false
  track_weights: false
  log_frequency: 50
  
  # Tags for tracking
  tags:
    - gpt2
    - large
    - generative
    - autoregressive

# Model card information
model_card:
  description: "GPT-2-Large adapted for AG News classification task"
  intended_use: "News article classification using autoregressive language model"
  training_data: "AG News dataset with special formatting for GPT-2"
  eval_results:
    accuracy: 0.952
    f1_macro: 0.951
    inference_time_ms: 20
  limitations: "Left-to-right attention only, requires careful padding strategy"
  ethical_considerations: "Pre-trained on web text, may contain biases"

# Notes and references
notes: |
  GPT-2-Large adapted for classification using the last token representation.
  Key considerations:
  - Uses left padding to ensure last token aligns properly
  - Can leverage prompt-based or generation-based classification
  - Autoregressive nature may not be optimal for classification
  
  Expected performance:
  - Accuracy: ~95.2%
  - F1-macro: ~95.1%
  - Inference: ~20ms per sample on V100

references:
  - paper: "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
  - github: "https://github.com/openai/gpt-2"
  - huggingface: "https://huggingface.co/gpt2-large"
