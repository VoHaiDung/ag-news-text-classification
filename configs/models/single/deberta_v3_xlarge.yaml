# DeBERTa-v3-xlarge Model Configuration
# State-of-the-art model for AG News Text Classification

name: deberta_v3_xlarge
type: transformer
architecture: deberta-v3

# Model identifiers
model:
  pretrained_model_name: microsoft/deberta-v3-xlarge
  model_revision: main
  cache_dir: ./.cache/models
  use_auth_token: false

# Architecture parameters
architecture_params:
  hidden_size: 1536
  num_hidden_layers: 48
  num_attention_heads: 24
  intermediate_size: 6144
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  type_vocab_size: 0
  initializer_range: 0.02
  layer_norm_eps: 1e-7
  relative_attention: true
  position_biased_input: false
  pos_att_type: p2c|c2p
  pooler_dropout: 0.0
  pooler_hidden_act: gelu
  
# Task-specific configuration
task:
  num_labels: 4
  problem_type: single_label_classification
  id2label:
    0: World
    1: Sports
    2: Business
    3: Sci/Tech
  label2id:
    World: 0
    Sports: 1
    Business: 2
    Sci/Tech: 3

# Classification head
classifier:
  type: custom_deberta_head
  hidden_dropout_prob: 0.2
  classifier_dropout: 0.2
  use_pooler: false
  pooling_strategy: mean  # Options: cls, mean, max, attention
  
  # Multi-layer head
  num_hidden_layers: 2
  hidden_sizes: [768, 256]
  activation: gelu
  use_batch_norm: true
  use_layer_norm: true

# Fine-tuning configuration
fine_tuning:
  # Layer freezing
  freeze_embeddings: false
  freeze_encoder: false
  freeze_encoder_layers: 0  # Number of layers to freeze from bottom
  
  # Differential learning rates
  use_differential_lr: true
  layer_lr_decay: 0.95  # Decay factor for lower layers
  
  # Gradual unfreezing
  gradual_unfreezing: false
  unfreezing_schedule: [0, 12, 24, 36]  # Layers to unfreeze at each epoch

# Training hyperparameters
training:
  learning_rate: 1e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Batch settings
  train_batch_size: 16
  eval_batch_size: 32
  gradient_accumulation_steps: 4
  
  # Schedule
  num_train_epochs: 10
  warmup_ratio: 0.1
  warmup_steps: 1000
  
  # Learning rate schedule
  lr_scheduler_type: cosine
  lr_scheduler_kwargs:
    num_cycles: 0.5
    last_epoch: -1
  
  # Optimization
  fp16: true
  fp16_opt_level: O1
  gradient_checkpointing: true
  
  # Regularization
  label_smoothing_factor: 0.1
  dropout_rate: 0.1
  weight_dropout: 0.0
  attention_dropout: 0.1

# Data processing
data_processing:
  max_length: 512
  truncation: true
  padding: max_length
  pad_to_multiple_of: 8
  return_attention_mask: true
  return_token_type_ids: false
  
  # Special tokens
  add_special_tokens: true
  sep_token: "[SEP]"
  cls_token: "[CLS]"
  pad_token: "[PAD]"
  mask_token: "[MASK]"

# Advanced techniques
advanced:
  # Adversarial training
  adversarial_training:
    enabled: true
    method: fgm  # Options: fgm, pgd, freelb
    epsilon: 1.0
    alpha: 0.3
    steps: 3
  
  # R-Drop regularization
  r_drop:
    enabled: true
    alpha: 0.5
  
  # Mixout regularization
  mixout:
    enabled: false
    probability: 0.2
  
  # Ensemble distillation readiness
  distillation_ready: true
  temperature: 3.0

# Optimization techniques
optimization:
  # Quantization
  quantization:
    enabled: false
    method: dynamic
    bits: 8
  
  # Pruning
  pruning:
    enabled: false
    method: magnitude
    sparsity: 0.3
  
  # Compilation (PyTorch 2.0+)
  compile_model: true
  compile_mode: reduce-overhead

# Model outputs
outputs:
  output_hidden_states: true
  output_attentions: false
  use_cache: true
  
  # Save settings
  save_safetensors: true
  save_torch: true
  save_onnx: false

# Evaluation metrics
evaluation:
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - f1_macro
    - f1_micro
    - f1_weighted
    - matthews_corrcoef
  
  primary_metric: f1_macro
  greater_is_better: true
  
  # Evaluation settings
  eval_accumulation_steps: 10
  eval_delay: 0
  per_device_eval_batch_size: 32

# Inference configuration
inference:
  # Optimization
  use_fast_tokenizer: true
  use_cache: true
  torch_compile: true
  
  # Batching
  max_batch_size: 128
  dynamic_batching: true
  
  # Post-processing
  apply_softmax: true
  return_all_scores: false
  top_k: 1

# Hardware requirements
hardware_requirements:
  min_gpu_memory_gb: 24
  recommended_gpu_memory_gb: 40
  min_ram_gb: 32
  recommended_ram_gb: 64
  
  # Supported GPUs
  supported_gpus:
    - A100
    - A6000
    - V100-32GB
    - RTX-4090
    - RTX-3090

# Experiment tracking
tracking:
  track_gradients: true
  track_weights: false
  log_frequency: 100
  
  # Tags for tracking
  tags:
    - deberta-v3
    - xlarge
    - sota
    - production

# Model card information
model_card:
  description: "DeBERTa-v3-xlarge fine-tuned for AG News classification"
  intended_use: "News article classification into 4 categories"
  training_data: "AG News dataset with augmentation"
  eval_results:
    accuracy: 0.964
    f1_macro: 0.963
    inference_time_ms: 25
  limitations: "May not generalize well to non-English news or very short texts"
  ethical_considerations: "Model should not be used for censorship or biased content filtering"

# Notes and references
notes: |
  DeBERTa-v3-xlarge is the primary model for achieving SOTA performance.
  Key improvements over DeBERTa-v2:
  - ELECTRA-style pre-training with RTD (Replaced Token Detection)
  - Gradient-disentangled embedding sharing
  - Enhanced mask decoder with position embeddings
  
  Expected performance:
  - Accuracy: ~96.4%
  - F1-macro: ~96.3%
  - Inference: ~25ms per sample on A100

references:
  - paper: "https://arxiv.org/abs/2111.09543"
  - github: "https://github.com/microsoft/DeBERTa"
  - huggingface: "https://huggingface.co/microsoft/deberta-v3-xlarge"
