# XLNet-Large Model Configuration
# Autoregressive pretraining for AG News Text Classification

name: xlnet_large
type: transformer
architecture: xlnet

# Model identifiers
model:
  pretrained_model_name: xlnet-large-cased
  model_revision: main
  cache_dir: ./.cache/models
  use_auth_token: false

# Architecture parameters
architecture_params:
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  type_vocab_size: 2
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  
  # XLNet specific
  use_mems_eval: true
  use_mems_train: true
  bi_data: false
  clamp_len: -1
  same_length: false
  reuse_len: null
  mem_len: null

# Task configuration
task:
  num_labels: 4
  problem_type: single_label_classification
  id2label:
    0: World
    1: Sports  
    2: Business
    3: Sci/Tech

# Training hyperparameters
training:
  learning_rate: 2e-5
  train_batch_size: 24
  eval_batch_size: 48
  num_train_epochs: 8
  warmup_ratio: 0.1
  weight_decay: 0.01
  
  # XLNet benefits from specific optimization
  optimizer: lamb
  scheduler: polynomial

# Notes
notes: |
  XLNet-large provides unique advantages:
  - Permutation language modeling
  - Bidirectional context modeling
  - Better handling of dependencies
  
  Expected performance:
  - Accuracy: ~95.5%
  - Good for ensemble diversity
