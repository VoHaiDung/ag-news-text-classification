# RoBERTa-large Model Configuration
# High-performance model for AG News Text Classification

name: roberta_large
type: transformer
architecture: roberta

# Model identifiers
model:
  pretrained_model_name: roberta-large
  model_revision: main
  cache_dir: ./.cache/models
  use_auth_token: false

# Architecture parameters
architecture_params:
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 514
  type_vocab_size: 1
  initializer_range: 0.02
  layer_norm_eps: 1e-5
  position_embedding_type: absolute
  use_cache: true
  classifier_dropout: 0.1

# Task-specific configuration
task:
  num_labels: 4
  problem_type: single_label_classification
  id2label:
    0: World
    1: Sports
    2: Business
    3: Sci/Tech
  label2id:
    World: 0
    Sports: 1
    Business: 2
    Sci/Tech: 3

# Classification head
classifier:
  type: roberta_classification_head
  hidden_dropout_prob: 0.2
  use_pooler: false
  pooling_strategy: mean
  
  # Custom head architecture
  num_hidden_layers: 2
  hidden_sizes: [512, 128]
  activation: gelu
  use_batch_norm: true

# Fine-tuning configuration
fine_tuning:
  freeze_embeddings: false
  freeze_encoder: false
  freeze_encoder_layers: 0
  
  # Differential learning rates
  use_differential_lr: true
  layer_lr_decay: 0.9
  
  # Smart layer freezing
  smart_freezing: true
  freeze_batch_norm: false

# Training hyperparameters
training:
  learning_rate: 2e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_epsilon: 1e-6
  max_grad_norm: 1.0
  
  # Batch settings
  train_batch_size: 32
  eval_batch_size: 64
  gradient_accumulation_steps: 2
  
  # Schedule
  num_train_epochs: 8
  warmup_ratio: 0.06
  warmup_steps: 500
  
  # Learning rate schedule
  lr_scheduler_type: linear
  
  # Optimization
  fp16: true
  fp16_opt_level: O2
  gradient_checkpointing: false
  
  # Regularization
  label_smoothing_factor: 0.05
  dropout_rate: 0.1

# Data processing
data_processing:
  max_length: 512
  truncation: true
  padding: max_length
  pad_to_multiple_of: 8
  return_attention_mask: true
  return_token_type_ids: false
  
  # RoBERTa specific
  add_prefix_space: true
  use_fast_tokenizer: true

# Advanced techniques
advanced:
  # Adversarial training
  adversarial_training:
    enabled: false
    method: pgd
    epsilon: 0.5
  
  # Mixup augmentation
  mixup:
    enabled: true
    alpha: 0.2
  
  # Stochastic depth
  stochastic_depth:
    enabled: false
    drop_rate: 0.1

# Optimization techniques
optimization:
  # Quantization
  quantization:
    enabled: false
    method: dynamic
    bits: 8
  
  # Knowledge distillation readiness
  distillation_ready: true
  
  # ONNX export
  onnx_export: true
  onnx_opset: 14

# Model outputs
outputs:
  output_hidden_states: true
  output_attentions: false
  use_cache: true
  
  # Save settings
  save_safetensors: true
  save_torch: true

# Evaluation metrics
evaluation:
  metrics:
    - accuracy
    - f1_macro
    - precision_macro
    - recall_macro
  
  primary_metric: f1_macro
  greater_is_better: true

# Inference configuration
inference:
  use_fast_tokenizer: true
  use_cache: true
  torch_compile: true
  
  # Batching
  max_batch_size: 256
  dynamic_batching: true

# Hardware requirements
hardware_requirements:
  min_gpu_memory_gb: 16
  recommended_gpu_memory_gb: 24
  min_ram_gb: 16
  recommended_ram_gb: 32

# Model card information
model_card:
  description: "RoBERTa-large fine-tuned for AG News Text Classification"
  intended_use: "News article classification into 4 categories"
  eval_results:
    accuracy: 0.957
    f1_macro: 0.956
    inference_time_ms: 18
  limitations: "Optimized for English news articles"

# Notes
notes: |
  RoBERTa-large serves as a strong baseline and ensemble member.
  Key advantages:
  - Robust pre-training with dynamic masking
  - No NSP task, trained on longer sequences
  - Excellent for ensemble diversity
  
  Expected performance:
  - Accuracy: ~95.7%
  - F1-macro: ~95.6%
  - Inference: ~18ms per sample on V100

references:
  - paper: "https://arxiv.org/abs/1907.11692"
  - huggingface: "https://huggingface.co/roberta-large"
