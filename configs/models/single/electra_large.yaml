# ELECTRA-Large Model Configuration
# Discriminative pre-training approach for AG News Classification

name: electra_large
type: transformer
architecture: electra

# Model identifiers
model:
  pretrained_model_name: google/electra-large-discriminator
  model_revision: main
  cache_dir: ./.cache/models
  use_auth_token: false

# Architecture parameters
architecture_params:
  hidden_size: 1024
  num_hidden_layers: 24
  num_attention_heads: 16
  intermediate_size: 4096
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  type_vocab_size: 2
  initializer_range: 0.02
  layer_norm_eps: 1e-12
  embedding_size: 128
  
# Task-specific configuration
task:
  num_labels: 4
  problem_type: single_label_classification
  id2label:
    0: World
    1: Sports
    2: Business
    3: Sci/Tech
  label2id:
    World: 0
    Sports: 1
    Business: 2
    Sci/Tech: 3

# Classification head
classifier:
  type: electra_classification_head
  hidden_dropout_prob: 0.1
  classifier_dropout: 0.1
  use_pooler: false
  pooling_strategy: first_token  # ELECTRA uses CLS token
  
  # Multi-layer head
  num_hidden_layers: 2
  hidden_sizes: [512, 128]
  activation: gelu
  use_batch_norm: false
  use_layer_norm: true

# Fine-tuning configuration
fine_tuning:
  # Layer freezing
  freeze_embeddings: false
  freeze_encoder: false
  freeze_encoder_layers: 0
  
  # Differential learning rates
  use_differential_lr: true
  layer_lr_decay: 0.9
  
  # Gradual unfreezing
  gradual_unfreezing: false
  unfreezing_schedule: [0, 6, 12, 18]

# Training hyperparameters
training:
  learning_rate: 2e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-6
  max_grad_norm: 1.0
  
  # Batch settings
  train_batch_size: 32
  eval_batch_size: 64
  gradient_accumulation_steps: 2
  
  # Schedule
  num_train_epochs: 8
  warmup_ratio: 0.1
  warmup_steps: 500
  
  # Learning rate schedule
  lr_scheduler_type: linear
  lr_scheduler_kwargs:
    last_epoch: -1
  
  # Optimization
  fp16: true
  fp16_opt_level: O1
  gradient_checkpointing: false
  
  # Regularization
  label_smoothing_factor: 0.05
  dropout_rate: 0.1
  weight_dropout: 0.0
  attention_dropout: 0.1

# Data processing
data_processing:
  max_length: 512
  truncation: true
  padding: max_length
  pad_to_multiple_of: 8
  return_attention_mask: true
  return_token_type_ids: true
  
  # Special tokens
  add_special_tokens: true
  sep_token: "[SEP]"
  cls_token: "[CLS]"
  pad_token: "[PAD]"
  mask_token: "[MASK]"

# Advanced techniques
advanced:
  # Adversarial training
  adversarial_training:
    enabled: true
    method: pgd
    epsilon: 0.5
    alpha: 0.1
    steps: 2
  
  # R-Drop regularization
  r_drop:
    enabled: false
    alpha: 0.3
  
  # Discriminative fine-tuning
  discriminative_fine_tuning:
    enabled: true
    base_lr: 2e-5
    decay_factor: 0.9
  
  # Ensemble distillation readiness
  distillation_ready: true
  temperature: 2.0

# Optimization techniques
optimization:
  # Quantization
  quantization:
    enabled: false
    method: dynamic
    bits: 8
  
  # Pruning
  pruning:
    enabled: false
    method: structured
    sparsity: 0.2
  
  # Compilation
  compile_model: true
  compile_mode: default

# Model outputs
outputs:
  output_hidden_states: true
  output_attentions: false
  use_cache: false
  
  # Save settings
  save_safetensors: true
  save_torch: true
  save_onnx: false

# Evaluation metrics
evaluation:
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - f1_macro
    - f1_micro
    - f1_weighted
    - matthews_corrcoef
  
  primary_metric: accuracy
  greater_is_better: true
  
  # Evaluation settings
  eval_accumulation_steps: 10
  eval_delay: 0
  per_device_eval_batch_size: 64

# Inference configuration
inference:
  # Optimization
  use_fast_tokenizer: true
  use_cache: false
  torch_compile: true
  
  # Batching
  max_batch_size: 256
  dynamic_batching: true
  
  # Post-processing
  apply_softmax: true
  return_all_scores: false
  top_k: 1

# Hardware requirements
hardware_requirements:
  min_gpu_memory_gb: 16
  recommended_gpu_memory_gb: 24
  min_ram_gb: 16
  recommended_ram_gb: 32
  
  # Supported GPUs
  supported_gpus:
    - A100
    - V100
    - RTX-3090
    - RTX-4090
    - T4

# Experiment tracking
tracking:
  track_gradients: false
  track_weights: false
  log_frequency: 50
  
  # Tags for tracking
  tags:
    - electra
    - large
    - discriminative
    - efficient

# Model card information
model_card:
  description: "ELECTRA-Large discriminator fine-tuned for AG News classification"
  intended_use: "News article classification with efficient discriminative approach"
  training_data: "AG News dataset"
  eval_results:
    accuracy: 0.958
    f1_macro: 0.957
    inference_time_ms: 15
  limitations: "Less effective on very long documents beyond 512 tokens"
  ethical_considerations: "Model trained on news data may reflect biases in news reporting"

# Notes and references
notes: |
  ELECTRA uses discriminative pre-training instead of masked language modeling.
  Advantages:
  - More sample-efficient than BERT
  - Faster convergence during fine-tuning
  - Better performance with same compute budget
  
  Expected performance:
  - Accuracy: ~95.8%
  - F1-macro: ~95.7%
  - Inference: ~15ms per sample on V100

references:
  - paper: "https://arxiv.org/abs/2003.10555"
  - github: "https://github.com/google-research/electra"
  - huggingface: "https://huggingface.co/google/electra-large-discriminator"
