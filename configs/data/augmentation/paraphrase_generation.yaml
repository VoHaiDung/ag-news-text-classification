# Paraphrase Generation Configuration
# ====================================
#
# Generate paraphrases for data augmentation following:
# - Prakash et al. (2016): "Neural Paraphrase Generation with Stacked Residual LSTM Networks"
# - Li et al. (2018): "Paraphrase Generation with Deep Reinforcement Learning"
# - Kazemnejad et al. (2020): "Paraphrase Generation by Learning How to Edit from Samples"
#
# Author: Võ Hải Dũng

name: paraphrase_generation
type: augmentation
description: "Generate paraphrases using neural models"

# Paraphrase model
model:
  # Model selection
  model_name: "tuner007/pegasus_paraphrase"
  
  # Alternative models
  alternatives:
    - "Vamsi/T5_Paraphrase_Paws"
    - "ramsrigouthamg/t5-large-paraphraser-diverse-high-quality"
    - "humarin/chatgpt_paraphraser_on_T5_base"
  
  # Model parameters
  device: "cuda"
  max_memory: 8  # GB

# Generation settings
generation:
  # Number of paraphrases
  num_paraphrases: 3
  
  # Diversity control
  temperature: 1.2
  top_k: 50
  top_p: 0.95
  
  # Beam search
  num_beams: 10
  num_beam_groups: 5
  diversity_penalty: 1.0
  
  # Length control
  length_penalty: 1.0
  no_repeat_ngram_size: 3
  
  # Early stopping
  early_stopping: true

# Prompt templates
prompts:
  - "Paraphrase: {text}"
  - "Rewrite this: {text}"
  - "Say this differently: {text}"
  - "Alternative phrasing: {text}"

# Quality filters
quality:
  # Semantic similarity
  min_similarity: 0.75
  max_similarity: 0.95
  
  # Lexical diversity
  min_word_overlap: 0.3
  max_word_overlap: 0.8
  
  # Length constraints
  min_length_ratio: 0.8
  max_length_ratio: 1.2
  
  # Fluency check
  fluency_model: "textattack/roberta-base-CoLA"
  min_fluency_score: 0.7

# Diversity metrics
diversity:
  # Self-BLEU threshold
  max_self_bleu: 0.7
  
  # N-gram diversity
  min_ngram_diversity: 0.6
  
  # Syntactic diversity
  check_syntax: true
  min_syntax_diversity: 0.5

# Application strategy
strategy:
  # Target specific classes
  target_classes: "all"  # all, minority, specific
  
  # Sampling
  sample_ratio: 0.5
  
  # Combination with original
  keep_original: true
  mix_ratio: 0.5

# Performance optimization
optimization:
  batch_size: 16
  cache_enabled: true
  parallel_generation: true
  num_gpus: 1

# Expected impact
expected_impact:
  data_multiplier: 3
  vocabulary_expansion: "+20%"
  model_robustness: "+2-3%"
  
references:
  - "https://arxiv.org/abs/1610.03098"
  - "https://arxiv.org/abs/1711.00279"
