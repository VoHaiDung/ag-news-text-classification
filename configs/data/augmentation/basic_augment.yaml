# Basic Data Augmentation Configuration
# ======================================
#
# This configuration implements fundamental text augmentation techniques
# following best practices from:
# - Wei & Zou (2019): "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"
# - Kobayashi (2018): "Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations"
# - Xie et al. (2020): "Unsupervised Data Augmentation for Consistency Training"
#
# Author: Võ Hải Dũng
# License: MIT

name: basic_augmentation
type: augmentation
description: "Basic text augmentation techniques for AG News classification"

# Augmentation pipeline configuration
pipeline:
  # Enable/disable augmentation
  enabled: true
  
  # Augmentation probability per sample
  augmentation_probability: 0.5
  
  # Number of augmented versions per sample
  num_augmentations: 2
  
  # Random seed for reproducibility (Dodge et al., 2019)
  seed: 42
  
  # Pipeline order (executed sequentially)
  order:
    - synonym_replacement
    - random_insertion
    - random_swap
    - random_deletion

# EDA: Easy Data Augmentation (Wei & Zou, 2019)
# Paper: https://arxiv.org/abs/1901.11196
eda:
  # Synonym Replacement
  synonym_replacement:
    enabled: true
    probability: 0.1  # Probability of replacing each word
    num_replacements: 3  # Maximum number of words to replace
    
    # WordNet configuration (Miller, 1995)
    wordnet:
      use_wordnet: true
      language: "eng"
      pos_tags: ["NOUN", "VERB", "ADJ", "ADV"]
      
    # Contextual synonyms (Kobayashi, 2018)
    contextual:
      use_contextual: false
      model: "bert-base-uncased"
      top_k: 10
      threshold: 0.7
    
    # Preserve important words
    protected_words:
      - entities: true  # Preserve named entities
      - numbers: true   # Preserve numbers
      - dates: true     # Preserve dates
      - urls: true      # Preserve URLs
      - custom: []      # Custom protected words
  
  # Random Insertion
  random_insertion:
    enabled: true
    probability: 0.1
    num_insertions: 3
    
    # Insertion strategy
    strategy: "synonym"  # Options: synonym, random_word, contextual
    
    # Position constraints
    avoid_start: true  # Don't insert at sentence start
    avoid_end: true    # Don't insert at sentence end
  
  # Random Swap
  random_swap:
    enabled: true
    probability: 0.1
    num_swaps: 3
    
    # Swap constraints
    max_distance: 5  # Maximum distance between swapped words
    preserve_order_tags: ["DATE", "TIME", "MONEY"]  # Don't swap these
  
  # Random Deletion
  random_deletion:
    enabled: true
    probability: 0.1
    max_deletions: 2
    
    # Deletion constraints
    min_sentence_length: 5  # Don't delete if sentence becomes too short
    preserve_ratio: 0.8  # Keep at least 80% of original text

# Token-level augmentation (Xie et al., 2020)
token_augmentation:
  # Token replacement
  token_replacement:
    enabled: false
    method: "mask"  # Options: mask, random, similar
    probability: 0.15
    
    # BERT-style masking (Devlin et al., 2019)
    mask_token: "[MASK]"
    random_token_probability: 0.1
    unchanged_probability: 0.1
  
  # Token cutoff (Shen et al., 2020)
  token_cutoff:
    enabled: false
    min_cutoff: 1
    max_cutoff: 5
    strategy: "continuous"  # Options: continuous, random
  
  # Feature cutoff
  feature_cutoff:
    enabled: false
    cutoff_rate: 0.1
    cutoff_dim: "last"  # Options: last, random

# Sentence-level augmentation
sentence_augmentation:
  # Sentence shuffling
  sentence_shuffling:
    enabled: false
    probability: 0.1
    preserve_first: true  # Keep first sentence in place
    preserve_last: false
  
  # Sentence cropping
  sentence_cropping:
    enabled: false
    min_crop_ratio: 0.8
    max_crop_ratio: 1.0
    position: "random"  # Options: start, end, random
  
  # Back translation (Edunov et al., 2018)
  back_translation:
    enabled: false  # Handled in separate config
    languages: ["de", "fr"]

# Contextual augmentation (Kobayashi, 2018)
contextual_augmentation:
  enabled: false
  model: "bert-base-uncased"
  
  # Label conditioning
  use_label_conditioning: true
  label_embedding_dim: 100
  
  # Sampling parameters
  temperature: 1.0
  top_k: 10
  top_p: 0.9
  
  # Filtering
  min_similarity: 0.7
  max_similarity: 0.95

# Noise injection
noise_injection:
  # Character-level noise
  character_noise:
    enabled: false
    probability: 0.05
    
    # Noise types
    typos: true  # Simulate typos
    keyboard_errors: true  # Adjacent key errors
    ocr_errors: false  # OCR-like errors
    
  # Word-level noise
  word_noise:
    enabled: false
    probability: 0.05
    
    # Noise types
    spelling_errors: true
    homophone_replacement: false
    abbreviation_expansion: true

# Quality control
quality_control:
  # Semantic similarity check (Reimers & Gurevych, 2019)
  check_similarity:
    enabled: true
    model: "sentence-transformers/all-MiniLM-L6-v2"
    min_similarity: 0.8
    max_similarity: 0.98
  
  # Grammar check
  check_grammar:
    enabled: false
    tool: "language_tool"
    max_errors: 2
  
  # Length constraints
  length_constraints:
    min_length_ratio: 0.7
    max_length_ratio: 1.3
    min_words: 10
    max_words: 1000
  
  # Label preservation
  preserve_label:
    enabled: true
    validation_model: null  # Use original model for validation
    confidence_threshold: 0.8

# Sampling strategy
sampling:
  # Class-balanced augmentation
  balance_classes: true
  
  # Augmentation distribution
  distribution:
    uniform: false
    weighted: true
    
    # Weight by class frequency (inverse)
    class_weights: "inverse_frequency"
  
  # Difficulty-based sampling (Bengio et al., 2009)
  curriculum_sampling:
    enabled: false
    difficulty_metric: "model_confidence"
    schedule: "linear"

# Caching and optimization
optimization:
  # Cache augmented samples
  cache_augmented: true
  cache_dir: "./.cache/augmentation"
  cache_size_gb: 2
  
  # Batch processing
  batch_size: 100
  num_workers: 4
  
  # Memory management
  max_memory_gb: 8
  clear_cache_frequency: 1000

# Integration with training
training_integration:
  # Online vs offline augmentation
  augmentation_mode: "offline"  # Options: online, offline, hybrid
  
  # Augmentation scheduling
  schedule:
    enabled: false
    warmup_epochs: 2
    increase_rate: 0.1
    max_augmentation_ratio: 3.0
  
  # Loss weighting
  augmented_loss_weight: 0.8
  original_loss_weight: 1.0

# Monitoring and logging
monitoring:
  # Track augmentation statistics
  track_statistics: true
  
  # Statistics to track
  statistics:
    - augmentation_rate
    - similarity_scores
    - length_changes
    - vocabulary_diversity
    - label_preservation_rate
  
  # Save examples
  save_examples: true
  num_examples: 100
  example_dir: "./outputs/augmentation_examples"

# Expected impact (based on Wei & Zou, 2019)
expected_impact:
  accuracy_improvement: 0.01  # 1% improvement
  robustness_improvement: 0.02  # 2% on adversarial
  data_size_multiplier: 2.0  # 2x effective data size
  training_time_increase: 1.2  # 20% increase

# Notes
notes: |
  Basic augmentation configuration optimized for AG News:
  
  1. EDA Techniques (Wei & Zou, 2019):
     - Synonym replacement: 10% probability, max 3 words
     - Random insertion: Adds diversity
     - Random swap: Local reordering
     - Random deletion: Forces model to use context
  
  2. Quality Control:
     - Semantic similarity > 0.8 ensures meaning preservation
     - Length constraints prevent degenerate cases
     - Label preservation validates augmentation
  
  3. Best Practices:
     - Preserve named entities and numbers (important for news)
     - Balance augmentation across classes
     - Cache for efficiency
     - Monitor augmentation quality
  
  4. Expected Benefits:
     - 1% accuracy improvement
     - Better generalization
     - Reduced overfitting
     - Improved robustness
  
  5. Computational Cost:
     - ~0.5 seconds per sample
     - ~1 hour for full dataset
     - Storage: ~200MB cached

references:
  - eda: "https://arxiv.org/abs/1901.11196"
  - contextual: "https://arxiv.org/abs/1805.06201"
  - uda: "https://arxiv.org/abs/1904.12848"
  - nlpaug: "https://github.com/makcedward/nlpaug"
