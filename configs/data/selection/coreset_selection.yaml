# Coreset Selection Configuration
# ================================
#
# Coreset selection for efficient training following:
# - Sener & Savarese (2018): "Active Learning for Convolutional Neural Networks: A Core-Set Approach"
# - Mirzasoleiman et al. (2020): "Coresets for Data-efficient Training of Machine Learning Models"
# - Campbell & Broderick (2018): "Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent"
#
# Author: Võ Hải Dũng
# License: MIT

name: coreset_selection
type: data_selection
description: "Select representative subset using coreset algorithms"

# Algorithm selection
algorithm:
  # Main algorithm
  method: "k_center_greedy"  # k_center_greedy, craig, gradient_matching
  
  # Alternative algorithms
  alternatives:
    - "k_means_coreset"
    - "uncertainty_coreset"
    - "diversity_coreset"
    - "craig"  # Coresets for Accelerated Incremental Gradient
    - "glister"  # Generalization based Data Subset Selection
  
  # Algorithm-specific parameters
  k_center_greedy:
    distance_metric: "euclidean"  # euclidean, cosine, manhattan
    initialization: "random"  # random, k_means++, diversity
    
  craig:
    facility_location: true
    submodular_optimization: true
    lazy_greedy: true  # Faster optimization
    
  gradient_matching:
    per_class: true
    gradient_type: "last_layer"  # last_layer, all_layers

# Selection parameters
selection:
  # Subset size
  subset_fraction: 0.1  # Select 10% of data
  min_samples_per_class: 10
  max_samples_per_class: 1000
  
  # Balancing
  balanced_selection: true
  class_weights: "inverse_frequency"  # inverse_frequency, uniform, custom
  
  # Incremental selection
  incremental: false
  increment_size: 100
  
  # Warm start
  warm_start: false
  initial_subset: []

# Feature extraction
features:
  # Feature source
  source: "embeddings"  # embeddings, gradients, activations
  
  # Embedding configuration
  embeddings:
    model: "sentence-transformers/all-MiniLM-L6-v2"
    layer: -1  # Last layer
    pooling: "mean"  # mean, max, cls
    normalize: true
    
  # Gradient features
  gradients:
    accumulate_steps: 1
    normalize: true
    clip_norm: 1.0
    
  # Activation features
  activations:
    layers: [-1, -2, -3]  # Which layers to use
    aggregation: "concatenate"  # concatenate, mean, weighted

# Distance computation
distance:
  # Metric
  metric: "euclidean"  # euclidean, cosine, manhattan, mahalanobis
  
  # Optimization
  use_approximate: false  # Use approximate nearest neighbors
  approximate_method: "faiss"  # faiss, annoy, hnswlib
  
  # Batch processing
  batch_size: 1000
  use_gpu: true
  
  # Caching
  cache_distances: true
  cache_size_gb: 2

# Diversity constraints
diversity:
  # Ensure diversity
  enforce_diversity: true
  diversity_weight: 0.3
  
  # Coverage radius
  min_coverage_radius: 0.1
  max_coverage_radius: 1.0
  
  # Outlier handling
  include_outliers: true
  outlier_fraction: 0.05

# Quality metrics
quality:
  # Validation metrics
  compute_metrics: true
  metrics:
    - "coverage"  # How well coreset covers full dataset
    - "gradient_error"  # Gradient approximation error
    - "loss_difference"  # Loss difference between coreset and full
    - "diversity_score"  # Diversity of selected samples
    
  # Thresholds
  min_coverage: 0.9
  max_gradient_error: 0.1
  max_loss_difference: 0.05

# Computational efficiency
efficiency:
  # Parallelization
  num_workers: 4
  multiprocessing: true
  
  # Memory management
  max_memory_gb: 8
  streaming: false  # Stream large datasets
  
  # GPU acceleration
  use_cuda: true
  cuda_device: 0

# Update strategy
update:
  # When to recompute coreset
  recompute_frequency: "epoch"  # epoch, never, adaptive
  
  # Adaptive recomputation
  adaptive:
    monitor_metric: "validation_loss"
    threshold: 0.01  # Recompute if change > threshold
    
  # Online coreset
  online_update: false
  update_batch_size: 100

# Integration with training
training_integration:
  # How to use coreset
  usage: "subset"  # subset, importance_sampling, curriculum
  
  # Importance sampling
  importance_sampling:
    temperature: 1.0
    sampling_distribution: "uniform"  # uniform, weighted
    
  # Curriculum learning
  curriculum:
    start_fraction: 0.1
    end_fraction: 1.0
    schedule: "linear"  # linear, exponential, step

# Evaluation
evaluation:
  # Compare with baselines
  compare_baselines:
    - "random_selection"
    - "stratified_selection"
    - "uncertainty_selection"
    
  # Metrics to track
  track_metrics:
    - "training_time"
    - "final_accuracy"
    - "convergence_speed"
    - "generalization_gap"

# Hyperparameters
hyperparameters:
  # Tunable parameters
  tunable:
    subset_fraction:
      type: "float"
      range: [0.05, 0.3]
      
    diversity_weight:
      type: "float"
      range: [0.1, 0.5]
      
    distance_metric:
      type: "categorical"
      choices: ["euclidean", "cosine"]

# Expected results
expected_results:
  speedup: "5-10x"
  accuracy_retention: "95-98%"
  memory_reduction: "90%"
  convergence_epochs: "50% fewer"

# Logging and debugging
logging:
  verbose: true
  log_selection_process: true
  save_selected_indices: true
  save_distance_matrix: false
  visualize_selection: true

# References
references:
  - paper: "Sener & Savarese (2018)"
    title: "Active Learning for Convolutional Neural Networks: A Core-Set Approach"
    url: "https://arxiv.org/abs/1708.00489"
    
  - paper: "Mirzasoleiman et al. (2020)"
    title: "Coresets for Data-efficient Training of Machine Learning Models"
    url: "https://arxiv.org/abs/1906.01827"
    
  - paper: "Killamsetty et al. (2021)"
    title: "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning"
    url: "https://arxiv.org/abs/2012.10630"
