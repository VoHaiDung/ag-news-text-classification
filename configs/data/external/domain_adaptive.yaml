# Domain-Adaptive External Data Configuration
# ============================================
#
# Domain-specific data for adaptive pretraining following:
# - Gururangan et al. (2020): "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks"
# - Lee et al. (2020): "BioBERT: a pre-trained biomedical language representation model"
# - Beltagy et al. (2019): "SciBERT: A Pretrained Language Model for Scientific Text"
#
# Author: Võ Hải Dũng
# License: MIT

name: domain_adaptive
type: external_data
description: "Domain-specific data for adaptive pretraining"

# Target domains
domains:
  # News domain
  news:
    weight: 0.4
    sources:
      - "reuters"
      - "ap_news"
      - "bbc"
      - "cnn"
    characteristics:
      style: "journalistic"
      formality: "high"
      objectivity: "high"
      
  # Business domain
  business:
    weight: 0.3
    sources:
      - "financial_times"
      - "wsj"
      - "bloomberg"
      - "forbes"
    characteristics:
      terminology: "financial"
      numerical_density: "high"
      
  # Technology domain
  technology:
    weight: 0.2
    sources:
      - "techcrunch"
      - "ars_technica"
      - "wired"
      - "ieee"
    characteristics:
      terminology: "technical"
      acronym_density: "high"
      
  # Sports domain
  sports:
    weight: 0.1
    sources:
      - "espn"
      - "sports_illustrated"
      - "athletic"
    characteristics:
      style: "narrative"
      emotion: "high"

# Domain identification
identification:
  # Method
  method: "classifier"  # classifier, keywords, clustering
  
  # Classifier-based
  classifier:
    model: "distilbert-base-uncased-finetuned-sst-2-english"
    confidence_threshold: 0.8
    
  # Keyword-based
  keywords:
    use_tfidf: true
    top_k_terms: 100
    min_match_score: 0.5
    
  # Clustering-based
  clustering:
    algorithm: "kmeans"
    num_clusters: 10
    distance_metric: "cosine"

# Data collection
collection:
  # Collection strategy
  strategy: "targeted"  # targeted, broad, adaptive
  
  # Targeted collection
  targeted:
    use_domain_keywords: true
    use_url_patterns: true
    use_metadata: true
    
  # Adaptive collection
  adaptive:
    start_broad: true
    refine_iteratively: true
    feedback_mechanism: "model_performance"

# Domain characteristics analysis
analysis:
  # Vocabulary analysis
  vocabulary:
    extract_domain_terms: true
    compute_term_frequency: true
    identify_neologisms: true
    
  # Style analysis
  style:
    analyze_sentence_length: true
    analyze_complexity: true
    analyze_formality: true
    
  # Topic analysis
  topics:
    use_lda: true
    num_topics: 50
    analyze_evolution: true

# Domain mixing
mixing:
  # Mixing strategy
  strategy: "weighted"  # weighted, uniform, curriculum
  
  # Weighted mixing
  weighted:
    use_domain_weights: true
    adjust_dynamically: false
    
  # Curriculum mixing
  curriculum:
    start_general: true
    increase_specificity: true
    schedule: "linear"
    
  # Temperature sampling
  temperature: 1.0

# Adaptation strategies
adaptation:
  # Vocabulary adaptation
  vocabulary:
    add_domain_tokens: true
    max_new_tokens: 10000
    min_frequency: 10
    
  # Continued pretraining
  pretraining:
    method: "DAPT"  # DAPT (Domain-Adaptive PreTraining), TAPT (Task-Adaptive)
    steps: 50000
    learning_rate: 5e-5
    warmup_steps: 1000
    
  # Task-adaptive pretraining
  task_adaptive:
    use_task_data: true
    mix_ratio: 0.5
    mask_strategy: "whole_word"

# Quality control
quality:
  # Domain relevance
  relevance:
    min_score: 0.7
    scoring_method: "classifier"
    
  # Text quality
  text_quality:
    min_length: 50
    max_length: 2000
    min_quality_score: 0.75
    
  # Diversity
  diversity:
    ensure_topic_diversity: true
    min_unique_terms: 1000
    max_repetition: 0.3

# Processing pipeline
pipeline:
  # Steps
  steps:
    - name: "collect"
      parallel: true
      
    - name: "identify_domain"
      batch_size: 100
      
    - name: "filter_quality"
      threshold: 0.7
      
    - name: "extract_features"
      features: ["vocabulary", "style", "topics"]
      
    - name: "mix_domains"
      strategy: "weighted"
      
  # Resources
  num_workers: 8
  batch_size: 500

# Output
output:
  # Format
  format: "jsonl"
  include_metadata: true
  
  # Organization
  organize_by_domain: true
  separate_files: true
  
  # Statistics
  generate_stats: true
  stats_per_domain: true

# Evaluation
evaluation:
  # Domain shift metrics
  compute_domain_shift: true
  reference_corpus: "general_news"
  
  # Adaptation effectiveness
  measure_adaptation: true
  baseline_model: "bert-base-uncased"
  
  # Downstream impact
  evaluate_downstream: true
  downstream_task: "ag_news_classification"

# Expected impact
expected_impact:
  domain_performance: "+3-5%"
  generalization: "maintained"
  vocabulary_coverage: "+15%"
  training_efficiency: "2x faster convergence"

# References
references:
  - paper: "Gururangan et al. (2020)"
    title: "Don't Stop Pretraining"
    url: "https://arxiv.org/abs/2004.10964"
    
  - paper: "Lee et al. (2020)"
    title: "BioBERT"
    url: "https://arxiv.org/abs/1901.08746"
