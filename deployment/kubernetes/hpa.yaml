# Kubernetes Horizontal Pod Autoscaler Configuration for AG News
# ================================================================================
# This configuration defines horizontal pod autoscaling policies for automatic
# scaling of AG News services based on various metrics including CPU, memory,
# and custom metrics.
#
# Scaling strategies:
#   - CPU-based scaling for general workloads
#   - Memory-based scaling for memory-intensive services
#   - Custom metrics for ML-specific scaling
#   - Predictive scaling for anticipated load
#
# References:
#   - HPA Documentation: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
#   - HPA Walkthrough: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/
#   - Custom Metrics: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics
#
# Author: Võ Hải Dũng
# License: MIT

---
# HPA for Inference Service
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: inference-hpa
  namespace: agnews-production
  labels:
    app: agnews
    component: inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-inference
  minReplicas: 3
  maxReplicas: 20
  metrics:
  # CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # Memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # Custom metric: Request rate
  - type: Pods
    pods:
      metric:
        name: inference_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  # Custom metric: Queue depth
  - type: External
    external:
      metric:
        name: redis_queue_depth
        selector:
          matchLabels:
            queue_name: "inference"
      target:
        type: Value
        value: "1000"
  # Custom metric: Response time
  - type: Pods
    pods:
      metric:
        name: inference_p95_latency_ms
      target:
        type: AverageValue
        averageValue: "500"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        value: 4
        periodSeconds: 60
      - type: Percent
        value: 100
        periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 2
        periodSeconds: 60
      - type: Percent
        value: 50
        periodSeconds: 60
      selectPolicy: Min

---
# HPA for Data Processor
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: data-processor-hpa
  namespace: agnews-data
  labels:
    app: agnews
    component: data-processor
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: data-processor
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 75
  # Kafka lag metric
  - type: External
    external:
      metric:
        name: kafka_consumer_lag
        selector:
          matchLabels:
            consumer_group: "agnews-processors"
      target:
        type: Value
        value: "10000"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Pods
        value: 2
        periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 600
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60

---
# HPA for Celery Workers
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: celery-worker-hpa
  namespace: agnews-production
  labels:
    app: agnews
    component: celery-worker
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: celery-worker
  minReplicas: 2
  maxReplicas: 15
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 85
  # Celery queue size
  - type: External
    external:
      metric:
        name: celery_queue_length
        selector:
          matchLabels:
            queue: "default"
      target:
        type: Value
        value: "100"
  # Task processing rate
  - type: Pods
    pods:
      metric:
        name: celery_tasks_per_minute
      target:
        type: AverageValue
        averageValue: "60"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 45
      policies:
      - type: Pods
        value: 3
        periodSeconds: 45
      - type: Percent
        value: 50
        periodSeconds: 45
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 120

---
# Vertical Pod Autoscaler for Training Controller
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: training-controller-vpa
  namespace: agnews-production
  labels:
    app: agnews
    component: training-controller
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: training-controller
  updatePolicy:
    updateMode: "Auto"  # Can be "Off", "Initial", or "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: training-controller
      minAllowed:
        cpu: 100m
        memory: 256Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
      controlledResources: ["cpu", "memory"]

---
# Custom HPA for ML Model Serving with Predictive Scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-serving-predictive-hpa
  namespace: agnews-production
  labels:
    app: agnews
    component: ml-serving
    scaling-type: predictive
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-inference
  minReplicas: 5
  maxReplicas: 30
  metrics:
  # GPU utilization (if applicable)
  - type: Resource
    resource:
      name: nvidia.com/gpu
      target:
        type: Utilization
        averageUtilization: 80
  # Model inference throughput
  - type: Pods
    pods:
      metric:
        name: model_inference_throughput
      target:
        type: AverageValue
        averageValue: "1000"  # inferences per second
  # Batch queue depth
  - type: External
    external:
      metric:
        name: batch_queue_depth
        selector:
          matchLabels:
            model: "deberta-v3"
      target:
        type: Value
        value: "500"
  # Time-based predictive scaling
  - type: External
    external:
      metric:
        name: predicted_load
        selector:
          matchLabels:
            time_window: "1h"
      target:
        type: Value
        value: "80"  # percentage of peak load
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Pods
        value: 5
        periodSeconds: 30  # Aggressive scaling for ML workloads
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 600  # Slower scale down to avoid model reloading
      policies:
      - type: Pods
        value: 2
        periodSeconds: 300
      selectPolicy: Min
