"""
Generated Protocol Buffer code for Training Service
================================================================================
This file is automatically generated by the Protocol Buffer compiler.
DO NOT EDIT MANUALLY - changes will be overwritten.

Generated from: src/api/grpc/protos/training.proto

Defines messages for model training operations, job management, and monitoring.
"""

import sys
from typing import Optional, List, Dict, Any
from datetime import datetime

_b = sys.version_info[0] < 3 and (lambda x: x) or (lambda x: x.encode('latin1'))

from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
from google.protobuf import timestamp_pb2
from google.protobuf import duration_pb2

# Symbol database
_sym_db = _symbol_database.Default()

# Import common types
from .common import types_pb2, status_pb2


class TrainingConfig(_message.Message):
    """
    Training configuration message
    
    Attributes:
        model_type: Type of model to train
        hyperparameters: Training hyperparameters
        data_config: Data configuration
        optimization_config: Optimization settings
        resource_config: Resource requirements
        experiment_config: Experiment tracking settings
    """
    
    class ModelType:
        """Model type enumeration"""
        UNSPECIFIED = 0
        DEBERTA_V3 = 1
        ROBERTA = 2
        XLNET = 3
        ELECTRA = 4
        LONGFORMER = 5
        GPT2 = 6
        T5 = 7
        ENSEMBLE = 8
        DISTILLED = 9
    
    class HyperParameters(_message.Message):
        """Training hyperparameters"""
        __slots__ = ['learning_rate', 'batch_size', 'num_epochs', 'warmup_steps',
                     'weight_decay', 'gradient_clip_norm', 'adam_epsilon',
                     'max_sequence_length', 'dropout_rate', 'label_smoothing']
        
        def __init__(self,
                     learning_rate: float = 2e-5,
                     batch_size: int = 32,
                     num_epochs: int = 3,
                     warmup_steps: int = 500,
                     weight_decay: float = 0.01,
                     gradient_clip_norm: float = 1.0,
                     adam_epsilon: float = 1e-8,
                     max_sequence_length: int = 512,
                     dropout_rate: float = 0.1,
                     label_smoothing: float = 0.0):
            super(TrainingConfig.HyperParameters, self).__init__()
            self.learning_rate = learning_rate
            self.batch_size = batch_size
            self.num_epochs = num_epochs
            self.warmup_steps = warmup_steps
            self.weight_decay = weight_decay
            self.gradient_clip_norm = gradient_clip_norm
            self.adam_epsilon = adam_epsilon
            self.max_sequence_length = max_sequence_length
            self.dropout_rate = dropout_rate
            self.label_smoothing = label_smoothing
    
    class DataConfig(_message.Message):
        """Data configuration"""
        __slots__ = ['dataset_id', 'train_split', 'validation_split', 'test_split',
                     'augmentation_enabled', 'augmentation_types', 'sampling_strategy']
        
        def __init__(self,
                     dataset_id: str = "",
                     train_split: float = 0.8,
                     validation_split: float = 0.1,
                     test_split: float = 0.1,
                     augmentation_enabled: bool = False,
                     augmentation_types: Optional[List[str]] = None,
                     sampling_strategy: str = "balanced"):
            super(TrainingConfig.DataConfig, self).__init__()
            self.dataset_id = dataset_id
            self.train_split = train_split
            self.validation_split = validation_split
            self.test_split = test_split
            self.augmentation_enabled = augmentation_enabled
            if augmentation_types:
                self.augmentation_types.extend(augmentation_types)
            self.sampling_strategy = sampling_strategy
    
    class OptimizationConfig(_message.Message):
        """Optimization configuration"""
        __slots__ = ['optimizer', 'scheduler', 'mixed_precision', 'gradient_accumulation_steps',
                     'distributed_training', 'num_gpus', 'early_stopping_patience']
        
        def __init__(self,
                     optimizer: str = "adamw",
                     scheduler: str = "cosine",
                     mixed_precision: bool = True,
                     gradient_accumulation_steps: int = 1,
                     distributed_training: bool = False,
                     num_gpus: int = 1,
                     early_stopping_patience: int = 3):
            super(TrainingConfig.OptimizationConfig, self).__init__()
            self.optimizer = optimizer
            self.scheduler = scheduler
            self.mixed_precision = mixed_precision
            self.gradient_accumulation_steps = gradient_accumulation_steps
            self.distributed_training = distributed_training
            self.num_gpus = num_gpus
            self.early_stopping_patience = early_stopping_patience
    
    class ResourceConfig(_message.Message):
        """Resource configuration"""
        __slots__ = ['cpu', 'memory', 'gpu', 'gpu_type', 'disk_size']
        
        def __init__(self,
                     cpu: str = "4",
                     memory: str = "16Gi",
                     gpu: int = 1,
                     gpu_type: str = "V100",
                     disk_size: str = "100Gi"):
            super(TrainingConfig.ResourceConfig, self).__init__()
            self.cpu = cpu
            self.memory = memory
            self.gpu = gpu
            self.gpu_type = gpu_type
            self.disk_size = disk_size
    
    __slots__ = ['model_type', 'hyperparameters', 'data_config', 
                 'optimization_config', 'resource_config', 'experiment_config']
    
    def __init__(self,
                 model_type: int = 0,
                 hyperparameters: Optional[HyperParameters] = None,
                 data_config: Optional[DataConfig] = None,
                 optimization_config: Optional[OptimizationConfig] = None,
                 resource_config: Optional[ResourceConfig] = None,
                 experiment_config: Optional[Dict[str, str]] = None):
        super(TrainingConfig, self).__init__()
        self.model_type = model_type
        if hyperparameters:
            self.hyperparameters.CopyFrom(hyperparameters)
        if data_config:
            self.data_config.CopyFrom(data_config)
        if optimization_config:
            self.optimization_config.CopyFrom(optimization_config)
        if resource_config:
            self.resource_config.CopyFrom(resource_config)
        if experiment_config:
            self.experiment_config.update(experiment_config)


class TrainingJob(_message.Message):
    """
    Training job information
    
    Attributes:
        job_id: Unique job identifier
        name: Job name
        status: Job status
        config: Training configuration
        metrics: Training metrics
        created_at: Creation timestamp
        started_at: Start timestamp
        completed_at: Completion timestamp
        error_message: Error message if failed
    """
    
    class Status:
        """Job status enumeration"""
        UNKNOWN = 0
        PENDING = 1
        PREPARING = 2
        RUNNING = 3
        COMPLETED = 4
        FAILED = 5
        CANCELLED = 6
        PAUSED = 7
    
    class Metrics(_message.Message):
        """Training metrics"""
        __slots__ = ['epoch', 'step', 'loss', 'accuracy', 'validation_loss',
                     'validation_accuracy', 'learning_rate', 'custom_metrics']
        
        def __init__(self,
                     epoch: int = 0,
                     step: int = 0,
                     loss: float = 0.0,
                     accuracy: float = 0.0,
                     validation_loss: float = 0.0,
                     validation_accuracy: float = 0.0,
                     learning_rate: float = 0.0,
                     custom_metrics: Optional[Dict[str, float]] = None):
            super(TrainingJob.Metrics, self).__init__()
            self.epoch = epoch
            self.step = step
            self.loss = loss
            self.accuracy = accuracy
            self.validation_loss = validation_loss
            self.validation_accuracy = validation_accuracy
            self.learning_rate = learning_rate
            if custom_metrics:
                self.custom_metrics.update(custom_metrics)
    
    __slots__ = ['job_id', 'name', 'status', 'config', 'metrics',
                 'created_at', 'started_at', 'completed_at', 'error_message']
    
    def __init__(self,
                 job_id: str = "",
                 name: str = "",
                 status: int = 0,
                 config: Optional[TrainingConfig] = None,
                 metrics: Optional[Metrics] = None,
                 created_at: Optional[timestamp_pb2.Timestamp] = None,
                 started_at: Optional[timestamp_pb2.Timestamp] = None,
                 completed_at: Optional[timestamp_pb2.Timestamp] = None,
                 error_message: str = ""):
        super(TrainingJob, self).__init__()
        self.job_id = job_id
        self.name = name
        self.status = status
        if config:
            self.config.CopyFrom(config)
        if metrics:
            self.metrics.CopyFrom(metrics)
        if created_at:
            self.created_at.CopyFrom(created_at)
        if started_at:
            self.started_at.CopyFrom(started_at)
        if completed_at:
            self.completed_at.CopyFrom(completed_at)
        self.error_message = error_message


class StartTrainingRequest(_message.Message):
    """
    Request to start training
    
    Attributes:
        config: Training configuration
        job_name: Optional job name
        tags: Job tags for organization
        callbacks: Callback configurations
    """
    __slots__ = ['config', 'job_name', 'tags', 'callbacks']
    
    def __init__(self,
                 config: Optional[TrainingConfig] = None,
                 job_name: str = "",
                 tags: Optional[Dict[str, str]] = None,
                 callbacks: Optional[List[str]] = None):
        super(StartTrainingRequest, self).__init__()
        if config:
            self.config.CopyFrom(config)
        self.job_name = job_name
        if tags:
            self.tags.update(tags)
        if callbacks:
            self.callbacks.extend(callbacks)


class StartTrainingResponse(_message.Message):
    """
    Response for training start request
    
    Attributes:
        job: Training job information
        estimated_duration: Estimated training duration
        metadata: Response metadata
    """
    __slots__ = ['job', 'estimated_duration', 'metadata']
    
    def __init__(self,
                 job: Optional[TrainingJob] = None,
                 estimated_duration: Optional[duration_pb2.Duration] = None,
                 metadata: Optional[types_pb2.ResponseMetadata] = None):
        super(StartTrainingResponse, self).__init__()
        if job:
            self.job.CopyFrom(job)
        if estimated_duration:
            self.estimated_duration.CopyFrom(estimated_duration)
        if metadata:
            self.metadata.CopyFrom(metadata)


class StopTrainingRequest(_message.Message):
    """
    Request to stop training
    
    Attributes:
        job_id: Job to stop
        save_checkpoint: Save current checkpoint
        reason: Stop reason
    """
    __slots__ = ['job_id', 'save_checkpoint', 'reason']
    
    def __init__(self,
                 job_id: str = "",
                 save_checkpoint: bool = True,
                 reason: str = ""):
        super(StopTrainingRequest, self).__init__()
        self.job_id = job_id
        self.save_checkpoint = save_checkpoint
        self.reason = reason


class StopTrainingResponse(_message.Message):
    """
    Response for training stop request
    
    Attributes:
        job: Updated job information
        checkpoint_path: Path to saved checkpoint
        metadata: Response metadata
    """
    __slots__ = ['job', 'checkpoint_path', 'metadata']
    
    def __init__(self,
                 job: Optional[TrainingJob] = None,
                 checkpoint_path: str = "",
                 metadata: Optional[types_pb2.ResponseMetadata] = None):
        super(StopTrainingResponse, self).__init__()
        if job:
            self.job.CopyFrom(job)
        self.checkpoint_path = checkpoint_path
        if metadata:
            self.metadata.CopyFrom(metadata)


class GetTrainingJobRequest(_message.Message):
    """
    Request for training job information
    
    Attributes:
        job_id: Job identifier
        include_logs: Include training logs
        include_metrics_history: Include full metrics history
    """
    __slots__ = ['job_id', 'include_logs', 'include_metrics_history']
    
    def __init__(self,
                 job_id: str = "",
                 include_logs: bool = False,
                 include_metrics_history: bool = False):
        super(GetTrainingJobRequest, self).__init__()
        self.job_id = job_id
        self.include_logs = include_logs
        self.include_metrics_history = include_metrics_history


class GetTrainingJobResponse(_message.Message):
    """
    Response with training job information
    
    Attributes:
        job: Training job information
        logs: Training logs
        metrics_history: Metrics history
        metadata: Response metadata
    """
    __slots__ = ['job', 'logs', 'metrics_history', 'metadata']
    
    def __init__(self,
                 job: Optional[TrainingJob] = None,
                 logs: Optional[List[str]] = None,
                 metrics_history: Optional[List[TrainingJob.Metrics]] = None,
                 metadata: Optional[types_pb2.ResponseMetadata] = None):
        super(GetTrainingJobResponse, self).__init__()
        if job:
            self.job.CopyFrom(job)
        if logs:
            self.logs.extend(logs)
        if metrics_history:
            self.metrics_history.extend(metrics_history)
        if metadata:
            self.metadata.CopyFrom(metadata)


class ListTrainingJobsRequest(_message.Message):
    """
    Request for listing training jobs
    
    Attributes:
        filter: Filter expression
        page_size: Number of jobs per page
        page_token: Pagination token
        sort_by: Sort field
        sort_order: Sort order
    """
    __slots__ = ['filter', 'page_size', 'page_token', 'sort_by', 'sort_order']
    
    def __init__(self,
                 filter: str = "",
                 page_size: int = 10,
                 page_token: str = "",
                 sort_by: str = "created_at",
                 sort_order: str = "DESC"):
        super(ListTrainingJobsRequest, self).__init__()
        self.filter = filter
        self.page_size = page_size
        self.page_token = page_token
        self.sort_by = sort_by
        self.sort_order = sort_order


class ListTrainingJobsResponse(_message.Message):
    """
    Response with list of training jobs
    
    Attributes:
        jobs: List of training jobs
        next_page_token: Token for next page
        total_count: Total number of jobs
        metadata: Response metadata
    """
    __slots__ = ['jobs', 'next_page_token', 'total_count', 'metadata']
    
    def __init__(self,
                 jobs: Optional[List[TrainingJob]] = None,
                 next_page_token: str = "",
                 total_count: int = 0,
                 metadata: Optional[types_pb2.ResponseMetadata] = None):
        super(ListTrainingJobsResponse, self).__init__()
        if jobs:
            self.jobs.extend(jobs)
        self.next_page_token = next_page_token
        self.total_count = total_count
        if metadata:
            self.metadata.CopyFrom(metadata)


# Register message types
_sym_db.RegisterMessage(TrainingConfig)
_sym_db.RegisterMessage(TrainingJob)
_sym_db.RegisterMessage(StartTrainingRequest)
_sym_db.RegisterMessage(StartTrainingResponse)
_sym_db.RegisterMessage(StopTrainingRequest)
_sym_db.RegisterMessage(StopTrainingResponse)
_sym_db.RegisterMessage(GetTrainingJobRequest)
_sym_db.RegisterMessage(GetTrainingJobResponse)
_sym_db.RegisterMessage(ListTrainingJobsRequest)
_sym_db.RegisterMessage(ListTrainingJobsResponse)
