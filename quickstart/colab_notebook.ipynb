{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AG News Text Classification - Google Colab Quick Start\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a complete quick start guide for AG News classification in Google Colab, following methodologies from:\n",
    "- Wing (2006): \"Computational Thinking\"\n",
    "- Guzdial (2015): \"Learner-Centered Design of Computing Education\"\n",
    "- Zhang et al. (2015): \"Character-level Convolutional Networks for Text Classification\"\n",
    "\n",
    "### Learning Objectives\n",
    "1. Set up AG News classification environment in Colab\n",
    "2. Load and explore the dataset\n",
    "3. Train a transformer-based classifier\n",
    "4. Evaluate model performance\n",
    "5. Deploy model for inference\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"Warning: GPU not available. Training will be slower.\")\n",
    "    print(\"To enable GPU: Runtime -> Change runtime type -> Hardware accelerator -> GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/VoHaiDung/ag-news-text-classification.git\n",
    "%cd ag-news-text-classification\n",
    "\n",
    "# Verify repository structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"Installing required packages...\")\n",
    "!pip install -q -r requirements/minimal.txt\n",
    "\n",
    "# Import additional libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download AG News dataset\n",
    "print(\"Downloading AG News dataset...\")\n",
    "!python scripts/setup/download_all_data.py --dataset ag_news\n",
    "\n",
    "# Prepare data splits\n",
    "print(\"\\nPreparing data splits...\")\n",
    "!python scripts/data_preparation/prepare_ag_news.py\n",
    "\n",
    "print(\"\\nData preparation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore data\n",
    "data_dir = Path(\"data/processed\")\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(data_dir / \"train.csv\")\n",
    "val_df = pd.read_csv(data_dir / \"validation.csv\")\n",
    "test_df = pd.read_csv(data_dir / \"test.csv\")\n",
    "\n",
    "# Define class names\n",
    "AG_NEWS_CLASSES = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"Total samples: {len(train_df) + len(val_df) + len(test_df):,}\")\n",
    "print(f\"Number of classes: {len(AG_NEWS_CLASSES)}\")\n",
    "print(f\"Classes: {', '.join(AG_NEWS_CLASSES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "print(\"Sample Training Data:\")\n",
    "print(\"=\"*80)\n",
    "print(train_df.head())\n",
    "\n",
    "# Label distribution\n",
    "print(\"\\nLabel Distribution in Training Set:\")\n",
    "print(\"=\"*50)\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(train_df)) * 100\n",
    "    print(f\"  {AG_NEWS_CLASSES[label]}: {count:,} samples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts from each category\n",
    "print(\"Sample Texts from Each Category:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for label in range(len(AG_NEWS_CLASSES)):\n",
    "    print(f\"\\n{AG_NEWS_CLASSES[label].upper()}:\")\n",
    "    print(\"-\"*40)\n",
    "    samples = train_df[train_df['label'] == label]['text'].sample(2, random_state=42)\n",
    "    for i, text in enumerate(samples, 1):\n",
    "        # Truncate for display\n",
    "        display_text = text[:200] + \"...\" if len(text) > 200 else text\n",
    "        print(f\"  {i}. {display_text}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "train_df['word_count'] = train_df['text'].str.split().str.len()\n",
    "train_df['char_count'] = train_df['text'].str.len()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Average words per text: {train_df['word_count'].mean():.1f}\")\n",
    "print(f\"Std dev of word count: {train_df['word_count'].std():.1f}\")\n",
    "print(f\"Min words: {train_df['word_count'].min()}\")\n",
    "print(f\"Max words: {train_df['word_count'].max()}\")\n",
    "print(f\"Median words: {train_df['word_count'].median():.0f}\")\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_df['word_count'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Count Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for label in range(len(AG_NEWS_CLASSES)):\n",
    "    subset = train_df[train_df['label'] == label]['word_count']\n",
    "    plt.hist(subset, bins=30, alpha=0.5, label=AG_NEWS_CLASSES[label])\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Count by Category')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Transformers library imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class AGNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for AG News classification.\n",
    "    \n",
    "    Following dataset design patterns from:\n",
    "    - Paszke et al. (2019): \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # Fast and efficient for quick start\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(AG_NEWS_CLASSES)\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "train_dataset = AGNewsDataset(\n",
    "    train_df[\"text\"].values,\n",
    "    train_df[\"label\"].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = AGNewsDataset(\n",
    "    val_df[\"text\"].values,\n",
    "    val_df[\"label\"].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = AGNewsDataset(\n",
    "    test_df[\"text\"].values,\n",
    "    test_df[\"label\"].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 2  # Quick training for demo\n",
    "BATCH_SIZE = 32 if torch.cuda.is_available() else 16\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(WARMUP_RATIO * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {int(WARMUP_RATIO * total_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item():.4f})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on dataset.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return accuracy, avg_loss, all_preds, all_labels\n",
    "\n",
    "# Training\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_accuracy = 0\n",
    "training_history = {\"train_loss\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    training_history[\"train_loss\"].append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_accuracy, val_loss, _, _ = evaluate(model, val_loader, device)\n",
    "    training_history[\"val_loss\"].append(val_loss)\n",
    "    training_history[\"val_accuracy\"].append(val_accuracy)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        print(f\"  New best model! Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "\n",
    "test_accuracy, test_loss, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(test_labels, test_preds, target_names=AG_NEWS_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=AG_NEWS_CLASSES, \n",
    "            yticklabels=AG_NEWS_CLASSES)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\"*50)\n",
    "for i, class_name in enumerate(AG_NEWS_CLASSES):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i].sum()\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"{class_name}: {class_acc:.4f} ({class_correct}/{class_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text: str, model, tokenizer, device) -> Tuple[str, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Predict class for a single text input.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text to classify\n",
    "    model : transformers.PreTrainedModel\n",
    "        Trained model\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        Tokenizer for the model\n",
    "    device : torch.device\n",
    "        Device to run inference on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[str, float, np.ndarray]\n",
    "        Predicted class name, confidence score, and all probabilities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    pred_class = AG_NEWS_CLASSES[pred.item()]\n",
    "    confidence = probs[0][pred].item()\n",
    "    \n",
    "    return pred_class, confidence, probs[0].cpu().numpy()\n",
    "\n",
    "# Test predictions\n",
    "test_texts = [\n",
    "    \"Apple announces new iPhone with revolutionary camera system and AI features\",\n",
    "    \"Stock market reaches all-time high amid economic recovery optimism\",\n",
    "    \"Scientists discover potential signs of life on distant exoplanet\",\n",
    "    \"Local team wins championship in thrilling overtime victory against rivals\",\n",
    "    \"UN Security Council meets to discuss international peace efforts\"\n",
    "]\n",
    "\n",
    "print(\"Interactive Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for text in test_texts:\n",
    "    pred_class, confidence, probs = predict_text(text, model, tokenizer, device)\n",
    "    \n",
    "    print(f\"\\nText: {text[:60]}...\")\n",
    "    print(f\"Predicted: {pred_class} (confidence: {confidence:.4f})\")\n",
    "    print(f\"All probabilities:\")\n",
    "    for i, prob in enumerate(probs):\n",
    "        print(f\"  {AG_NEWS_CLASSES[i]}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "output_dir = Path(\"outputs/colab_model\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving model to {output_dir}...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save training history\n",
    "history_path = output_dir / \"training_history.json\"\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"model_name\": model_name,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"best_accuracy\": best_accuracy,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"author\": \"Võ Hải Dũng\",\n",
    "    \"email\": \"vohaidung.work@gmail.com\"\n",
    "}\n",
    "\n",
    "metadata_path = output_dir / \"metadata.json\"\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Model saved successfully!\")\n",
    "print(f\"Files saved:\")\n",
    "for file in output_dir.glob(\"*\"):\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading\n",
    "print(\"\\nTesting model loading...\")\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Test prediction with loaded model\n",
    "test_text = \"Breaking news: Major technological breakthrough announced\"\n",
    "loaded_model = loaded_model.to(device)\n",
    "pred_class, confidence, _ = predict_text(test_text, loaded_model, loaded_tokenizer, device)\n",
    "\n",
    "print(f\"Test prediction with loaded model:\")\n",
    "print(f\"  Text: {test_text}\")\n",
    "print(f\"  Prediction: {pred_class} (confidence: {confidence:.4f})\")\n",
    "print(\"\\nModel loaded and verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file with results\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "zip_filename = f\"colab_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for file in output_dir.rglob('*'):\n",
    "        if file.is_file():\n",
    "            zipf.write(file, file.relative_to(output_dir.parent))\n",
    "\n",
    "print(f\"Results compressed to: {zip_filename}\")\n",
    "\n",
    "# Download (only works in Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_filename)\n",
    "    print(\"Download started!\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab. File saved locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Next Steps\n",
    "\n",
    "### Summary\n",
    "\n",
    "You have successfully completed the AG News text classification quick start:\n",
    "\n",
    "1. **Environment Setup**: Configured Google Colab with GPU support\n",
    "2. **Data Preparation**: Loaded and explored AG News dataset\n",
    "3. **Model Training**: Trained DistilBERT classifier with {best_accuracy:.2%} validation accuracy\n",
    "4. **Evaluation**: Achieved {test_accuracy:.2%} test accuracy\n",
    "5. **Deployment**: Saved model for future use\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- Dataset is well-balanced across 4 categories\n",
    "- Text lengths suitable for standard transformer models\n",
    "- DistilBERT provides good balance of speed and accuracy\n",
    "- Model performs well even with minimal training\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Improve Performance**:\n",
    "   - Try larger models (RoBERTa, DeBERTa)\n",
    "   - Increase training epochs\n",
    "   - Experiment with hyperparameters\n",
    "   - Apply data augmentation\n",
    "\n",
    "2. **Advanced Techniques**:\n",
    "   - Implement ensemble methods\n",
    "   - Try prompt-based learning\n",
    "   - Explore few-shot learning\n",
    "   - Use advanced training strategies\n",
    "\n",
    "3. **Production Deployment**:\n",
    "   - Optimize model for inference\n",
    "   - Deploy via REST API\n",
    "   - Implement monitoring\n",
    "   - Add A/B testing\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Full Documentation**: [GitHub Repository](https://github.com/VoHaiDung/ag-news-text-classification)\n",
    "- **Advanced Notebooks**: See `notebooks/` directory\n",
    "- **API Examples**: Check `quickstart/api_quickstart.py`\n",
    "- **Contact**: vohaidung.work@gmail.com\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for using this quick start guide!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
