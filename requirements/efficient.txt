# ============================================================================
# Efficient Training Requirements for AG News Text Classification
# ============================================================================
# Project: AG News Text Classification (ag-news-text-classification)
# Description: Parameter-efficient fine-tuning and memory optimization
# Author: Võ Hải Dũng
# License: MIT
# Python: >=3.8,<3.12
# ============================================================================
# This file contains packages required for:
# - LoRA (Low-Rank Adaptation) fine-tuning
# - QLoRA (Quantized LoRA) with 4-bit/8-bit quantization
# - Adapter modules (Houlsby, Pfeiffer, Parallel)
# - Prefix tuning and prompt tuning
# - IA3 (Infused Adapter by Inhibiting and Amplifying)
# - Gradient checkpointing and memory optimization
# - Mixed precision training
# - Model quantization and pruning
# - Efficient inference optimization
# ============================================================================

# Include base requirements
-r base.txt

# ----------------------------------------------------------------------------
# Core Efficient Fine-Tuning Libraries
# ----------------------------------------------------------------------------
# PEFT for parameter-efficient fine-tuning
peft>=0.7.0,<0.12.0

# Adapters for transformer models
adapters>=0.2.0,<0.3.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Quantization Libraries
# ----------------------------------------------------------------------------
# BitsAndBytes for 4-bit/8-bit quantization
bitsandbytes>=0.41.0,<0.44.0

# AutoGPTQ for GPTQ quantization
auto-gptq>=0.6.0,<0.8.0; python_version >= "3.9"

# GPTQ-for-LLaMA
gptq-for-llama>=0.1.0,<0.2.0; python_version >= "3.9" and platform_system == "Linux"

# Optimum for quantization
optimum>=1.16.0,<1.21.0

# Intel Neural Compressor
neural-compressor>=2.4.0,<2.6.0

# ----------------------------------------------------------------------------
# LoRA-Specific Tools
# ----------------------------------------------------------------------------
# LoRA implementation (via PEFT)
# peft>=0.7.0 (already included above)

# LoRA weight merging utilities
# Custom implementation in src/models/efficient/lora/lora_utils.py

# ----------------------------------------------------------------------------
# Mixed Precision Training
# ----------------------------------------------------------------------------
# Accelerate for mixed precision
accelerate>=0.25.0,<0.31.0

# NVIDIA Apex for mixed precision
apex>=0.1; platform_system != "Windows"

# ----------------------------------------------------------------------------
# Gradient Checkpointing
# ----------------------------------------------------------------------------
# Gradient checkpointing utilities
# Built-in to transformers and PyTorch

# FairScale for activation checkpointing
fairscale>=0.4.13,<0.5.0

# ----------------------------------------------------------------------------
# Memory Optimization
# ----------------------------------------------------------------------------
# DeepSpeed ZeRO for memory optimization
deepspeed>=0.12.0,<0.15.0; platform_system != "Windows"

# FairScale for FSDP
fairscale>=0.4.13,<0.5.0

# PyTorch FSDP
# Built-in to PyTorch 2.0+

# ----------------------------------------------------------------------------
# Model Pruning
# ----------------------------------------------------------------------------
# Torch Pruning
torch-pruning>=1.3.0,<1.4.0

# Neural Network Pruning
# Custom implementation

# SparseGPT for LLM pruning
sparsegpt>=0.1.0,<0.2.0; python_version >= "3.9"

# Wanda pruning
wanda>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Knowledge Distillation
# ----------------------------------------------------------------------------
# Torch Distillation
# Custom implementation in src/training/strategies/distillation/

# Transformers KD utilities
# Via transformers library

# ----------------------------------------------------------------------------
# Flash Attention for Efficient Training
# ----------------------------------------------------------------------------
# Flash Attention 2
flash-attn>=2.4.0,<2.6.0; platform_system == "Linux" and python_version >= "3.9"

# xFormers for memory-efficient attention
xformers>=0.0.23,<0.0.27; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Efficient Transformers
# ----------------------------------------------------------------------------
# Longformer for efficient long sequences
# Via transformers library

# BigBird for sparse attention
# Via transformers library

# Linformer
# Custom implementation or use efficient-transformers

# ----------------------------------------------------------------------------
# Gradient Accumulation
# ----------------------------------------------------------------------------
# Gradient accumulation utilities
# Built-in to PyTorch and HuggingFace Trainer

# ----------------------------------------------------------------------------
# Dynamic Batching
# ----------------------------------------------------------------------------
# Dynamic batching for variable length sequences
# Custom implementation in src/data/loaders/dynamic_batching.py

# ----------------------------------------------------------------------------
# Efficient Data Loading
# ----------------------------------------------------------------------------
# Datasets library with streaming
datasets>=2.16.0,<2.20.0

# Prefetching data loader
# Custom implementation in src/data/loaders/prefetch_loader.py

# ----------------------------------------------------------------------------
# CPU Offloading
# ----------------------------------------------------------------------------
# Accelerate CPU offloading
# Via accelerate library

# DeepSpeed CPU offloading
# Via deepspeed library

# ----------------------------------------------------------------------------
# Model Parallelism
# ----------------------------------------------------------------------------
# Accelerate for model parallelism
accelerate>=0.25.0,<0.31.0

# DeepSpeed for pipeline parallelism
deepspeed>=0.12.0,<0.15.0; platform_system != "Windows"

# FairScale for model parallelism
fairscale>=0.4.13,<0.5.0

# ----------------------------------------------------------------------------
# Efficient Optimizers
# ----------------------------------------------------------------------------
# 8-bit optimizers (via bitsandbytes)
bitsandbytes>=0.41.0,<0.44.0

# Adafactor for memory-efficient optimization
# Via transformers library

# LAMB optimizer
# Custom implementation or torch-optimizer

# ----------------------------------------------------------------------------
# Gradient Compression
# ----------------------------------------------------------------------------
# Gradient compression
# Custom implementation

# ----------------------------------------------------------------------------
# Low-Rank Decomposition
# ----------------------------------------------------------------------------
# Tensor decomposition
tensorly>=0.8.0,<0.9.0

# ----------------------------------------------------------------------------
# Efficient Inference
# ----------------------------------------------------------------------------
# ONNX Runtime for inference
onnxruntime>=1.16.0,<1.18.0

# ONNX Runtime GPU
onnxruntime-gpu>=1.16.0,<1.18.0

# OpenVINO for inference optimization
openvino>=2023.2.0,<2024.2.0; python_version >= "3.9"

# TensorRT (optional, for NVIDIA GPUs)
# nvidia-tensorrt>=8.6.0,<10.0.0

# ----------------------------------------------------------------------------
# Model Compression
# ----------------------------------------------------------------------------
# PyTorch model compression
# Via torch.quantization

# ONNX model optimization
onnx>=1.15.0,<1.17.0

# ----------------------------------------------------------------------------
# Weight Sharing
# ----------------------------------------------------------------------------
# Weight tying
# Custom implementation

# ----------------------------------------------------------------------------
# Adaptive Computation
# ----------------------------------------------------------------------------
# Early exit mechanisms
# Custom implementation

# ----------------------------------------------------------------------------
# Efficient Embeddings
# ----------------------------------------------------------------------------
# Product quantization for embeddings
faiss-cpu>=1.7.4,<1.9.0

# Hashing embeddings
# Custom implementation

# ----------------------------------------------------------------------------
# Distributed Training Utilities
# ----------------------------------------------------------------------------
# Horovod for distributed training
horovod>=0.28.0,<0.29.0; platform_system != "Windows"

# ----------------------------------------------------------------------------
# Memory Profiling
# ----------------------------------------------------------------------------
# Memory profiler
memory-profiler>=0.61.0,<0.62.0

# PyTorch memory profiler
# Built-in to PyTorch

# GPU memory utilities
gputil>=1.4.0,<1.5.0

# PyNVML for NVIDIA GPU monitoring
pynvml>=11.5.0,<11.6.0

# ----------------------------------------------------------------------------
# Efficient Tokenization
# ----------------------------------------------------------------------------
# Fast tokenizers
tokenizers>=0.15.0,<0.16.0

# SentencePiece for efficient tokenization
sentencepiece>=0.1.99,<0.3.0

# ----------------------------------------------------------------------------
# Caching for Efficiency
# ----------------------------------------------------------------------------
# Disk cache
diskcache>=5.6.3,<5.7.0

# Joblib for caching
joblib>=1.3.2,<1.5.0

# ----------------------------------------------------------------------------
# Efficient Metrics Computation
# ----------------------------------------------------------------------------
# Fast metrics
# Via scikit-learn and custom implementations

# ----------------------------------------------------------------------------
# Lazy Loading
# ----------------------------------------------------------------------------
# Memory-mapped arrays
# Via numpy memmap

# ----------------------------------------------------------------------------
# Efficient Configuration
# ----------------------------------------------------------------------------
# YAML parsing (already in base.txt)
pyyaml>=6.0.1,<7.0.0

# ----------------------------------------------------------------------------
# Profiling and Benchmarking
# ----------------------------------------------------------------------------
# Line profiler
line-profiler>=4.1.0,<4.2.0

# Py-spy
py-spy>=0.3.0,<0.4.0

# Scalene
scalene>=1.5.0,<1.6.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Automatic Mixed Precision (AMP)
# ----------------------------------------------------------------------------
# PyTorch AMP
# Built-in to PyTorch 1.6+

# ----------------------------------------------------------------------------
# Efficient Loss Functions
# ----------------------------------------------------------------------------
# Custom efficient loss implementations
# In src/training/objectives/losses/

# ----------------------------------------------------------------------------
# Efficient Callbacks
# ----------------------------------------------------------------------------
# Memory monitoring callback
# Custom implementation in src/training/callbacks/memory_monitor_callback.py

# ----------------------------------------------------------------------------
# Efficient Batch Size Finding
# ----------------------------------------------------------------------------
# Auto batch size finder
# Via PyTorch Lightning or custom implementation

# ----------------------------------------------------------------------------
# Efficient Hyperparameter Search
# ----------------------------------------------------------------------------
# Optuna for efficient search
optuna>=3.5.0,<3.7.0

# ----------------------------------------------------------------------------
# Efficient Model Serving
# ----------------------------------------------------------------------------
# TorchServe
torchserve>=0.9.0,<0.10.0

# ----------------------------------------------------------------------------
# Efficient File I/O
# ----------------------------------------------------------------------------
# PyArrow for efficient data serialization
pyarrow>=14.0.0,<16.2.0

# HDF5 for large arrays
h5py>=3.10.0,<3.12.0

# ----------------------------------------------------------------------------
# Efficient Random Number Generation
# ----------------------------------------------------------------------------
# NumPy RNG
numpy>=1.24.0,<1.27.0

# ----------------------------------------------------------------------------
# Sharding for Large Models
# ----------------------------------------------------------------------------
# Safetensors for efficient model loading
safetensors>=0.4.0,<0.5.0

# ============================================================================
# Installation Notes for Efficient Training Requirements
# ============================================================================
# 1. Install efficient training dependencies:
#    pip install -r requirements/efficient.txt
#
# 2. LoRA fine-tuning example:
#    python scripts/training/single_model/train_xlarge_lora.py
#
# 3. QLoRA fine-tuning example:
#    python scripts/training/single_model/train_llm_qlora.py
#
# 4. Check memory usage:
#    python -c "import torch; print(f'CUDA: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
#
# 5. Gradient checkpointing:
#    model.gradient_checkpointing_enable()
#
# 6. Mixed precision training:
#    accelerate launch scripts/training/train_single_model.py --mixed_precision fp16
#
# 7. DeepSpeed ZeRO:
#    deepspeed scripts/training/train_single_model.py --deepspeed configs/training/efficient/zero3.json
#
# 8. Flash Attention installation (Linux only):
#    pip install flash-attn --no-build-isolation
#
# 9. Profile memory:
#    python -m memory_profiler scripts/training/train_single_model.py
#
# 10. Quantize model:
#     python scripts/optimization/quantization_optimization.py
#
# Parameter-efficient methods comparison:
# Method         | Trainable Params | Memory   | Speed    | Accuracy
# Full FT        | 100%             | High     | Slow     | Highest
# LoRA           | 0.1-1%           | Low      | Fast     | High
# QLoRA 4-bit    | 0.1-1%           | Very Low | Fast     | High
# Adapters       | 2-4%             | Low      | Medium   | High
# Prefix Tuning  | 0.1-3%           | Low      | Fast     | Medium
# Prompt Tuning  | <0.01%           | Very Low | Very Fast| Medium
# IA3            | 0.01-0.1%        | Very Low | Fast     | Medium
#
# LoRA configurations for AG News:
# Model Size     | Rank | Alpha | Trainable Params | Memory
# DeBERTa-base   | 8    | 16    | ~0.3M           | 4GB
# DeBERTa-large  | 16   | 32    | ~1.2M           | 8GB
# DeBERTa-xlarge | 32   | 64    | ~4.8M           | 16GB
# LLaMA-2-7B     | 64   | 128   | ~28M            | 24GB (4-bit)
#
# QLoRA memory requirements:
# Model          | Full FP32 | FP16  | 8-bit | 4-bit (QLoRA)
# LLaMA-2-7B     | 28GB     | 14GB  | 7GB   | 4-5GB
# LLaMA-2-13B    | 52GB     | 26GB  | 13GB  | 7-8GB
# LLaMA-2-70B    | 280GB    | 140GB | 70GB  | 35-40GB
# Mistral-7B     | 28GB     | 14GB  | 7GB   | 4-5GB
#
# Gradient checkpointing trade-offs:
# Benefit: Reduce memory by ~40-50%
# Cost: Increase training time by ~20-30%
# Recommended: For large models (XLarge+) on limited memory
#
# Mixed precision (FP16/BF16):
# Benefit: 2x speedup, 50% memory reduction
# Cost: Potential numerical instability
# Recommended: Almost always use with gradient scaling
#
# Flash Attention benefits:
# Speedup: 2-4x faster for long sequences
# Memory: O(N) instead of O(N^2)
# Limitation: Linux only, requires CUDA
#
# DeepSpeed ZeRO stages:
# Stage 1: Optimizer state partitioning (4x memory reduction)
# Stage 2: Gradient partitioning (8x memory reduction)
# Stage 3: Parameter partitioning (Linear memory reduction with GPUs)
#
# Project-specific efficient training configs:
# - LoRA configs: configs/training/efficient/lora/
# - QLoRA configs: configs/training/efficient/qlora/
# - Adapter configs: configs/training/efficient/adapters/
# - Prefix tuning: configs/training/efficient/prefix_tuning/
# - Prompt tuning: configs/training/efficient/prompt_tuning/
# - IA3: configs/training/efficient/ia3/
#
# Model-specific recommendations:
# - DeBERTa-v3-base (184M): LoRA rank 8-16, no quantization needed
# - DeBERTa-v3-large (435M): LoRA rank 16-32, FP16 mixed precision
# - DeBERTa-v3-xlarge (900M): LoRA rank 32-64, gradient checkpointing
# - DeBERTa-v2-xxlarge (1.5B): QLoRA 8-bit, gradient checkpointing
# - LLaMA-2-7B: QLoRA 4-bit, Flash Attention, rank 64
# - LLaMA-2-13B: QLoRA 4-bit, DeepSpeed ZeRO-2, rank 64
# - LLaMA-2-70B: QLoRA 4-bit, DeepSpeed ZeRO-3, multi-GPU
#
# Colab/Kaggle free tier optimization:
# - Use QLoRA 4-bit for models >1B parameters
# - Enable gradient checkpointing
# - Use gradient accumulation (effective batch size)
# - Reduce sequence length if possible
# - Use LoRA rank 8-16 (lower is more memory efficient)
#
# Training scripts:
# - LoRA: scripts/training/single_model/train_xlarge_lora.py
# - QLoRA: scripts/training/single_model/train_xxlarge_qlora.py
# - LLM QLoRA: scripts/training/single_model/train_llm_qlora.py
# - Adapters: scripts/training/single_model/train_with_adapters.py
#
# Memory optimization checklist:
# - Enable gradient checkpointing
# - Use mixed precision (FP16/BF16)
# - Use LoRA/QLoRA for large models
# - Enable gradient accumulation
# - Reduce batch size
# - Use Flash Attention for long sequences
# - Offload optimizer states (DeepSpeed ZeRO)
# - Use efficient data loading (prefetching)
# - Clear cache between training steps
#
# For detailed efficient training guide:
# - See docs/user_guide/lora_guide.md
# - See docs/user_guide/qlora_guide.md
# - See docs/best_practices/parameter_efficient_finetuning.md
# - See SOTA_MODELS_GUIDE.md for model-specific configs
# - See configs/training/efficient/ for all configurations
# ============================================================================
