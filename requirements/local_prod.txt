# ============================================================================
# Local Production Requirements for AG News Text Classification
# ============================================================================
# Project: AG News Text Classification (ag-news-text-classification)
# Description: Local production deployment and REST API serving
# Author: Võ Hải Dũng
# Email: vohaidung.work@gmail.com
# License: MIT
# Python: >=3.8,<3.12
# ============================================================================
# This file contains packages required for:
# - Production-grade REST API serving (FastAPI with Uvicorn/Gunicorn)
# - Model inference optimization (ONNX Runtime, quantization, batching)
# - Local monitoring and logging (TensorBoard, MLflow, Prometheus)
# - Caching and performance optimization (Redis, Diskcache)
# - Background task processing (Celery, APScheduler)
# - Health checks and readiness probes
# - Local authentication and security (token-based, rate limiting)
# - Docker containerization support
# - Systemd service integration for Linux
# - High availability and graceful shutdown
# - Request validation and error handling
# - API documentation generation (Swagger/OpenAPI)
# ============================================================================

# Include ML requirements for model loading and inference
-r ml.txt

# ----------------------------------------------------------------------------
# FastAPI REST API Framework
# ----------------------------------------------------------------------------
# FastAPI for modern, fast web API framework
fastapi>=0.109.0,<0.112.0

# Pydantic for data validation and settings management
pydantic>=2.5.0,<2.8.0
pydantic-settings>=2.1.0,<2.3.0

# Python-multipart for file upload support
python-multipart>=0.0.6,<0.0.10

# ----------------------------------------------------------------------------
# ASGI Server
# ----------------------------------------------------------------------------
# Uvicorn with standard extras (websockets, httptools, uvloop)
uvicorn[standard]>=0.27.0,<0.30.0

# Gunicorn for production WSGI/ASGI management
gunicorn>=21.2.0,<22.1.0

# Starlette ASGI framework (FastAPI dependency)
starlette>=0.36.0,<0.38.0

# ----------------------------------------------------------------------------
# Security and Authentication
# ----------------------------------------------------------------------------
# Python-jose for JWT token handling
python-jose[cryptography]>=3.3.0,<3.4.0

# Passlib for password hashing
passlib[bcrypt]>=1.7.4,<1.8.0

# Rate limiting
slowapi>=0.1.9,<0.2.0

# Cryptography for secure operations
cryptography>=41.0.7,<43.0.0

# ----------------------------------------------------------------------------
# HTTP Clients
# ----------------------------------------------------------------------------
# Requests for synchronous HTTP calls
requests>=2.31.0,<2.33.0

# HTTPX for async HTTP client
httpx>=0.25.0,<0.28.0

# ----------------------------------------------------------------------------
# WebSocket Support
# ----------------------------------------------------------------------------
# WebSockets library
websockets>=12.0,<13.0

# Python-socketio for Socket.IO support
python-socketio>=5.10.0,<5.12.0

# ----------------------------------------------------------------------------
# Server-Sent Events
# ----------------------------------------------------------------------------
# SSE for streaming responses
sse-starlette>=1.8.0,<2.2.0

# ----------------------------------------------------------------------------
# Background Task Processing
# ----------------------------------------------------------------------------
# Celery for distributed task queue
celery>=5.3.0,<5.4.0
celery[redis]>=5.3.0,<5.4.0

# APScheduler for cron-like scheduling
apscheduler>=3.10.0,<3.11.0

# ----------------------------------------------------------------------------
# Caching Layer
# ----------------------------------------------------------------------------
# Redis Python client
redis>=5.0.0,<5.1.0
hiredis>=2.3.0,<2.4.0

# Diskcache for local file-based caching
diskcache>=5.6.3,<5.7.0

# Cachetools for in-memory caching strategies
cachetools>=5.3.0,<5.4.0

# ----------------------------------------------------------------------------
# Database for Metadata
# ----------------------------------------------------------------------------
# SQLAlchemy ORM
sqlalchemy>=2.0.0,<2.1.0

# SQLite utilities
sqlite-utils>=3.36.0,<3.37.0

# Alembic for database migrations
alembic>=1.13.0,<1.14.0

# ----------------------------------------------------------------------------
# Local Monitoring (from local_monitoring.txt)
# ----------------------------------------------------------------------------
# TensorBoard for training visualization
tensorboard>=2.15.0,<2.17.0

# MLflow for experiment tracking
mlflow>=2.9.0,<2.15.0

# Prometheus client for metrics exposition
prometheus-client>=0.19.0,<0.21.0

# Statsd client for metric aggregation
statsd>=4.0.0,<4.1.0

# ----------------------------------------------------------------------------
# Structured Logging
# ----------------------------------------------------------------------------
# Loguru for beautiful structured logging
loguru>=0.7.0,<0.8.0

# Python JSON logger
python-json-logger>=2.0.7,<2.1.0

# ----------------------------------------------------------------------------
# System Monitoring
# ----------------------------------------------------------------------------
# psutil for system resource monitoring
psutil>=5.9.6,<6.1.0

# GPUtil for GPU monitoring
gputil>=1.4.0,<1.5.0

# PyNVML for NVIDIA GPU metrics
pynvml>=11.5.0,<11.6.0

# ----------------------------------------------------------------------------
# Performance Profiling
# ----------------------------------------------------------------------------
# Py-spy for production profiling
py-spy>=0.3.0,<0.4.0

# Memory profiler
memory-profiler>=0.61.0,<0.62.0

# ----------------------------------------------------------------------------
# Model Serving Optimization
# ----------------------------------------------------------------------------
# ONNX Runtime for optimized inference
onnxruntime>=1.16.0,<1.18.0

# ONNX Runtime GPU (optional, uncomment if using GPU)
# onnxruntime-gpu>=1.16.0,<1.18.0

# ----------------------------------------------------------------------------
# File System Operations
# ----------------------------------------------------------------------------
# File locking for concurrent access
filelock>=3.13.0,<3.15.0

# ----------------------------------------------------------------------------
# Configuration Management
# ----------------------------------------------------------------------------
# PyYAML for YAML config files
pyyaml>=6.0.1,<7.0.0

# Python-dotenv for environment variable loading
python-dotenv>=1.0.0,<1.1.0

# OmegaConf for hierarchical configuration
omegaconf>=2.3.0,<2.4.0

# ----------------------------------------------------------------------------
# Service Discovery (Optional)
# ----------------------------------------------------------------------------
# Python-consul for service discovery
python-consul>=1.1.0,<1.2.0

# ----------------------------------------------------------------------------
# Docker SDK
# ----------------------------------------------------------------------------
# Docker Python SDK for container management
docker>=6.1.0,<7.2.0

# ----------------------------------------------------------------------------
# Middleware and Request Tracking
# ----------------------------------------------------------------------------
# ASGI correlation ID for request tracking
asgi-correlation-id>=4.3.0,<4.4.0

# ----------------------------------------------------------------------------
# Error Tracking (Optional)
# ----------------------------------------------------------------------------
# Sentry SDK for error monitoring
sentry-sdk>=1.39.0,<2.8.0

# ----------------------------------------------------------------------------
# JSON Serialization
# ----------------------------------------------------------------------------
# orjson for fast JSON encoding/decoding
orjson>=3.9.0,<3.11.0

# ujson as alternative
ujson>=5.9.0,<5.11.0

# ----------------------------------------------------------------------------
# Data Serialization
# ----------------------------------------------------------------------------
# MessagePack for binary serialization
msgpack>=1.0.7,<1.1.0

# Protocol Buffers
protobuf>=4.25.0,<5.28.0

# ----------------------------------------------------------------------------
# Date and Time Handling
# ----------------------------------------------------------------------------
# Python-dateutil for date parsing
python-dateutil>=2.8.2,<2.10.0

# Pytz for timezone support
pytz>=2023.3,<2024.2

# Arrow for easier datetime manipulation
arrow>=1.3.0,<1.4.0

# ----------------------------------------------------------------------------
# Hash Functions
# ----------------------------------------------------------------------------
# xxhash for fast hashing
xxhash>=3.4.1,<3.5.0

# ----------------------------------------------------------------------------
# Concurrency Utilities
# ----------------------------------------------------------------------------
# Multiprocess for parallel processing
multiprocess>=0.70.15,<0.71.0

# ----------------------------------------------------------------------------
# Job Scheduling
# ----------------------------------------------------------------------------
# Python-crontab for cron expression parsing
python-crontab>=3.0.0,<3.3.0

# ----------------------------------------------------------------------------
# SSL/TLS Support
# ----------------------------------------------------------------------------
# PyOpenSSL for SSL certificate handling
pyopenssl>=23.3.0,<24.3.0

# ----------------------------------------------------------------------------
# Command Line Interface
# ----------------------------------------------------------------------------
# Click for CLI creation
click>=8.1.7,<8.2.0

# Rich for beautiful terminal output
rich>=13.7.0,<14.3.0

# Typer for modern CLI with type hints
typer[all]>=0.9.0,<0.13.0

# ----------------------------------------------------------------------------
# Progress Tracking
# ----------------------------------------------------------------------------
# TQDM for progress bars
tqdm>=4.66.0,<4.67.0

# ----------------------------------------------------------------------------
# Platform Detection
# ----------------------------------------------------------------------------
# Distro for Linux distribution info
distro>=1.8.0,<1.10.0

# ----------------------------------------------------------------------------
# URL Parsing
# ----------------------------------------------------------------------------
# urllib3 for URL utilities
urllib3>=2.1.0,<2.3.0

# ----------------------------------------------------------------------------
# Data Privacy
# ----------------------------------------------------------------------------
# Presidio for PII detection and anonymization
presidio-analyzer>=2.2.0,<2.3.0
presidio-anonymizer>=2.2.0,<2.3.0

# ----------------------------------------------------------------------------
# Testing for Production Endpoints
# ----------------------------------------------------------------------------
# Pytest for smoke tests
pytest>=7.4.0,<8.3.0

# Requests-mock for mocking HTTP requests
requests-mock>=1.11.0,<1.13.0

# ----------------------------------------------------------------------------
# API Documentation
# ----------------------------------------------------------------------------
# Markdown for documentation rendering
markdown>=3.5.1,<3.7.0

# ----------------------------------------------------------------------------
# Type Checking
# ----------------------------------------------------------------------------
# Typing extensions for advanced type hints
typing-extensions>=4.9.0,<4.12.0

# ----------------------------------------------------------------------------
# Retry Logic
# ----------------------------------------------------------------------------
# Tenacity for retry mechanisms
tenacity>=8.2.3,<8.4.0

# ============================================================================
# Installation Instructions for Local Production Deployment
# ============================================================================
#
# 1. Install local production dependencies:
#    pip install -r requirements/local_prod.txt
#
# 2. Setup local production environment:
#    bash scripts/setup/setup_local_environment.sh
#
# 3. Configure environment variables:
#    cp .env.example .env.local
#    Edit .env.local with your configuration
#
# 4. Start local API server (development):
#    uvicorn src.api.rest.app:app --reload --host 0.0.0.0 --port 8000
#    Access at: http://localhost:8000
#
# 5. Start local API server (production with Gunicorn):
#    gunicorn src.api.rest.app:app \
#      -w 4 \
#      -k uvicorn.workers.UvicornWorker \
#      --bind 0.0.0.0:8000 \
#      --timeout 60 \
#      --keepalive 5 \
#      --access-logfile outputs/logs/api/access.log \
#      --error-logfile outputs/logs/api/error.log
#
# 6. Start with all services (recommended):
#    bash deployment/local/scripts/start_all.sh
#
#    This starts:
#    - FastAPI application
#    - Redis cache server
#    - Celery worker for background tasks
#    - TensorBoard for monitoring
#    - MLflow UI for experiment tracking
#
# 7. Health check:
#    curl http://localhost:8000/health
#    
#    Expected response:
#    {
#      "status": "healthy",
#      "version": "1.0.0",
#      "models_loaded": true,
#      "cache_status": "connected"
#    }
#
# 8. API documentation (Swagger UI):
#    http://localhost:8000/docs
#
#    ReDoc alternative documentation:
#    http://localhost:8000/redoc
#
# 9. Prometheus metrics endpoint:
#    http://localhost:8000/metrics
#
# 10. Test prediction endpoint:
#     curl -X POST "http://localhost:8000/api/v1/predict" \
#       -H "Content-Type: application/json" \
#       -d '{
#         "text": "Apple announces new iPhone with advanced AI features",
#         "model": "deberta-v3-large-lora"
#       }'
#
# ============================================================================
# Environment Variables Configuration
# ============================================================================
#
# Create .env.local file with following variables:
#
# API Configuration
# API_HOST=0.0.0.0
# API_PORT=8000
# API_WORKERS=4
# API_RELOAD=false
# API_TITLE=AG News Text Classification API
# API_VERSION=1.0.0
# API_PREFIX=/api/v1
#
# Logging
# LOG_LEVEL=INFO
# LOG_FORMAT=json
# LOG_DIR=outputs/logs/api
#
# Redis Cache
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_DB=0
# REDIS_PASSWORD=
# CACHE_TTL=3600
#
# Model Configuration
# MODEL_PATH=outputs/models/fine_tuned
# DEFAULT_MODEL=deberta-v3-large-lora
# MODEL_CACHE_SIZE=3
# BATCH_SIZE=32
# MAX_LENGTH=512
#
# Performance
# MAX_WORKERS=4
# REQUEST_TIMEOUT=30
# KEEPALIVE_TIMEOUT=5
# GRACEFUL_SHUTDOWN_TIMEOUT=10
#
# Rate Limiting
# RATE_LIMIT_ENABLED=true
# RATE_LIMIT_PER_MINUTE=100
# RATE_LIMIT_PER_HOUR=1000
#
# Security
# API_KEY_ENABLED=true
# API_KEY_HEADER=X-API-Key
# CORS_ENABLED=true
# CORS_ORIGINS=http://localhost:3000,http://localhost:8501
#
# Monitoring
# METRICS_ENABLED=true
# METRICS_PORT=8000
# TENSORBOARD_ENABLED=true
# MLFLOW_ENABLED=true
#
# ============================================================================
# Production Deployment Checklist
# ============================================================================
#
# Pre-deployment:
# [x] Set environment variables in .env.local
# [x] Configure logging levels (INFO or WARNING for production)
# [x] Setup SSL/TLS certificates
# [x] Configure rate limiting thresholds
# [x] Setup backup strategy for models and data
# [x] Configure monitoring alerts
# [x] Test health check endpoints
# [x] Verify model loading and inference
# [x] Load test API endpoints
# [x] Setup firewall rules
# [x] Configure log rotation
#
# Post-deployment:
# [x] Monitor API response times
# [x] Check error rates and logs
# [x] Verify GPU utilization (if using GPU)
# [x] Monitor memory usage
# [x] Check cache hit rates
# [x] Verify rate limiting working
# [x] Test graceful shutdown
# [x] Verify backup procedures
#
# ============================================================================
# Systemd Service Installation (Linux)
# ============================================================================
#
# 1. Copy service files:
#    sudo cp deployment/local/systemd/ag-news-api.service /etc/systemd/system/
#    sudo cp deployment/local/systemd/ag-news-monitor.service /etc/systemd/system/
#
# 2. Reload systemd:
#    sudo systemctl daemon-reload
#
# 3. Enable services:
#    sudo systemctl enable ag-news-api
#    sudo systemctl enable ag-news-monitor
#
# 4. Start services:
#    sudo systemctl start ag-news-api
#    sudo systemctl start ag-news-monitor
#
# 5. Check status:
#    sudo systemctl status ag-news-api
#
# 6. View logs:
#    journalctl -u ag-news-api -f
#
# 7. Restart service:
#    sudo systemctl restart ag-news-api
#
# 8. Stop service:
#    sudo systemctl stop ag-news-api
#
# ============================================================================
# Nginx Reverse Proxy Setup
# ============================================================================
#
# 1. Install Nginx:
#    sudo apt-get install nginx
#
# 2. Copy configuration:
#    sudo cp deployment/local/nginx/ag-news.conf /etc/nginx/sites-available/
#
# 3. Create symlink:
#    sudo ln -s /etc/nginx/sites-available/ag-news.conf /etc/nginx/sites-enabled/
#
# 4. Test configuration:
#    sudo nginx -t
#
# 5. Reload Nginx:
#    sudo systemctl reload nginx
#
# 6. Access via Nginx:
#    http://your-domain.com/api/v1/
#
# Benefits of Nginx:
# - SSL/TLS termination
# - Load balancing across multiple workers
# - Static file serving
# - Request buffering
# - Gzip compression
# - Rate limiting at proxy level
#
# ============================================================================
# Docker Deployment
# ============================================================================
#
# 1. Build Docker image:
#    docker build -f deployment/docker/Dockerfile.local -t ag-news-api:latest .
#
# 2. Run container:
#    docker run -d \
#      --name ag-news-api \
#      -p 8000:8000 \
#      -v $(pwd)/outputs:/app/outputs \
#      -e API_HOST=0.0.0.0 \
#      -e API_PORT=8000 \
#      ag-news-api:latest
#
# 3. Docker Compose (recommended):
#    docker-compose -f deployment/docker/docker-compose.local.yml up -d
#
#    Services in docker-compose:
#    - api: FastAPI application
#    - redis: Cache server
#    - celery: Background worker
#    - tensorboard: Monitoring
#    - mlflow: Experiment tracking
#
# 4. View logs:
#    docker logs ag-news-api -f
#
# 5. Stop container:
#    docker stop ag-news-api
#
# 6. Remove container:
#    docker rm ag-news-api
#
# ============================================================================
# Performance Tuning
# ============================================================================
#
# Number of workers:
# - Formula: (2 × CPU cores) + 1
# - Example: 4 CPU cores → 9 workers
# - Adjust based on memory availability
#
# Worker class:
# - uvicorn.workers.UvicornWorker for async support
# - Supports WebSockets and Server-Sent Events
#
# Timeout settings:
# - Request timeout: 30-60 seconds
# - Keepalive: 5 seconds
# - Graceful shutdown: 10-30 seconds
#
# Worker restart:
# - Max requests per worker: 1000-5000
# - Prevents memory leaks in long-running workers
# - Automatic worker recycling
#
# Preload application:
# - Set to True for faster worker startup
# - Models loaded once and forked to workers
# - Reduces memory usage
#
# ============================================================================
# Monitoring in Production
# ============================================================================
#
# TensorBoard:
# - Training metrics and model graphs
# - Access: http://localhost:6006
#
# MLflow:
# - Experiment tracking and model registry
# - Access: http://localhost:5000
#
# Prometheus:
# - Real-time API and system metrics
# - Scrape endpoint: http://localhost:8000/metrics
# - Access Prometheus UI: http://localhost:9090
#
# Logs:
# - Application logs: outputs/logs/api/
# - Access logs: outputs/logs/api/access.log
# - Error logs: outputs/logs/api/error.log
# - Structured JSON format for parsing
#
# Health checks:
# - Liveness: /health
# - Readiness: /ready
# - Metrics: /metrics
#
# ============================================================================
# Caching Strategy
# ============================================================================
#
# Model caching:
# - Load models once at startup
# - Cache in memory for all workers (preload=True)
# - LRU cache for multiple models
#
# Prediction caching:
# - Cache frequent predictions in Redis
# - TTL: 1 hour (configurable)
# - Cache key: hash(text + model_name)
# - Significant speedup for repeated queries
#
# Static content caching:
# - Disk cache for preprocessing results
# - Tokenization cache
#
# Session data:
# - Redis for session storage
# - Persistent across server restarts
#
# ============================================================================
# Scaling Strategies
# ============================================================================
#
# Vertical scaling:
# - Increase number of workers
# - Add more RAM for model caching
# - Use larger GPU for faster inference
#
# Horizontal scaling:
# - Multiple API instances behind load balancer
# - Shared Redis cache across instances
# - Shared model storage (NFS, S3)
#
# Model parallelism:
# - Multiple GPUs with model sharding
# - Batch inference across GPUs
#
# Request batching:
# - Group requests into batches
# - Process batch on GPU
# - Dynamic batching based on load
#
# ============================================================================
# Security Best Practices
# ============================================================================
#
# Authentication:
# - API key in header or query parameter
# - JWT tokens for user sessions
# - Simple token-based auth for internal use
#
# Rate limiting:
# - Per IP address: 100 requests/minute
# - Per API key: 1000 requests/hour
# - Gradual backoff for repeated violations
#
# Input validation:
# - Pydantic models for request validation
# - Max text length enforcement
# - Sanitize inputs to prevent injection
#
# CORS configuration:
# - Whitelist allowed origins
# - Restrict to specific domains in production
#
# SSL/TLS:
# - Use HTTPS in production
# - Certificate from Let's Encrypt
# - Automatic renewal with Certbot
#
# PII detection:
# - Detect and mask personally identifiable information
# - Optional logging of PII-redacted requests
#
# ============================================================================
# Backup and Recovery
# ============================================================================
#
# Model checkpoints:
# - Daily backup to external storage
# - Version control for models
# - Quick rollback capability
#
# Configuration:
# - Version controlled in Git
# - Separate configs for dev/staging/prod
#
# Logs:
# - Rotate and archive old logs
# - Retention: 30 days
# - Compress with gzip or zstandard
#
# Database:
# - Daily SQLite backup
# - Export MLflow experiments
#
# Recovery procedures:
# - Document in deployment/local/recovery/
# - Test recovery process quarterly
#
# ============================================================================
# High Availability
# ============================================================================
#
# Multiple API instances:
# - Run 2+ instances for redundancy
# - Load balancer distributes traffic
# - Health checks for automatic failover
#
# Redis high availability:
# - Redis Sentinel for automatic failover
# - Master-slave replication
# - Automatic promotion of slaves
#
# Graceful shutdown:
# - Finish processing current requests
# - Reject new requests during shutdown
# - Timeout after 30 seconds
#
# Rolling updates:
# - Update one instance at a time
# - Verify health before updating next
# - Zero-downtime deployment
#
# ============================================================================
# Expected Performance Metrics
# ============================================================================
#
# Latency (single prediction):
# - CPU inference: 50-200ms
# - GPU inference (T4): 10-50ms
# - GPU inference (A100): 5-20ms
# - With caching: <5ms
#
# Throughput:
# - 4 workers, CPU: 100-200 req/sec
# - 4 workers, GPU: 500-1000 req/sec
# - With batching, GPU: 2000-5000 req/sec
#
# Resource requirements:
# - CPU: 4+ cores recommended
# - RAM: 8GB minimum, 16GB recommended
# - GPU: Optional, improves throughput 5-10x
# - Disk: 20GB for models, cache, logs
# - Network: 1Gbps for high throughput
#
# ============================================================================
# API Endpoints
# ============================================================================
#
# Core endpoints:
# - POST /api/v1/predict - Single text classification
# - POST /api/v1/predict/batch - Batch classification
# - GET /api/v1/models - List available models
# - POST /api/v1/models/{model_id}/load - Load specific model
# - DELETE /api/v1/cache - Clear prediction cache
#
# Health and monitoring:
# - GET /health - Liveness probe
# - GET /ready - Readiness probe
# - GET /metrics - Prometheus metrics
# - GET /stats - API statistics
#
# Documentation:
# - GET /docs - Swagger UI
# - GET /redoc - ReDoc documentation
# - GET /openapi.json - OpenAPI schema
#
# ============================================================================
# Integration with Project Structure
# ============================================================================
#
# API implementation:
# - src/api/rest/app.py - FastAPI application
# - src/api/rest/routers/classification.py - Prediction endpoints
# - src/api/rest/routers/health.py - Health check endpoints
# - src/api/rest/routers/metrics.py - Metrics endpoints
#
# Services:
# - src/services/core/prediction_service.py - Prediction logic
# - src/services/core/model_management_service.py - Model loading
# - src/services/local/local_cache_service.py - Caching layer
# - src/services/local/local_queue_service.py - Background tasks
#
# Inference optimization:
# - src/inference/predictors/single_predictor.py - Single prediction
# - src/inference/predictors/ensemble_predictor.py - Ensemble prediction
# - src/inference/optimization/model_quantization.py - Model compression
# - src/inference/serving/batch_predictor.py - Batch inference
#
# Deployment configs:
# - deployment/local/docker-compose.local.yml - Docker Compose
# - deployment/local/systemd/ - Systemd services
# - deployment/local/nginx/ - Nginx configuration
# - deployment/local/scripts/start_all.sh - Startup script
#
# ============================================================================
# For Detailed Documentation
# ============================================================================
#
# Deployment guides:
# - FREE_DEPLOYMENT_GUIDE.md - Zero-cost deployment
# - LOCAL_MONITORING_GUIDE.md - Local monitoring setup
# - deployment/local/README.md - Local deployment details
#
# User guides:
# - docs/getting_started/local_setup.md - Initial setup
# - docs/user_guide/local_deployment.md - Deployment guide
# - docs/api_reference/rest_api.md - API documentation
#
# API reference:
# - Swagger UI: http://localhost:8000/docs
# - ReDoc: http://localhost:8000/redoc
#
# ============================================================================
