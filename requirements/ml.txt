# ============================================================================
# Machine Learning Requirements for AG News Text Classification
# ============================================================================
# Project: AG News Text Classification (ag-news-text-classification)
# Description: ML/DL specific libraries for model training and evaluation
# Author: Võ Hải Dũng
# License: MIT
# Python: >=3.8,<3.12
# ============================================================================
# This file contains packages required for:
# - Deep learning model training (transformers, LLMs)
# - Parameter-efficient fine-tuning (LoRA, QLoRA, Adapters)
# - Ensemble methods (voting, stacking, blending)
# - Classical ML baselines
# - Model evaluation and metrics
# - Hyperparameter optimization
# - Data augmentation
# ============================================================================

# Include base requirements
-r base.txt

# ----------------------------------------------------------------------------
# Parameter-Efficient Fine-Tuning (PEFT)
# ----------------------------------------------------------------------------
# LoRA, QLoRA, Adapters, Prefix Tuning, Prompt Tuning
peft>=0.7.0,<0.12.0

# Quantization for QLoRA (4-bit, 8-bit)
bitsandbytes>=0.41.0,<0.44.0

# Adapter modules for transformers
adapters>=0.2.0,<0.3.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Advanced Training Techniques
# ----------------------------------------------------------------------------
# Mixed precision training utilities
apex>=0.1; platform_system != "Windows"

# Gradient checkpointing and memory optimization
fairscale>=0.4.13,<0.5.0

# DeepSpeed for efficient training
deepspeed>=0.12.0,<0.15.0; platform_system != "Windows"

# Fully Sharded Data Parallel (FSDP)
torch-fsdp>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# NLP Libraries and Tools
# ----------------------------------------------------------------------------
# Natural Language Toolkit
nltk>=3.8.0,<3.9.0

# Sentence piece tokenization
sentencepiece>=0.1.99,<0.3.0

# Moses tokenizer
sacremoses>=0.1.0,<0.2.0

# spaCy for advanced NLP
spacy>=3.7.0,<3.8.0

# Language detection
langdetect>=1.0.9,<1.1.0
fasttext>=0.9.2,<0.10.0

# ----------------------------------------------------------------------------
# Text Augmentation
# ----------------------------------------------------------------------------
# NLP augmentation library
nlpaug>=1.1.11,<1.2.0

# TextAttack for adversarial examples
textattack>=0.3.8,<0.4.0

# Text generation for augmentation
textaugment>=1.4.0,<1.5.0

# ----------------------------------------------------------------------------
# Classical Machine Learning
# ----------------------------------------------------------------------------
# Gradient boosting frameworks for ensemble meta-learners
xgboost>=2.0.0,<2.1.0
lightgbm>=4.1.0,<4.4.0
catboost>=1.2.0,<1.3.0

# Support Vector Machines
thundersvm>=0.3.4,<0.4.0; platform_system != "Windows"

# Feature selection
scikit-learn-extra>=0.3.0,<0.4.0

# Imbalanced learning
imbalanced-learn>=0.11.0,<0.13.0

# ----------------------------------------------------------------------------
# Ensemble Methods
# ----------------------------------------------------------------------------
# Stacking and blending utilities
mlxtend>=0.23.0,<0.24.0

# Bayesian optimization for ensemble weights
scikit-optimize>=0.9.0,<0.10.0

# Snapshot ensemble
torch-snapshot-ensemble>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Evaluation Metrics
# ----------------------------------------------------------------------------
# HuggingFace evaluate library
evaluate>=0.4.0,<0.5.0

# Sequence labeling evaluation
seqeval>=1.2.2,<1.3.0

# Metrics for NLP
nlp-metrics>=0.1.0,<0.2.0

# Interpretability metrics
interpret>=0.5.0,<0.7.0

# Uncertainty quantification
uncertainty-toolbox>=0.1.1,<0.2.0

# ----------------------------------------------------------------------------
# Hyperparameter Optimization
# ----------------------------------------------------------------------------
# Optuna for hyperparameter search
optuna>=3.5.0,<3.7.0
optuna-integration>=3.5.0,<3.7.0

# Ray Tune for distributed hyperparameter tuning
ray[tune]>=2.9.0,<2.10.0

# Weights & Biases sweeps
wandb>=0.16.0,<0.17.0

# Hyperband algorithm
hyperopt>=0.2.7,<0.3.0

# Bayesian optimization
bayes-opt>=1.4.3,<1.5.0

# ----------------------------------------------------------------------------
# Knowledge Distillation
# ----------------------------------------------------------------------------
# Distillation utilities
torch-distillation>=0.1.0,<0.2.0; python_version >= "3.9"

# Knowledge distillation for transformers
transformers-interpret>=0.10.0,<0.11.0

# ----------------------------------------------------------------------------
# Regularization Techniques
# ----------------------------------------------------------------------------
# R-Drop implementation
r-drop>=0.1.0,<0.2.0; python_version >= "3.9"

# Mixout regularization
mixout>=0.1.0,<0.2.0; python_version >= "3.9"

# Spectral normalization
spectral-normalization-pytorch>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Adversarial Training
# ----------------------------------------------------------------------------
# Free Large-scale Adversarial Training
freelb>=0.1.0,<0.2.0; python_version >= "3.9"

# SMART adversarial regularization
smart-pytorch>=0.1.0,<0.2.0; python_version >= "3.9"

# Adversarial training utilities
adversarial-robustness-toolbox>=1.15.0,<1.18.0

# ----------------------------------------------------------------------------
# Contrastive Learning
# ----------------------------------------------------------------------------
# SimCLR and contrastive learning
pytorch-metric-learning>=2.4.0,<2.6.0

# Supervised contrastive learning
supcontrast>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Meta Learning
# ----------------------------------------------------------------------------
# MAML and meta-learning algorithms
learn2learn>=0.2.0,<0.3.0

# ----------------------------------------------------------------------------
# Curriculum Learning
# ----------------------------------------------------------------------------
# Curriculum learning utilities
curricularface>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Model Compression
# ----------------------------------------------------------------------------
# Neural network pruning
torch-pruning>=1.3.0,<1.4.0

# Model quantization
neural-compressor>=2.4.0,<2.6.0

# ONNX for model optimization
onnx>=1.15.0,<1.17.0
onnxruntime>=1.16.0,<1.18.0

# OpenVINO for inference optimization
openvino>=2023.2.0,<2024.2.0; python_version >= "3.9"

# TensorRT (optional, for NVIDIA GPUs)
# nvidia-tensorrt>=8.6.0,<10.0.0

# ----------------------------------------------------------------------------
# Attention Visualization
# ----------------------------------------------------------------------------
# BertViz for attention visualization
bertviz>=1.4.0,<1.5.0

# Captum for model interpretability
captum>=0.7.0,<0.8.0

# ----------------------------------------------------------------------------
# Feature Extraction
# ----------------------------------------------------------------------------
# TF-IDF and text features
textstat>=0.7.3,<0.8.0

# Word embeddings
gensim>=4.3.0,<4.4.0

# FastText embeddings
fasttext>=0.9.2,<0.10.0

# ----------------------------------------------------------------------------
# Data Selection and Sampling
# ----------------------------------------------------------------------------
# Coreset selection
coresets>=0.1.0,<0.2.0; python_version >= "3.9"

# Influence functions
pytorch-influence-functions>=0.1.1,<0.2.0

# Active learning
modAL>=0.4.1,<0.5.0

# ----------------------------------------------------------------------------
# Prompt Engineering and Instruction Tuning
# ----------------------------------------------------------------------------
# Prompt tuning utilities
openprompt>=1.0.1,<1.1.0

# Instruction tuning
stanford-alpaca>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Multi-Task Learning
# ----------------------------------------------------------------------------
# Multi-task learning framework
pytorch-adapt>=0.2.0,<0.3.0

# ----------------------------------------------------------------------------
# Domain Adaptation
# ----------------------------------------------------------------------------
# Domain adaptation toolkit
adapt>=0.4.0,<0.5.0

# Transfer learning utilities
pytorch-adapt>=0.2.0,<0.3.0

# ----------------------------------------------------------------------------
# Text Generation (for LLM-based augmentation)
# ----------------------------------------------------------------------------
# Text generation utilities
textgenrnn>=2.0.0,<3.0.0

# Controlled generation
ctrl-gen>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Vocabulary and Tokenization
# ----------------------------------------------------------------------------
# Byte-pair encoding
tokenizers>=0.15.0,<0.16.0

# SentencePiece training
sentencepiece-trainer>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Loss Functions
# ----------------------------------------------------------------------------
# Advanced loss functions
pytorch-softmax>=1.0.0,<2.0.0

# Focal loss
focal-loss>=0.0.7,<0.1.0

# Label smoothing
label-smoothing-pytorch>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Optimizers
# ----------------------------------------------------------------------------
# Advanced optimizers
torch-optimizer>=0.3.0,<0.4.0

# SAM (Sharpness-Aware Minimization)
sam-pytorch>=0.1.0,<0.2.0; python_version >= "3.9"

# AdamW variants
pytorch-optimizer>=3.0.0,<3.1.0

# LAMB optimizer
lamb-optimizer>=0.1.0,<0.2.0; python_version >= "3.9"

# Lookahead optimizer
lookahead-pytorch>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Learning Rate Schedulers
# ----------------------------------------------------------------------------
# Advanced LR schedulers
torch-lr-scheduler>=0.1.0,<0.2.0; python_version >= "3.9"

# Cosine annealing with warm restarts
cosine-annealing-warmup>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Gradient Utilities
# ----------------------------------------------------------------------------
# Gradient accumulation
gradient-accumulator>=0.4.0,<0.5.0

# Gradient clipping
torch-gradclip>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Memory Optimization
# ----------------------------------------------------------------------------
# Gradient checkpointing
checkpoint-sequential>=0.1.0,<0.2.0; python_version >= "3.9"

# Memory profiling
memory-profiler>=0.61.0,<0.62.0

# GPU memory management
gputil>=1.4.0,<1.5.0

# ----------------------------------------------------------------------------
# Distributed Training
# ----------------------------------------------------------------------------
# Horovod for distributed training
horovod>=0.28.0,<0.29.0; platform_system != "Windows"

# ----------------------------------------------------------------------------
# Experiment Tracking Integration
# ----------------------------------------------------------------------------
# MLflow integration
mlflow>=2.9.0,<2.15.0

# TensorBoard
tensorboard>=2.15.0,<2.17.0
tensorboardX>=2.6.2,<2.7.0

# Weights & Biases
wandb>=0.16.0,<0.17.0

# Neptune.ai
neptune>=1.8.0,<1.11.0

# Comet ML
comet-ml>=3.35.0,<3.44.0

# ----------------------------------------------------------------------------
# Statistical Analysis
# ----------------------------------------------------------------------------
# Statistical tests
scipy>=1.10.0,<1.13.0
statsmodels>=0.14.0,<0.15.0

# Permutation tests
permute>=0.2.0,<0.3.0; python_version >= "3.9"

# Bootstrap confidence intervals
arch>=6.2.0,<7.1.0

# ----------------------------------------------------------------------------
# Visualization
# ----------------------------------------------------------------------------
# Plotting libraries
matplotlib>=3.8.0,<3.10.0
seaborn>=0.13.0,<0.14.0

# Interactive plots
plotly>=5.18.0,<5.23.0

# Confusion matrix visualization
scikit-plot>=0.3.7,<0.4.0

# ----------------------------------------------------------------------------
# Data Quality and Validation
# ----------------------------------------------------------------------------
# Data validation
great-expectations>=0.18.0,<0.19.0

# Data profiling
pandas-profiling>=3.6.0,<3.7.0

# ----------------------------------------------------------------------------
# Special Model Support
# ----------------------------------------------------------------------------
# Longformer for long documents
longformer>=0.1.0,<0.2.0; python_version >= "3.9"

# BigBird for long sequences
bigbird>=0.1.0,<0.2.0; python_version >= "3.9"

# T5 utilities
t5>=0.9.4,<0.10.0

# ----------------------------------------------------------------------------
# Robustness Testing
# ----------------------------------------------------------------------------
# Contrast sets
contrast-sets>=0.1.0,<0.2.0; python_version >= "3.9"

# Out-of-distribution detection
pytorch-ood>=0.2.0,<0.3.0

# Calibration metrics
netcal>=1.3.5,<1.4.0

# ----------------------------------------------------------------------------
# Efficient Inference
# ----------------------------------------------------------------------------
# ONNX Runtime
onnxruntime>=1.16.0,<1.18.0

# TorchScript optimization
torch-jit-utils>=0.1.0,<0.2.0; python_version >= "3.9"

# ----------------------------------------------------------------------------
# Platform-Specific Dependencies
# ----------------------------------------------------------------------------
# Apple Silicon optimization (M1/M2)
# torch-mlx>=0.1.0,<0.2.0; platform_system == "Darwin" and platform_machine == "arm64"

# ROCm for AMD GPUs
# torch-rocm>=2.0.0,<2.2.0; platform_system == "Linux"

# ============================================================================
# Installation Notes:
# ============================================================================
# 1. Install ML requirements:
#    pip install -r requirements/ml.txt
#
# 2. For specific use cases:
#    - LoRA/QLoRA only: pip install -r requirements/efficient.txt
#    - LLM training: pip install -r requirements/llm.txt
#    - Research: pip install -r requirements/research.txt
#
# 3. GPU-specific installation:
#    - CUDA 11.8: pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu118
#    - CUDA 12.1: pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cu121
#    - ROCm: pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/rocm5.7
#    - CPU only: pip install torch==2.1.0 --index-url https://download.pytorch.org/whl/cpu
#
# 4. Memory requirements by model size:
#    - Base models (110M-340M params): 4GB+ GPU/RAM
#    - Large models (340M-770M params): 8GB+ GPU/RAM
#    - XLarge models (770M-1.5B params): 16GB+ GPU/RAM with LoRA
#    - XXLarge models (1.5B+ params): 24GB+ GPU/RAM with QLoRA
#    - LLMs (7B-13B params): 24GB+ GPU with QLoRA 4-bit
#    - LLMs (70B+ params): 80GB+ GPU with QLoRA 4-bit or multi-GPU
#
# 5. Troubleshooting:
#    - Import errors: Run python scripts/setup/verify_dependencies.py
#    - Version conflicts: Use requirements/lock/ for locked versions
#    - Platform issues: See docs/getting_started/troubleshooting.md
#
# 6. Optional dependencies (install as needed):
#    - apex: For mixed precision (install from source)
#    - deepspeed: For ZeRO optimization (Linux only)
#    - horovod: For distributed training (requires MPI)
#
# 7. Compatibility matrix:
#    See configs/compatibility_matrix.yaml for tested combinations
#
# For detailed documentation on each package:
# - See docs/user_guide/model_training.md
# - See docs/best_practices/parameter_efficient_finetuning.md
# - See SOTA_MODELS_GUIDE.md for model-specific requirements
# ============================================================================
