# ============================================================================
# Free Tier Requirements for AG News Text Classification
# ============================================================================
# Project: AG News Text Classification (ag-news-text-classification)
# Description: Unified dependencies for all free cloud platforms
# Author: Võ Hải Dũng
# Email: vohaidung.work@gmail.com
# License: MIT
# Python: >=3.8,<3.12
# ============================================================================
# This file contains packages optimized for free cloud platforms:
# - Google Colab Free (T4 GPU, 12.7GB RAM, 12-hour session)
# - Google Colab Pro (A100/V100, 25GB RAM, extended session)
# - Kaggle Kernels (P100/T4 GPU, 16GB RAM, 30 hours/week GPU quota)
# - Hugging Face Spaces Free (CPU only, 16GB RAM, persistent)
# - Streamlit Cloud Free (CPU only, 1GB RAM, persistent)
# - GitHub Codespaces Free (2 cores, 4GB RAM, 60 hours/month)
# - Gitpod Free (4 cores, 8GB RAM, 50 hours/month)
#
# This is a minimal, unified requirements file optimized for resource
# constraints across all free cloud platforms with zero monthly cost.
# ============================================================================

# ----------------------------------------------------------------------------
# Core Deep Learning Framework
# ----------------------------------------------------------------------------
# PyTorch (may be pre-installed on Colab and Kaggle)
torch>=2.1.0,<2.3.0

# ----------------------------------------------------------------------------
# HuggingFace Transformers Ecosystem
# ----------------------------------------------------------------------------
# Transformers library for SOTA models
transformers>=4.36.0,<4.41.0

# Fast tokenizers
tokenizers>=0.15.0,<0.16.0

# Datasets library for data loading
datasets>=2.16.0,<2.20.0

# Accelerate for distributed training
accelerate>=0.25.0,<0.31.0

# SafeTensors for efficient model storage
safetensors>=0.4.0,<0.5.0

# HuggingFace Hub for model download
huggingface-hub>=0.20.0,<0.24.0

# ----------------------------------------------------------------------------
# Parameter-Efficient Fine-Tuning (Critical for Free Tiers)
# ----------------------------------------------------------------------------
# PEFT for LoRA and QLoRA
peft>=0.7.0,<0.12.0

# BitsAndBytes for model quantization (GPU only)
bitsandbytes>=0.41.0,<0.44.0

# ----------------------------------------------------------------------------
# Scientific Computing Libraries
# ----------------------------------------------------------------------------
# NumPy for numerical operations (may be pre-installed)
numpy>=1.24.0,<1.27.0

# Pandas for data manipulation (may be pre-installed)
pandas>=2.0.0,<2.3.0

# Scikit-learn for metrics and baselines (may be pre-installed)
scikit-learn>=1.3.0,<1.5.0

# ----------------------------------------------------------------------------
# Configuration Management
# ----------------------------------------------------------------------------
# YAML configuration parsing
pyyaml>=6.0.1,<7.0.0

# Environment variable management
python-dotenv>=1.0.0,<1.1.0

# OmegaConf for hierarchical configuration
omegaconf>=2.3.0,<2.4.0

# Pydantic for data validation
pydantic>=2.5.0,<2.8.0

# ----------------------------------------------------------------------------
# Natural Language Processing
# ----------------------------------------------------------------------------
# NLTK for text processing
nltk>=3.8.0,<3.9.0

# SentencePiece for tokenization
sentencepiece>=0.1.99,<0.3.0

# ----------------------------------------------------------------------------
# API Framework for Deployment
# ----------------------------------------------------------------------------
# FastAPI for REST API
fastapi>=0.109.0,<0.112.0

# Uvicorn for ASGI server
uvicorn[standard]>=0.27.0,<0.30.0

# ----------------------------------------------------------------------------
# User Interface Frameworks
# ----------------------------------------------------------------------------
# Gradio for interactive demos (optimized for HuggingFace Spaces)
gradio>=4.12.0,<4.38.0

# Streamlit for dashboards (optimized for Streamlit Cloud)
streamlit>=1.30.0,<1.37.0

# ----------------------------------------------------------------------------
# Visualization Libraries
# ----------------------------------------------------------------------------
# Plotly for interactive visualizations
plotly>=5.18.0,<5.23.0

# Matplotlib for static plots (may be pre-installed)
matplotlib>=3.8.0,<3.10.0

# ----------------------------------------------------------------------------
# Evaluation Metrics
# ----------------------------------------------------------------------------
# HuggingFace evaluate library
evaluate>=0.4.0,<0.5.0

# ----------------------------------------------------------------------------
# Logging and Monitoring
# ----------------------------------------------------------------------------
# Loguru for structured logging
loguru>=0.7.0,<0.8.0

# ----------------------------------------------------------------------------
# Utility Libraries
# ----------------------------------------------------------------------------
# Type hints extensions
typing-extensions>=4.9.0,<4.12.0

# Progress bars
tqdm>=4.66.0,<4.67.0

# HTTP requests
requests>=2.31.0,<2.33.0

# Rich for terminal formatting
rich>=13.7.0,<13.8.0

# File locking utilities
filelock>=3.13.0,<3.15.0

# Retry logic for robust operations
tenacity>=8.2.3,<8.4.0

# ----------------------------------------------------------------------------
# Efficient Data Serialization
# ----------------------------------------------------------------------------
# PyArrow for efficient data formats
pyarrow>=14.0.0,<16.2.0

# MessagePack for binary serialization
msgpack>=1.0.7,<1.1.0

# ----------------------------------------------------------------------------
# Hash Functions
# ----------------------------------------------------------------------------
# xxHash for fast hashing
xxhash>=3.4.1,<3.5.0

# ----------------------------------------------------------------------------
# Testing Framework
# ----------------------------------------------------------------------------
# Pytest for unit testing
pytest>=7.4.0,<8.3.0

# ============================================================================
# Platform-Specific Configuration Notes
# ============================================================================
# Google Colab Free Tier:
# - Pre-installed: torch, numpy, pandas, matplotlib, scikit-learn
# - GPU: T4 (15GB VRAM)
# - RAM: 12.7GB
# - Session: 12 hours maximum
# - Storage: Temporary, use Google Drive for persistence
# - Installation: pip install -q -r requirements/free_tier.txt
#
# Google Colab Pro Tier:
# - GPU: A100, V100, or T4
# - RAM: Up to 25GB
# - Session: Extended duration
# - Background execution supported
#
# Kaggle Kernels:
# - Pre-installed: torch, transformers, datasets, numpy, pandas, scikit-learn
# - GPU: P100 (16GB VRAM) or T4 (16GB VRAM)
# - RAM: 16GB
# - Session: 9 hours for GPU, 12 hours for CPU
# - GPU Quota: 30 hours per week
# - TPU Quota: 20 hours per week
# - Storage: 73GB disk, persistent datasets
# - Installation: pip install -q -r requirements/free_tier.txt
#
# Hugging Face Spaces Free Tier:
# - Pre-installed: None (start from minimal base)
# - CPU only (no GPU on free tier)
# - RAM: 16GB
# - Storage: Limited persistent storage
# - Deployment: Persistent, always-on
# - Best for: Gradio demos and inference
#
# Streamlit Cloud Free Tier:
# - Pre-installed: streamlit
# - CPU only (no GPU)
# - RAM: 1GB (very limited)
# - Storage: GitHub repository only
# - Deployment: Persistent, always-on
# - Best for: Lightweight dashboards with small models
# - Use CPU-only PyTorch: torch --index-url https://download.pytorch.org/whl/cpu
#
# GitHub Codespaces Free Tier:
# - Pre-installed: None
# - CPU: 2 cores
# - RAM: 4GB
# - Storage: 32GB
# - Quota: 60 hours per month
# - Best for: Development and code editing, not training
#
# Gitpod Free Tier:
# - Pre-installed: None
# - CPU: 4 cores
# - RAM: 8GB
# - Storage: 30GB
# - Quota: 50 hours per month
# - Best for: Development with light training experiments
#
# ============================================================================
# Installation Instructions by Platform
# ============================================================================
# Google Colab:
# !pip install -q -r requirements/free_tier.txt
# !python scripts/setup/setup_colab.sh
#
# Kaggle:
# !pip install -q -r requirements/free_tier.txt
# !python scripts/setup/setup_kaggle.sh
#
# Hugging Face Spaces:
# Add this file as requirements.txt in space repository root
# Gradio app will auto-deploy from app.py
#
# Streamlit Cloud:
# Add minimal subset to requirements.txt:
# streamlit>=1.30.0
# transformers>=4.36.0
# torch>=2.1.0 --index-url https://download.pytorch.org/whl/cpu
# (Use CPU-only PyTorch to reduce memory footprint)
#
# GitHub Codespaces:
# pip install -r requirements/free_tier.txt
# Best for code development, not model training
#
# Gitpod:
# pip install -r requirements/free_tier.txt
# Can handle light training experiments
#
# ============================================================================
# Memory Optimization for Free Tier Platforms
# ============================================================================
# All platforms have strict memory constraints:
#
# Model selection by platform:
# - Streamlit Cloud (1GB RAM): 
#   Use pre-quantized small models or ONNX runtime
#   Example: distilbert-base-uncased, distilroberta-base
#
# - GitHub Codespaces (4GB RAM):
#   Inference only with base models
#   Example: roberta-base, deberta-v3-base
#
# - Gitpod (8GB RAM):
#   Light training with base models, inference with large models
#   Example: deberta-v3-base with LoRA
#
# - Colab/Kaggle/HF Spaces (12-16GB RAM):
#   Training up to xlarge models with LoRA/QLoRA
#   Example: deberta-v3-xlarge with LoRA, llama-2-7b with QLoRA
#
# Memory optimization techniques:
# - Use LoRA for models above 500M parameters
# - Use QLoRA 4-bit for models above 1B parameters
# - Enable gradient checkpointing for all large models
# - Use smaller batch sizes (1-8 for large models)
# - Reduce maximum sequence length (256-512 for AG News)
# - Clear CUDA cache frequently: torch.cuda.empty_cache()
# - Use CPU offloading for very large models
# - Quantize models for inference deployment
#
# ============================================================================
# Deployment Strategy by Platform
# ============================================================================
# Google Colab Free:
# - Best for: Training experiments and research
# - Model size: Up to 7B parameters with QLoRA
# - Persistence: None, save to Google Drive
# - Use case: Development and experimentation
# - Auto-training: python quickstart/auto_start.py
#
# Kaggle Kernels:
# - Best for: Reproducible research and competitions
# - Model size: Up to 13B parameters with QLoRA
# - Persistence: 6 months for outputs (up to 20GB)
# - Use case: Benchmarking and competition submissions
# - Auto-training: python quickstart/auto_start.py
#
# Hugging Face Spaces:
# - Best for: Model demos and inference APIs
# - Model size: Up to base models on CPU
# - Persistence: Permanent (free tier)
# - Use case: Public demos and result sharing
# - Deployment: Automatic from repository
#
# Streamlit Cloud:
# - Best for: Lightweight dashboards and visualizations
# - Model size: Tiny models only (DistilBERT, small variants)
# - Persistence: Permanent
# - Use case: Simple demos and result visualization
# - Deployment: Automatic from GitHub repository
#
# GitHub Codespaces:
# - Best for: Code development and debugging
# - Model size: Inference only with base models
# - Persistence: Repository and workspace
# - Use case: Development environment
# - Not recommended for training
#
# Gitpod:
# - Best for: Development with light training
# - Model size: Base models for training, large for inference
# - Persistence: Repository and workspace
# - Use case: Development and light experiments
# - Limited training capabilities
#
# ============================================================================
# Cost Optimization (All Platforms are Free)
# ============================================================================
# All platforms in this file are completely free with usage limits:
#
# Google Colab Free Tier:
# - CPU: Unlimited usage
# - GPU: Subject to availability
# - TPU: Subject to availability
# - Sessions may disconnect after 12 hours or inactivity
# - No monthly cost
#
# Kaggle Kernels Free Tier:
# - CPU: Unlimited usage
# - GPU: 30 hours per week quota
# - TPU: 20 hours per week quota
# - No session disconnects within time limit
# - No monthly cost
#
# Hugging Face Spaces Free Tier:
# - CPU: Unlimited usage
# - RAM: 16GB
# - Storage: Limited
# - Always-on deployment
# - No monthly cost
#
# Streamlit Cloud Free Tier:
# - 1 private app allowed
# - Unlimited public apps
# - 1GB RAM per app
# - Always-on deployment
# - No monthly cost
#
# GitHub Codespaces Free Tier:
# - 60 hours per month (2-core)
# - 15GB persistent storage
# - Pre-builds count against quota
# - No monthly cost within limits
#
# Gitpod Free Tier:
# - 50 hours per month (4-core)
# - 30GB persistent storage
# - Better specifications than Codespaces
# - No monthly cost within limits
#
# Total monthly cost for all platforms combined: $0
#
# ============================================================================
# Free Deployment Workflow Strategy
# ============================================================================
# Recommended workflow for zero-cost deployment:
#
# Phase 1 - Development:
#    Platform: GitHub Codespaces or Gitpod
#    Purpose: Write and test code
#    GPU: Not required
#    Duration: Within free tier limits
#
# Phase 2 - Training:
#    Platform: Kaggle for serious training (P100, more reliable)
#              Colab for quick experiments (easy Drive integration)
#    Purpose: Train models and hyperparameter tuning
#    GPU: Required
#    Save: Best models and checkpoints
#
# Phase 3 - Demo Deployment:
#    Platform: Hugging Face Spaces for Gradio demos
#              Streamlit Cloud for dashboards (if lightweight)
#    Purpose: Public demonstration and sharing
#    Persistence: Permanent
#
# Phase 4 - API Deployment:
#    Platform: Hugging Face Spaces (Gradio or FastAPI)
#    Purpose: Inference API endpoint
#    Benefits: Free, persistent, public URL
#
# Phase 5 - Monitoring:
#    Platform: Weights & Biases free tier
#              TensorBoard (logs in repository)
#    Purpose: Track experiments and model performance
#    Cost: Free
#
# Total workflow cost: $0 per month
#
# ============================================================================
# Platform-Specific Quick Start Examples
# ============================================================================
# Google Colab Quick Start:
# !git clone https://github.com/VoHaiDung/ag-news-text-classification.git
# %cd ag-news-text-classification
# !pip install -q -r requirements/free_tier.txt
# !python quickstart/auto_start.py
#
# Kaggle Quick Start:
# !git clone https://github.com/VoHaiDung/ag-news-text-classification.git
# %cd ag-news-text-classification
# !pip install -q -r requirements/free_tier.txt
# !python quickstart/auto_start.py
#
# Hugging Face Space Quick Start:
# See deployment/huggingface/app.py for Gradio interface
# Add this file as requirements.txt in space repository
#
# Streamlit Cloud Quick Start:
# See app/streamlit_app.py for lightweight dashboard
# Add minimal requirements to requirements.txt
#
# Auto-Training Mode (All Platforms):
# python quickstart/auto_start.py
# The system will automatically detect platform and select optimal configuration
#
# ============================================================================
# Troubleshooting Common Issues
# ============================================================================
# Issue: Out of Memory Error
# Solution: 
# - Reduce batch size
# - Enable gradient checkpointing
# - Use smaller model or apply QLoRA
# - Clear CUDA cache: torch.cuda.empty_cache()
#
# Issue: Slow Package Installation
# Solution:
# - Use -q flag for quiet mode
# - Install only required packages
# - Use platform-specific pre-installed packages
#
# Issue: Package Version Conflicts
# Solution:
# - Use virtual environment
# - Check platform pre-installed package versions
# - Use exact versions from this file
#
# Issue: Deployment Failure
# Solution:
# - Check RAM requirements for platform
# - Use CPU-only PyTorch for inference-only apps
# - Optimize model size and quantization
#
# Issue: Session Timeout
# Solution:
# - Save checkpoints frequently
# - Use platform-specific keep-alive scripts
# - Kaggle has no mid-session disconnects
#
# Issue: GPU Not Available
# Solution:
# - Check platform GPU quota
# - Verify GPU is enabled in settings
# - Use CPU fallback for development
#
# ============================================================================
# Additional Resources and Documentation
# ============================================================================
# - Free deployment guide: FREE_DEPLOYMENT_GUIDE.md
# - Platform optimization: PLATFORM_OPTIMIZATION_GUIDE.md
# - Colab setup notebook: notebooks/00_setup/01_colab_setup.ipynb
# - Kaggle setup notebook: notebooks/00_setup/02_kaggle_setup.ipynb
# - Auto-training tutorial: notebooks/01_tutorials/00_auto_training_tutorial.ipynb
# - HuggingFace Spaces deployment: deployment/huggingface/
# - Streamlit Cloud deployment: deployment/streamlit_cloud/
# - Quick start guide: QUICK_START.md
# - Platform comparison: docs/platform_guides/platform_comparison.md
# - Quota management: docs/user_guide/quota_management.md
# - Platform detection: src/deployment/platform_detector.py
# - Auto-training: src/training/trainers/auto_trainer.py
# ============================================================================
