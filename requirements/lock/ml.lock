# ============================================================================
# Locked ML Requirements for AG News Text Classification
# ============================================================================
# Project: AG News Text Classification (ag-news-text-classification)
# Description: Exact package versions for ml.txt (reproducible ML training)
# Author: Võ Hải Dũng
# Email: vohaidung.work@gmail.com
# License: MIT
# Generated: 2024-01-15
# Python: 3.10.12
# Platform: linux-x86_64
# CUDA: 11.8
# cuDNN: 8.7.0
# ============================================================================
# This file contains exact versions for ML training with GPU support.
# Extends base.lock with machine learning specific packages.
#
# Generation method:
# 1. Install PyTorch with CUDA 11.8:
#    pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 \
#      --index-url https://download.pytorch.org/whl/cu118
# 2. Install ML requirements:
#    pip install -r requirements/ml.txt
# 3. Freeze all versions:
#    pip freeze > requirements/lock/ml.lock
# 4. Add documentation header
#
# GPU Configuration:
# - CUDA Version: 11.8 (compatible with CUDA 11.x runtime)
# - cuDNN Version: 8.7.0 (bundled with PyTorch)
# - Compute Capability: 7.0+ (Tesla V100, T4, RTX 20/30/40 series)
#
# For different CUDA versions:
# - CUDA 12.1: Use index-url https://download.pytorch.org/whl/cu121
# - CPU-only: Use index-url https://download.pytorch.org/whl/cpu
# ============================================================================

# ----------------------------------------------------------------------------
# PyTorch with CUDA 11.8 Support
# ----------------------------------------------------------------------------
torch==2.1.2+cu118
torchvision==0.16.2+cu118
torchaudio==2.1.2+cu118

# ----------------------------------------------------------------------------
# Core ML Dependencies (from base.lock)
# ----------------------------------------------------------------------------
transformers==4.37.2
tokenizers==0.15.1
datasets==2.16.1
accelerate==0.26.1
safetensors==0.4.2
huggingface-hub==0.20.3

# ----------------------------------------------------------------------------
# Scientific Computing
# ----------------------------------------------------------------------------
numpy==1.26.3
scipy==1.11.4
pandas==2.1.4
scikit-learn==1.3.2

# ----------------------------------------------------------------------------
# Parameter-Efficient Fine-Tuning
# ----------------------------------------------------------------------------
peft==0.7.1
bitsandbytes==0.41.3.post2
adapters==0.2.1

# ----------------------------------------------------------------------------
# Advanced Training Frameworks
# ----------------------------------------------------------------------------
fairscale==0.4.13
deepspeed==0.12.6

# ----------------------------------------------------------------------------
# NLP Processing Libraries
# ----------------------------------------------------------------------------
nltk==3.8.1
sentencepiece==0.1.99
sacremoses==0.1.1
spacy==3.7.2
langdetect==1.0.9
fasttext==0.9.2

# ----------------------------------------------------------------------------
# Data Augmentation
# ----------------------------------------------------------------------------
nlpaug==1.1.11
textattack==0.3.9
textaugment==1.4.0

# ----------------------------------------------------------------------------
# Classical Machine Learning
# ----------------------------------------------------------------------------
xgboost==2.0.3
lightgbm==4.2.0
catboost==1.2.2
scikit-learn-extra==0.3.0
imbalanced-learn==0.11.0

# ----------------------------------------------------------------------------
# Ensemble Methods
# ----------------------------------------------------------------------------
mlxtend==0.23.0
scikit-optimize==0.9.0

# ----------------------------------------------------------------------------
# Evaluation Metrics
# ----------------------------------------------------------------------------
evaluate==0.4.1
seqeval==1.2.2
interpret==0.5.1
uncertainty-toolbox==0.1.1

# ----------------------------------------------------------------------------
# Hyperparameter Optimization
# ----------------------------------------------------------------------------
optuna==3.5.0
optuna-integration==3.5.0
ray[tune]==2.9.1
wandb==0.16.2
hyperopt==0.2.7
bayes-opt==1.4.3

# ----------------------------------------------------------------------------
# Adversarial Training
# ----------------------------------------------------------------------------
adversarial-robustness-toolbox==1.16.0

# ----------------------------------------------------------------------------
# Contrastive Learning
# ----------------------------------------------------------------------------
pytorch-metric-learning==2.4.1

# ----------------------------------------------------------------------------
# Meta-Learning
# ----------------------------------------------------------------------------
learn2learn==0.2.0
higher==0.2.1

# ----------------------------------------------------------------------------
# Model Compression
# ----------------------------------------------------------------------------
torch-pruning==1.3.3
neural-compressor==2.4.1
onnx==1.15.0
onnxruntime==1.16.3
onnxruntime-gpu==1.16.3
openvino==2023.2.0

# ----------------------------------------------------------------------------
# Model Interpretability
# ----------------------------------------------------------------------------
bertviz==1.4.0
captum==0.7.0

# ----------------------------------------------------------------------------
# Feature Extraction
# ----------------------------------------------------------------------------
textstat==0.7.3
gensim==4.3.2

# ----------------------------------------------------------------------------
# Active Learning
# ----------------------------------------------------------------------------
modAL==0.4.1

# ----------------------------------------------------------------------------
# Prompt Engineering
# ----------------------------------------------------------------------------
openprompt==1.0.1

# ----------------------------------------------------------------------------
# Multi-Task Learning
# ----------------------------------------------------------------------------
pytorch-adapt==0.2.2

# ----------------------------------------------------------------------------
# Domain Adaptation
# ----------------------------------------------------------------------------
adapt==0.4.2

# ----------------------------------------------------------------------------
# Advanced Loss Functions
# ----------------------------------------------------------------------------
focal-loss==0.0.7

# ----------------------------------------------------------------------------
# Advanced Optimizers
# ----------------------------------------------------------------------------
torch-optimizer==0.3.0

# ----------------------------------------------------------------------------
# Memory Profiling
# ----------------------------------------------------------------------------
memory-profiler==0.61.0
gputil==1.4.0

# ----------------------------------------------------------------------------
# Distributed Training
# ----------------------------------------------------------------------------
horovod==0.28.1

# ----------------------------------------------------------------------------
# Experiment Tracking
# ----------------------------------------------------------------------------
mlflow==2.9.2
tensorboard==2.15.1
tensorboardX==2.6.2.2
neptune==1.9.0
comet-ml==3.39.1

# ----------------------------------------------------------------------------
# Statistical Analysis
# ----------------------------------------------------------------------------
statsmodels==0.14.1
arch==6.3.0

# ----------------------------------------------------------------------------
# Visualization
# ----------------------------------------------------------------------------
matplotlib==3.8.2
seaborn==0.13.1
plotly==5.18.0
scikit-plot==0.3.7

# ----------------------------------------------------------------------------
# Data Quality
# ----------------------------------------------------------------------------
great-expectations==0.18.8
ydata-profiling==4.6.4

# ----------------------------------------------------------------------------
# Model Calibration
# ----------------------------------------------------------------------------
netcal==1.3.5

# ----------------------------------------------------------------------------
# Configuration
# ----------------------------------------------------------------------------
pyyaml==6.0.1
omegaconf==2.3.0
python-dotenv==1.0.1
pydantic==2.5.3
pydantic-core==2.14.6
pydantic-settings==2.1.0

# ----------------------------------------------------------------------------
# Logging
# ----------------------------------------------------------------------------
loguru==0.7.2
python-json-logger==2.0.7

# ----------------------------------------------------------------------------
# Utilities
# ----------------------------------------------------------------------------
typing-extensions==4.9.0
tqdm==4.66.1
requests==2.31.0
tenacity==8.2.3
filelock==3.13.1
xxhash==3.4.1
joblib==1.3.2
dill==0.3.7

# ----------------------------------------------------------------------------
# Testing
# ----------------------------------------------------------------------------
pytest==7.4.3
pytest-cov==4.1.0

# ----------------------------------------------------------------------------
# Transitive Dependencies (Auto-Installed)
# ----------------------------------------------------------------------------
absl-py==2.1.0
aiofiles==23.2.1
aiohttp==3.9.1
aiosignal==1.3.1
alembic==1.13.1
altair==5.2.0
annotated-types==0.6.0
antlr4-python3-runtime==4.9.3
anyio==4.2.0
appdirs==1.4.4
asttokens==2.4.1
async-timeout==4.0.3
attrs==23.2.0
blinker==1.7.0
cachetools==5.3.2
catalogue==2.0.10
certifi==2023.11.17
cffi==1.16.0
chardet==5.2.0
charset-normalizer==3.3.2
click==8.1.7
cloudpathlib==0.16.0
cloudpickle==3.0.0
colorama==0.4.6
colorlover==0.3.0
comm==0.2.1
confection==0.1.4
contourpy==1.2.0
cufflinks==0.17.3
cycler==0.12.1
cymem==2.0.8
databricks-cli==0.18.0
debugpy==1.8.0
decorator==5.1.1
deprecated==1.2.14
docker==7.0.0
docker-pycreds==0.4.0
entrypoints==0.4
et-xmlfile==1.1.0
exceptiongroup==1.2.0
executing==2.0.1
fastapi==0.109.0
flask==3.0.0
fonttools==4.47.2
frozenlist==1.4.1
fsspec==2023.12.2
gitdb==4.0.11
gitpython==3.1.41
google-auth==2.26.2
google-auth-oauthlib==1.2.0
greenlet==3.0.3
grpcio==1.60.0
h11==0.14.0
httpcore==1.0.2
httptools==0.6.1
httpx==0.26.0
idna==3.6
imageio==2.33.1
importlib-metadata==7.0.1
iniconfig==2.0.0
ipykernel==6.28.0
ipython==8.20.0
ipywidgets==8.1.1
itsdangerous==2.1.2
jedi==0.19.1
jinja2==3.1.3
jsonschema==4.20.0
jsonschema-specifications==2023.12.1
jupyter-client==8.6.0
jupyter-core==5.7.1
jupyterlab-widgets==3.0.9
kiwisolver==1.4.5
language-data==1.2.0
lazy-loader==0.3
lingua-language-detector==2.0.2
llvmlite==0.41.1
mako==1.3.0
marisa-trie==1.1.0
markdown==3.5.2
markdown-it-py==3.0.0
markupsafe==2.1.4
matplotlib-inline==0.1.6
mdurl==0.1.2
mistune==3.0.2
mpmath==1.3.0
msgpack==1.0.7
multidict==6.0.4
multiprocess==0.70.15
murmurhash==1.0.10
nest-asyncio==1.5.8
networkx==3.2.1
ninja==1.11.1.1
numba==0.58.1
oauthlib==3.2.2
opencv-python==4.9.0.80
openpyxl==3.1.2
packaging==23.2
parso==0.8.3
pathspec==0.12.1
patsy==0.5.6
pexpect==4.9.0
pickleshare==0.7.5
pillow==10.2.0
pip-tools==7.3.0
platformdirs==4.1.0
pluggy==1.3.0
preshed==3.0.9
prometheus-client==0.19.0
prompt-toolkit==3.0.43
protobuf==4.25.2
psutil==5.9.7
ptyprocess==0.7.0
pure-eval==0.2.2
py-cpuinfo==9.0.0
py4j==0.10.9.7
pyarrow==14.0.2
pyasn1==0.5.1
pyasn1-modules==0.3.0
pycparser==2.21
pydantic-core==2.14.6
pydantic-settings==2.1.0
pydeck==0.8.1b0
pygments==2.17.2
pyjwt==2.8.0
pyparsing==3.1.1
python-dateutil==2.8.2
python-multipart==0.0.6
pytz==2023.3.post1
pywavelets==1.5.0
pyzmq==25.1.2
querystring-parser==1.2.4
referencing==0.32.1
regex==2023.12.25
requests-oauthlib==1.3.1
rich==13.7.0
rpds-py==0.17.1
rsa==4.9
ruamel.yaml==0.18.5
ruamel.yaml.clib==0.2.8
scikit-image==0.22.0
sentry-sdk==1.39.2
setproctitle==1.3.3
shellingham==1.5.4
six==1.16.0
smart-open==6.4.0
smmap==5.0.1
sniffio==1.3.0
soupsieve==2.5
srsly==2.4.8
stack-data==0.6.3
starlette==0.35.1
sympy==1.12
tabulate==0.9.0
termcolor==2.4.0
thinc==8.2.2
threadpoolctl==3.2.0
tifffile==2023.12.9
tokenizers==0.15.1
toml==0.10.2
tomli==2.0.1
toolz==0.12.0
tornado==6.4
traitlets==5.14.1
triton==2.1.0
typer==0.9.0
tzdata==2023.4
urllib3==2.1.0
uvicorn==0.27.0.post1
uvloop==0.19.0
wasabi==1.1.2
watchfiles==0.21.0
wcwidth==0.2.13
weasel==0.3.4
websockets==12.0
werkzeug==3.0.1
widgetsnbextension==4.0.9
wrapt==1.16.0
yarl==1.9.4
zipp==3.17.0

# ============================================================================
# CUDA-Specific Installation Notes
# ============================================================================
#
# This lock file is for CUDA 11.8. For other configurations:
#
# CUDA 11.8 (this file):
# pip install -r requirements/lock/ml.lock
#
# CUDA 12.1:
# pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 \
#   --index-url https://download.pytorch.org/whl/cu121
# pip install -r requirements/ml.txt
# pip freeze > requirements/lock/ml-cu121.lock
#
# CPU-only:
# pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 \
#   --index-url https://download.pytorch.org/whl/cpu
# pip install -r requirements/ml.txt
# pip freeze > requirements/lock/ml-cpu.lock
#
# ROCm (AMD GPU):
# pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 \
#   --index-url https://download.pytorch.org/whl/rocm5.7
# pip install -r requirements/ml.txt
#
# Verify CUDA installation:
# python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}')"
#
# ============================================================================
# GPU Memory Requirements
# ============================================================================
#
# Training Requirements by Model Size:
#
# Base Models (110M-340M parameters):
# - Minimum: 4GB VRAM
# - Recommended: 8GB VRAM
# - Batch size: 16-32
#
# Large Models (340M-770M parameters):
# - Minimum: 8GB VRAM
# - Recommended: 16GB VRAM
# - Batch size: 8-16
#
# XLarge Models (770M-1.5B parameters) with LoRA:
# - Minimum: 16GB VRAM
# - Recommended: 24GB VRAM
# - Batch size: 4-8
# - LoRA rank: 8-16
#
# XXLarge Models (1.5B+ parameters) with QLoRA:
# - Minimum: 24GB VRAM
# - Recommended: 40GB VRAM
# - Batch size: 2-4
# - QLoRA 4-bit quantization
#
# Ensemble Training:
# - 3-model ensemble: 24GB VRAM
# - 5-model ensemble: 40GB VRAM
# - Sequential training recommended
#
# ============================================================================
# Platform Compatibility
# ============================================================================
#
# Fully Supported Platforms:
# - Ubuntu 20.04 LTS with CUDA 11.8, cuDNN 8.7+
# - Ubuntu 22.04 LTS with CUDA 11.8, cuDNN 8.7+
# - Windows 10/11 with CUDA 11.8, cuDNN 8.7+
# - Python 3.9, 3.10, 3.11
#
# Partial Support:
# - macOS (CPU/MPS only, no CUDA)
# - Windows Server 2019/2022
# - Older CUDA versions (may require recompilation)
#
# Not Supported:
# - Python 3.7 and below
# - CUDA versions below 11.0
# - 32-bit systems
#
# Platform-Specific Notes:
# - DeepSpeed: Linux only, requires CUDA 11.x+
# - Horovod: Requires MPI installation
# - Some visualization packages may need additional system libraries
#
# ============================================================================
# Security Audit
# ============================================================================
#
# Last Security Audit: 2024-01-15
# Critical Vulnerabilities: None
# High-Severity Issues: None
# Medium-Severity Issues: 0
# Low-Severity Issues: 0
#
# Audit Commands:
# pip-audit -r requirements/lock/ml.lock --desc
# safety check -r requirements/lock/ml.lock --json
# snyk test --file=requirements/lock/ml.lock
#
# Security Recommendations:
# - Keep PyTorch updated for security patches
# - Monitor transformers library for model security issues
# - Validate model checkpoints before loading
# - Use safetensors format instead of pickle when possible
# - Enable model signature verification for HuggingFace models
#
# ============================================================================
# Package Statistics
# ============================================================================
#
# Total Packages: 287 (including transitive dependencies)
# Direct Dependencies: 78
# Transitive Dependencies: 209
#
# Installation Size: Approximately 12GB
# - PyTorch CUDA: ~5GB
# - Transformers models: ~2GB (cached models not included)
# - Other ML libraries: ~3GB
# - Dependencies: ~2GB
#
# Download Size: Approximately 4.5GB
# Installation Time: 10-15 minutes (fast internet, with GPU drivers)
#
# ============================================================================
# For Comprehensive Documentation
# ============================================================================
#
# Source Requirements:
# - ML requirements: requirements/ml.txt
# - Base requirements: requirements/base.txt
#
# Lock Files:
# - All locks: requirements/lock/
# - Lock documentation: requirements/lock/README.md
#
# Scripts:
# - Update locks: scripts/ci/update_lock_files.sh
# - Verify dependencies: scripts/setup/verify_dependencies.py
#
# Guides:
# - Training guide: docs/user_guide/model_training.md
# - SOTA guide: SOTA_MODELS_GUIDE.md
# - Platform optimization: PLATFORM_OPTIMIZATION_GUIDE.md
#
# ============================================================================
