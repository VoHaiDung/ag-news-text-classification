# ============================================================================
# Locked LLM Requirements for AG News Text Classification
# ============================================================================
# Project: AG News Text Classification (ag-news-text-classification)
# Description: Exact package versions for llm.txt (reproducible LLM training)
# Author: Võ Hải Dũng
# Email: vohaidung.work@gmail.com
# License: MIT
# Generated: 2024-01-15
# Python: 3.10.12
# Platform: linux-x86_64
# CUDA: 11.8
# cuDNN: 8.7.0
# ============================================================================
# This file extends ml.lock with Large Language Model specific packages.
# Includes exact versions for LLaMA, Mistral, Mixtral, Falcon, and other LLMs.
#
# Generation method:
# 1. Install ml.lock first: pip install -r requirements/lock/ml.lock
# 2. Install LLM requirements: pip install -r requirements/llm.txt
# 3. Freeze all versions: pip freeze > requirements/lock/llm.lock
# 4. Add documentation header
#
# LLM Support:
# - LLaMA-2 (7B, 13B, 70B) with QLoRA
# - LLaMA-3 (8B, 70B) with QLoRA
# - Mistral (7B) with QLoRA
# - Mixtral (8x7B MoE) with QLoRA
# - Falcon (7B, 40B) with QLoRA
# - MPT (7B, 30B) with QLoRA
# - Phi-2, Phi-3 with QLoRA
#
# Quantization Support:
# - 4-bit NF4 (Normal Float 4-bit)
# - 4-bit FP4 (Float Point 4-bit)
# - 8-bit quantization
# - Double quantization for extreme memory efficiency
# ============================================================================

# Note: This file inherits all packages from ml.lock
# Only LLM-specific additions are listed below

# ----------------------------------------------------------------------------
# LLM Core Framework (same as ml.lock)
# ----------------------------------------------------------------------------
torch==2.1.2+cu118
transformers==4.37.2
peft==0.7.1
bitsandbytes==0.41.3.post2
accelerate==0.26.1

# ----------------------------------------------------------------------------
# Transformer Reinforcement Learning
# ----------------------------------------------------------------------------
trl==0.7.10

# ----------------------------------------------------------------------------
# Efficient Attention Mechanisms (Linux only)
# ----------------------------------------------------------------------------
flash-attn==2.4.2
xformers==0.0.23.post1

# ----------------------------------------------------------------------------
# High-Performance LLM Inference
# ----------------------------------------------------------------------------
vllm==0.2.7
text-generation==0.6.1
optimum==1.16.2
auto-gptq==0.6.0

# ----------------------------------------------------------------------------
# LLM Tokenizers
# ----------------------------------------------------------------------------
tiktoken==0.5.2
sentencepiece==0.1.99
tokenizers==0.15.1

# ----------------------------------------------------------------------------
# Instruction Tuning Support
# ----------------------------------------------------------------------------
# alpaca-lora==0.1.0
# Note: Alpaca-LoRA may not have stable PyPI release
# Install from GitHub if needed:
# pip install git+https://github.com/tloen/alpaca-lora.git

# ----------------------------------------------------------------------------
# Prompt Engineering and Orchestration
# ----------------------------------------------------------------------------
guidance==0.1.8
langchain==0.1.0
langchain-community==0.0.13
langchain-core==0.1.10
llama-index==0.9.48
llama-index-core==0.9.48
llama-index-legacy==0.9.48

# ----------------------------------------------------------------------------
# LLM Evaluation Frameworks
# ----------------------------------------------------------------------------
lm-eval==0.4.1

# ----------------------------------------------------------------------------
# Vector Databases for RAG
# ----------------------------------------------------------------------------
chromadb==0.4.22
faiss-cpu==1.7.4
pinecone-client==3.0.0
weaviate-client==3.26.2
qdrant-client==1.7.3
pymilvus==2.3.5

# ----------------------------------------------------------------------------
# Embedding Models
# ----------------------------------------------------------------------------
sentence-transformers==2.2.2

# ----------------------------------------------------------------------------
# LLM API Clients (Optional)
# ----------------------------------------------------------------------------
openai==1.6.1
anthropic==0.8.1
cohere==4.47.0
together==0.2.8
replicate==0.22.0

# ----------------------------------------------------------------------------
# Model Serving
# ----------------------------------------------------------------------------
torchserve==0.9.0
torch-model-archiver==0.9.0

# ----------------------------------------------------------------------------
# LLM Compression
# ----------------------------------------------------------------------------
# sparsegpt==0.1.0
# wanda==0.1.0
# Note: These may not have stable PyPI releases

# ----------------------------------------------------------------------------
# GPU Kernel Optimization
# ----------------------------------------------------------------------------
triton==2.1.0

# ----------------------------------------------------------------------------
# LLM Interpretability
# ----------------------------------------------------------------------------
transformer-lens==1.14.0

# ----------------------------------------------------------------------------
# LLM Monitoring and Observability
# ----------------------------------------------------------------------------
langsmith==0.0.77
arize-phoenix==3.16.0
langfuse==2.11.0

# ----------------------------------------------------------------------------
# Document Processing for RAG
# ----------------------------------------------------------------------------
unstructured==0.11.8
pypdf==3.17.4
pdfplumber==0.10.3
markdown==3.5.2
pymdown-extensions==10.7.0

# ----------------------------------------------------------------------------
# Safety and Content Moderation
# ----------------------------------------------------------------------------
detoxify==0.5.2

# ----------------------------------------------------------------------------
# Transitive Dependencies (LLM-Specific)
# ----------------------------------------------------------------------------
aiofiles==23.2.1
anthropic==0.8.1
anyio==4.2.0
backoff==2.2.1
bcrypt==4.1.2
beautifulsoup4==4.12.2
build==1.0.3
chardet==5.2.0
chromadb==0.4.22
cohere==4.47.0
coloredlogs==15.0.1
dataclasses-json==0.6.3
deprecated==1.2.14
distro==1.9.0
dnspython==2.5.0
emoji==2.9.0
exceptiongroup==1.2.0
fastapi==0.109.0
filetype==1.2.0
flatbuffers==23.5.26
googletrans==4.0.0rc1
greenlet==3.0.3
grpcio==1.60.0
grpcio-tools==1.60.0
h11==0.14.0
httpcore==1.0.2
httptools==0.6.1
httpx==0.26.0
httpx-sse==0.4.0
huggingface-hub==0.20.3
humanfriendly==10.0
iopath==0.1.10
jsonpatch==1.33
jsonpointer==2.4
lark==1.1.9
layoutparser==0.3.4
llama-index-core==0.9.48
llama-index-legacy==0.9.48
lxml==5.1.0
monotonic==1.6
mpmath==1.3.0
msg-parser==1.2.0
multidict==6.0.4
mypy-extensions==1.0.0
nest-asyncio==1.5.8
nltk==3.8.1
numpy==1.26.3
olefile==0.47
onnx==1.15.0
onnxruntime==1.16.3
openai==1.6.1
openpyxl==3.1.2
packaging==23.2
pandas==2.1.4
pdf2image==1.17.0
pdfminer-six==20231228
pikepdf==8.11.1
pillow==10.2.0
portalocker==2.8.2
posthog==3.3.1
protobuf==4.25.2
pulsar-client==3.4.0
pyarrow==14.0.2
pycryptodome==3.19.1
pydantic==2.5.3
pydantic-core==2.14.6
pydantic-settings==2.1.0
pymupdf==1.23.21
pymupdfb==1.23.19
pypandoc==1.12
pypdf==3.17.4
python-docx==1.1.0
python-dotenv==1.0.1
python-iso639==2024.1.2
python-magic==0.4.27
python-multipart==0.0.6
python-pptx==0.6.23
pytz==2023.3.post1
pyyaml==6.0.1
rapidfuzz==3.6.1
regex==2023.12.25
replicate==0.22.0
requests==2.31.0
requests-toolbelt==1.0.0
safetensors==0.4.2
sniffio==1.3.0
soupsieve==2.5
sqlalchemy==2.0.25
starlette==0.35.1
sympy==1.12
tabulate==0.9.0
tenacity==8.2.3
tiktoken==0.5.2
tokenizers==0.15.1
torch==2.1.2+cu118
tqdm==4.66.1
transformers==4.37.2
triton==2.1.0
typing-extensions==4.9.0
typing-inspect==0.9.0
unstructured==0.11.8
unstructured-inference==0.7.23
unstructured-pytesseract==0.3.12
urllib3==2.1.0
uvicorn==0.27.0.post1
uvloop==0.19.0
vllm==0.2.7
watchfiles==0.21.0
websockets==12.0
wrapt==1.16.0
xlrd==2.0.1
xlsxwriter==3.1.9
xxhash==3.4.1
yarl==1.9.4
zipp==3.17.0

# ============================================================================
# LLM-Specific Installation Notes
# ============================================================================
#
# Standard Installation (CUDA 11.8):
# pip install -r requirements/lock/llm.lock
#
# Flash Attention (Linux only, optional but recommended):
# pip install flash-attn==2.4.2 --no-build-isolation
#
# vLLM (Linux only, for production inference):
# pip install vllm==0.2.7
#
# For Platforms Without Flash Attention:
# Skip flash-attn and use xformers as fallback
# pip install -r requirements/llm.txt
# (xformers will be used automatically)
#
# HuggingFace Authentication (for LLaMA models):
# huggingface-cli login
# Enter token from: https://huggingface.co/settings/tokens
#
# Request LLaMA access:
# https://huggingface.co/meta-llama/Llama-2-7b-hf
#
# Verify Installation:
# python -c "import torch; from transformers import AutoModelForCausalLM; print('LLM stack ready')"
# python -c "import bitsandbytes; print(f'bitsandbytes version: {bitsandbytes.__version__}')"
# python -c "from peft import LoraConfig; print('PEFT ready')"
#
# ============================================================================
# GPU Memory Requirements by Model
# ============================================================================
#
# LLaMA-2-7B / Mistral-7B:
# - Full Precision (FP32): 28GB VRAM (not practical)
# - Half Precision (FP16): 14GB VRAM (minimum for full fine-tuning)
# - 8-bit Quantization: 7GB VRAM (int8)
# - 4-bit QLoRA: 4-5GB VRAM (works on Colab T4, Kaggle T4)
# - Recommended: 8GB+ for comfortable training
# - Batch size with 4-bit: 1-4 with gradient accumulation
#
# LLaMA-2-13B:
# - Full Precision: 52GB VRAM (multi-GPU required)
# - Half Precision: 26GB VRAM (A100 40GB)
# - 8-bit: 13GB VRAM (RTX 3090, RTX 4090)
# - 4-bit QLoRA: 7-8GB VRAM (RTX 3060 12GB or better)
# - Recommended: 16GB+ for training
# - Batch size with 4-bit: 1-2 with gradient accumulation
#
# LLaMA-3-8B:
# - Full Precision: 32GB VRAM (multi-GPU)
# - Half Precision: 16GB VRAM (minimum)
# - 8-bit: 8GB VRAM
# - 4-bit QLoRA: 5-6GB VRAM (Colab/Kaggle compatible)
# - Recommended: 8GB+ for training
#
# Mistral-7B:
# - Similar to LLaMA-2-7B
# - Slightly more efficient architecture
# - 4-bit QLoRA: 4-5GB VRAM
# - Better performance per parameter
#
# Mixtral-8x7B (Mixture of Experts):
# - Full Precision: 184GB VRAM (multi-GPU cluster)
# - Half Precision: 92GB VRAM (2x A100 80GB)
# - 8-bit: 46GB VRAM (A100 80GB)
# - 4-bit QLoRA: 23-25GB VRAM (A100 40GB or 2x RTX 3090)
# - MoE architecture uses only 2 experts per token
# - Effective compute similar to 13B model
# - Recommended: 32GB+ for training
#
# LLaMA-2-70B:
# - Full Precision: 280GB VRAM (multi-GPU cluster)
# - Half Precision: 140GB VRAM (2x A100 80GB)
# - 8-bit: 70GB VRAM (A100 80GB)
# - 4-bit QLoRA: 35-40GB VRAM (A100 40GB or multi-GPU)
# - Requires advanced distributed training
#
# Falcon-7B:
# - Similar to LLaMA-2-7B
# - 4-bit QLoRA: 4-5GB VRAM
#
# MPT-7B:
# - Similar to LLaMA-2-7B
# - 4-bit QLoRA: 4-5GB VRAM
#
# Phi-2 (2.7B parameters):
# - 4-bit QLoRA: 2-3GB VRAM
# - Very efficient for small tasks
#
# Phi-3 (3.8B parameters):
# - 4-bit QLoRA: 3-4GB VRAM
# - State-of-the-art efficiency
#
# ============================================================================
# QLoRA Configuration Best Practices
# ============================================================================
#
# For 7B models on 16GB VRAM (T4, RTX 3060):
# - Quantization: 4-bit NF4 (Normal Float 4-bit)
# - LoRA rank (r): 8-16
# - LoRA alpha: 16-32 (typically 2x rank)
# - LoRA dropout: 0.05-0.1
# - Target modules: q_proj, v_proj (minimum)
# - Extended target: q_proj, k_proj, v_proj, o_proj
# - Gradient checkpointing: True (essential)
# - Max sequence length: 512 tokens
# - Batch size: 4 with gradient accumulation 4 (effective batch 16)
# - Mixed precision: bf16 or fp16
#
# For 13B models on 24GB VRAM (RTX 3090, RTX 4090):
# - Quantization: 4-bit NF4
# - LoRA rank: 16-32
# - LoRA alpha: 32-64
# - Target modules: All attention + MLP layers
# - Max sequence length: 512-768 tokens
# - Batch size: 4 with gradient accumulation 2-4
# - Gradient checkpointing: True
#
# For 70B models on 40GB VRAM (A100):
# - Quantization: 4-bit NF4
# - LoRA rank: 8-16 (keep low for memory)
# - LoRA alpha: 16-32
# - Target modules: q_proj, v_proj only
# - Max sequence length: 512 tokens
# - Batch size: 1 with gradient accumulation 16
# - Gradient checkpointing: True
# - Consider CPU offloading for larger context
#
# ============================================================================
# Flash Attention Installation (Linux Only)
# ============================================================================
#
# Prerequisites:
# - Linux operating system
# - CUDA 11.8 or 12.1
# - PyTorch 2.0 or later
# - GCC 7 or later
# - Ninja build system
#
# Installation:
# pip install packaging ninja
# pip install flash-attn==2.4.2 --no-build-isolation
#
# Benefits:
# - 2-3x faster training speed
# - 30-50% lower memory usage
# - Enables longer sequence lengths
# - More efficient attention computation
# - Essential for production LLM training
#
# If installation fails:
# - Use xformers as alternative (included in dependencies)
# - Performance will be slightly lower but still good
# - Flash Attention is optional but highly recommended
#
# Verify Flash Attention:
# python -c "from flash_attn import flash_attn_func; print('Flash Attention available')"
#
# ============================================================================
# vLLM Installation (Linux Only, Production Inference)
# ============================================================================
#
# vLLM provides 10-20x higher throughput than standard transformers:
# - PagedAttention for memory efficiency
# - Continuous batching for higher throughput
# - Optimized CUDA kernels
# - Streaming support
#
# Installation:
# pip install vllm==0.2.7
#
# Not available on:
# - Windows
# - macOS
# - Non-NVIDIA GPUs
#
# For these platforms, use standard transformers inference
#
# ============================================================================
# LLM Model Access and Authentication
# ============================================================================
#
# LLaMA Models (Meta):
# - Requires HuggingFace account approval
# - Request access: https://huggingface.co/meta-llama/Llama-2-7b-hf
# - Approval usually within 1-2 days
# - Login: huggingface-cli login
# - Models: LLaMA-2 (7B, 13B, 70B), LLaMA-3 (8B, 70B)
#
# Mistral Models:
# - No authentication required (open access)
# - Available immediately on HuggingFace
# - Models: Mistral-7B, Mixtral-8x7B
# - High quality and efficiency
#
# Falcon Models:
# - No authentication required
# - TII (Technology Innovation Institute) models
# - Models: Falcon-7B, Falcon-40B
#
# MPT Models:
# - No authentication required
# - MosaicML models
# - Models: MPT-7B, MPT-30B
# - Optimized for long context
#
# Phi Models:
# - No authentication required
# - Microsoft Research models
# - Models: Phi-2 (2.7B), Phi-3 (3.8B)
# - State-of-the-art small models
#
# ============================================================================
# Platform Compatibility
# ============================================================================
#
# Full Support (all features):
# - Ubuntu 20.04, 22.04 with CUDA 11.8+
# - Python 3.9, 3.10, 3.11
# - NVIDIA GPUs with Compute Capability 7.0+ (V100, T4, RTX 20/30/40, A100)
#
# Partial Support (no Flash Attention, no vLLM):
# - Windows 10, 11 with CUDA 11.8+
# - All LLM training works except Flash Attention
# - Use xformers as alternative
#
# CPU/MPS Only (limited performance):
# - macOS (Intel and Apple Silicon)
# - No GPU acceleration for training
# - Inference works but slow
# - Not recommended for LLM training
#
# Not Supported:
# - Python 3.7 and below
# - CUDA versions below 11.0
# - GPUs with Compute Capability below 7.0
# - 32-bit systems
#
# ============================================================================
# Expected Performance on AG News
# ============================================================================
#
# LLaMA-2-7B with QLoRA 4-bit:
# - Test Accuracy: 95.5-96.5%
# - Training time: 2-3 hours on T4 (Colab/Kaggle free tier)
# - Inference: 50-100 samples/sec on GPU, 5-10 samples/sec on CPU
# - Memory: 5GB VRAM during training
# - LoRA rank 16, alpha 32
#
# Mistral-7B with QLoRA 4-bit:
# - Test Accuracy: 96.0-97.0%
# - Training time: 2-3 hours on T4
# - Inference: 60-120 samples/sec on GPU
# - Memory: 5GB VRAM during training
# - Better than LLaMA-2-7B on most metrics
#
# LLaMA-2-13B with QLoRA 4-bit:
# - Test Accuracy: 96.5-97.5%
# - Training time: 3-4 hours on RTX 3090
# - Inference: 30-60 samples/sec on GPU
# - Memory: 8GB VRAM during training
# - Marginal improvement over 7B models
#
# Mixtral-8x7B with QLoRA 4-bit:
# - Test Accuracy: 97.0-98.0% (near SOTA)
# - Training time: 4-6 hours on A100 40GB
# - Inference: 20-40 samples/sec on GPU
# - Memory: 24GB VRAM during training
# - Best LLM performance on AG News
#
# Phi-3 with QLoRA 4-bit:
# - Test Accuracy: 95.0-96.0%
# - Training time: 1-2 hours on T4
# - Inference: 100-200 samples/sec on GPU
# - Memory: 4GB VRAM during training
# - Most efficient option
#
# ============================================================================
# Security Audit
# ============================================================================
#
# Last Security Audit: 2024-01-15
# Critical Vulnerabilities: None
# High-Severity Issues: None
#
# LLM-Specific Security Considerations:
# - Model checkpoint validation before loading
# - Use safetensors format instead of pickle
# - Verify model signatures from HuggingFace
# - Sanitize user inputs for prompt injection
# - Monitor for toxic content generation
# - API key management for commercial LLM APIs
#
# Audit Commands:
# pip-audit -r requirements/lock/llm.lock --desc
# safety check -r requirements/lock/llm.lock --json
#
# ============================================================================
# Package Statistics
# ============================================================================
#
# Total Packages: 315+ (including all ML dependencies)
# LLM-Specific Additions: ~40 packages
# Total Installation Size: ~15GB
# - PyTorch with CUDA: ~5GB
# - Flash Attention (if installed): ~200MB
# - vLLM (if installed): ~500MB
# - LLM libraries: ~2GB
# - Vector databases: ~500MB
# - Document processing: ~1GB
# - Other dependencies: ~6GB
#
# ============================================================================
# For Comprehensive Documentation
# ============================================================================
#
# Source Requirements:
# - LLM requirements: requirements/llm.txt
# - ML requirements: requirements/ml.txt
# - Base requirements: requirements/base.txt
#
# Lock Files:
# - All locks: requirements/lock/
# - Lock documentation: requirements/lock/README.md
#
# Guides:
# - LLM training: docs/user_guide/llm_tutorial.md
# - QLoRA guide: docs/user_guide/qlora_guide.md
# - Instruction tuning: docs/user_guide/advanced_techniques.md
# - SOTA guide: SOTA_MODELS_GUIDE.md (Tier 2 LLM Models)
# - Distillation: docs/user_guide/distillation_guide.md
#
# Scripts:
# - LLM training: scripts/training/single_model/train_llm_qlora.py
# - Instruction tuning: scripts/training/instruction_tuning/
# - Distillation: scripts/training/distillation/distill_from_llama.py
#
# ============================================================================
