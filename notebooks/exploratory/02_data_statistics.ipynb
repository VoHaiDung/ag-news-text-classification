{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of AG News Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs comprehensive statistical analysis following methodologies from:\n",
    "- Bengio & Grandvalet (2004): \"No Unbiased Estimator of the Variance of K-Fold Cross-Validation\"\n",
    "- McNemar (1947): \"Note on the Sampling Error of the Difference Between Correlated Proportions\"\n",
    "\n",
    "### Statistical Tests Performed\n",
    "1. Descriptive statistics\n",
    "2. Distribution analysis\n",
    "3. Correlation analysis\n",
    "4. Hypothesis testing\n",
    "5. Feature importance analysis\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Date: 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Data manipulation and statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency, kstest, normaltest, shapiro\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.data.preprocessing.feature_extraction import FeatureExtractor, FeatureExtractionConfig\n",
    "from src.utils.io_utils import safe_save, ensure_dir\n",
    "from configs.constants import AG_NEWS_CLASSES, DATA_DIR, ID_TO_LABEL\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Statistical Analysis of AG News Dataset\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "config = AGNewsConfig(data_dir=DATA_DIR / \"processed\")\n",
    "train_dataset = AGNewsDataset(config, split=\"train\")\n",
    "val_dataset = AGNewsDataset(config, split=\"validation\")\n",
    "test_dataset = AGNewsDataset(config, split=\"test\")\n",
    "\n",
    "# Create DataFrames\n",
    "train_df = pd.DataFrame({\n",
    "    'text': train_dataset.texts,\n",
    "    'label': train_dataset.labels,\n",
    "    'label_name': train_dataset.label_names\n",
    "})\n",
    "\n",
    "# Add text statistics\n",
    "train_df['word_count'] = train_df['text'].str.split().str.len()\n",
    "train_df['char_count'] = train_df['text'].str.len()\n",
    "train_df['avg_word_length'] = train_df['char_count'] / train_df['word_count']\n",
    "train_df['sentence_count'] = train_df['text'].str.count(r'[.!?]') + 1\n",
    "\n",
    "print(f\"Dataset loaded: {len(train_df):,} training samples\")\n",
    "print(f\"Features computed: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "print(\"Overall Text Statistics\")\n",
    "print(\"=\"*50)\n",
    "print(train_df[['word_count', 'char_count', 'avg_word_length', 'sentence_count']].describe())\n",
    "\n",
    "# Per-class statistics\n",
    "print(\"\\nPer-Class Statistics\")\n",
    "print(\"=\"*50)\n",
    "class_stats = train_df.groupby('label_name')[['word_count', 'char_count']].agg([\n",
    "    'mean', 'std', 'min', 'max', 'median'\n",
    "]).round(2)\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for normality\n",
    "print(\"Normality Tests (Shapiro-Wilk)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample for Shapiro test (max 5000 samples)\n",
    "sample_size = min(5000, len(train_df))\n",
    "sample_df = train_df.sample(sample_size, random_state=42)\n",
    "\n",
    "for feature in ['word_count', 'char_count', 'avg_word_length']:\n",
    "    stat, p_value = shapiro(sample_df[feature])\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Statistic: {stat:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.4e}\")\n",
    "    print(f\"  Normal: {'No' if p_value < 0.05 else 'Yes'} (α=0.05)\")\n",
    "    print()\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "features = ['word_count', 'char_count', 'avg_word_length', 'sentence_count']\n",
    "for idx, (ax, feature) in enumerate(zip(axes.flat, features)):\n",
    "    for label_name in AG_NEWS_CLASSES:\n",
    "        data = train_df[train_df['label_name'] == label_name][feature]\n",
    "        ax.hist(data, alpha=0.5, label=label_name, bins=30)\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Distributions by Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA test for differences between classes\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "print(\"One-Way ANOVA: Testing for Differences Between Classes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feature in ['word_count', 'char_count', 'avg_word_length', 'sentence_count']:\n",
    "    groups = [train_df[train_df['label_name'] == label][feature] for label in AG_NEWS_CLASSES]\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    \n",
    "    print(f\"\\n{feature.replace('_', ' ').title()}:\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.4e}\")\n",
    "    print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'} (α=0.05)\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        # Post-hoc pairwise comparisons\n",
    "        from itertools import combinations\n",
    "        print(\"  Pairwise comparisons (t-test):\")\n",
    "        for class1, class2 in combinations(AG_NEWS_CLASSES, 2):\n",
    "            data1 = train_df[train_df['label_name'] == class1][feature]\n",
    "            data2 = train_df[train_df['label_name'] == class2][feature]\n",
    "            t_stat, p_val = stats.ttest_ind(data1, data2)\n",
    "            if p_val < 0.05:\n",
    "                print(f\"    {class1} vs {class2}: p={p_val:.4f} *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "numeric_features = ['word_count', 'char_count', 'avg_word_length', 'sentence_count', 'label']\n",
    "correlation_matrix = train_df[numeric_features].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute Spearman correlation (non-parametric)\n",
    "print(\"\\nSpearman Correlation with Labels:\")\n",
    "print(\"=\"*40)\n",
    "for feature in ['word_count', 'char_count', 'avg_word_length', 'sentence_count']:\n",
    "    corr, p_value = stats.spearmanr(train_df[feature], train_df['label'])\n",
    "    print(f\"{feature}: ρ={corr:.4f}, p={p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chi-Square Test for Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize continuous features for chi-square test\n",
    "train_df['word_count_bin'] = pd.qcut(train_df['word_count'], q=4, labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "\n",
    "# 
