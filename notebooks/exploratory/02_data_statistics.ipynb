{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis of AG News Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs comprehensive statistical analysis following methodologies from:\n",
    "- Bengio & Grandvalet (2004): \"No Unbiased Estimator of the Variance of K-Fold Cross-Validation\"\n",
    "- McNemar (1947): \"Note on the Sampling Error of the Difference Between Correlated Proportions\"\n",
    "\n",
    "### Statistical Tests Performed\n",
    "1. Descriptive statistics\n",
    "2. Distribution analysis\n",
    "3. Correlation analysis\n",
    "4. Hypothesis testing\n",
    "5. Feature importance analysis\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Date: 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency, kstest, normaltest, shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.data.preprocessing.feature_extraction import FeatureExtractor, FeatureExtractionConfig\n",
    "from configs.constants import AG_NEWS_CLASSES, DATA_DIR\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "config = AGNewsConfig(data_dir=DATA_DIR / \"processed\")\n",
    "train_dataset = AGNewsDataset(config, split=\"train\")\n",
    "val_dataset = AGNewsDataset(config, split=\"validation\")\n",
    "test_dataset = AGNewsDataset(config, split=\"test\")\n",
    "\n",
    "# Create DataFrames\n",
    "train_df = pd.DataFrame({\n",
    "    'text': train_dataset.texts,\n",
    "    'label': train_dataset.labels,\n",
    "    'label_name': train_dataset.label_names\n",
    "})\n",
    "\n",
    "# Add text statistics\n",
    "train_df['word_count'] = train_df['text'].str.split().str.len()\n",
    "train_df['char_count'] = train_df['text'].str.len()\n",
    "train_df['avg_word_length'] = train_df['char_count'] / train_df['word_count']\n",
    "train_df['sentence_count'] = train_df['text'].str.count(r'[.!?]') + 1\n",
    "\n",
    "print(f\"Dataset loaded: {len(train_df):,} training samples\")\n",
    "print(f\"Features computed: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "print(\"Overall Text Statistics\")\n",
    "print(\"=\"*50)\n",
    "print(train_df[['word_count', 'char_count', 'avg_word_length', 'sentence_count']].describe())\n",
    "\n",
    "# Per-class statistics\n",
    "print(\"\\nPer-Class Statistics\")\n",
    "print(\"=\"*50)\n",
    "class_stats = train_df.groupby('label_name')[['word_count', 'char_count']].agg([\n",
    "    'mean', 'std', 'min', 'max', 'median'\n",
    "]).round(2)\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for normality\n",
    "print(\"Normality Tests (Shapiro-Wilk)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample for Shapiro test (max 5000 samples)\n",
    "sample_size = min(5000, len(train_df))\n",
    "sample_df = train_df.sample(sample_size, random_state=42)\n",
    "\n",
    "for feature in ['word_count', 'char_count', 'avg_word_length']:\n",
    "    stat, p_value = shapiro(sample_df[feature])\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Statistic: {stat:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.4e}\")\n",
    "    print(f\"  Normal: {'No' if p_value < 0.05 else 'Yes'} (α=0.05)\")\n",
    "    print()\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "features = ['word_count', 'char_count', 'avg_word_length', 'sentence_count']\n",
    "for idx, (ax, feature) in enumerate(zip(axes.flat, features)):\n",
    "    for label_name in AG_NEWS_CLASSES:\n",
    "        data = train_df[train_df['label_name'] == label_name][feature]\n",
    "        ax.hist(data, alpha=0.5, label=label_name, bins=30)\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Distributions by Class', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA test for differences between classes\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "print(\"One-Way ANOVA: Testing for Differences Between Classes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feature in ['word_count', 'char_count', 'avg_word_length', 'sentence_count']:\n",
    "    groups = [train_df[train_df['label_name'] == label][feature] for label in AG_NEWS_CLASSES]\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    \n",
    "    print(f\"\\n{feature.replace('_', ' ').title()}:\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.4e}\")\n",
    "    print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'} (α=0.05)\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        # Post-hoc pairwise comparisons\n",
    "        from itertools import combinations\n",
    "        print(\"  Pairwise comparisons (t-test):\")\n",
    "        for class1, class2 in combinations(AG_NEWS_CLASSES, 2):\n",
    "            data1 = train_df[train_df['label_name'] == class1][feature]\n",
    "            data2 = train_df[train_df['label_name'] == class2][feature]\n",
    "            t_stat, p_val = stats.ttest_ind(data1, data2)\n",
    "            if p_val < 0.05:\n",
    "                print(f\"    {class1} vs {class2}: p={p_val:.4f} *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "numeric_features = ['word_count', 'char_count', 'avg_word_length', 'sentence_count', 'label']\n",
    "correlation_matrix = train_df[numeric_features].corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute Spearman correlation (non-parametric)\n",
    "print(\"\\nSpearman Correlation with Labels:\")\n",
    "print(\"=\"*40)\n",
    "for feature in ['word_count', 'char_count', 'avg_word_length', 'sentence_count']:\n",
    "    corr, p_value = stats.spearmanr(train_df[feature], train_df['label'])\n",
    "    print(f\"{feature}: ρ={corr:.4f}, p={p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chi-Square Test for Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize continuous features for chi-square test\n",
    "train_df['word_count_bin'] = pd.qcut(train_df['word_count'], q=4, labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = pd.crosstab(train_df['label_name'], train_df['word_count_bin'])\n",
    "\n",
    "print(\"Contingency Table: Label vs Text Length\")\n",
    "print(\"=\"*50)\n",
    "print(contingency_table)\n",
    "\n",
    "# Perform chi-square test\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nChi-Square Test Results:\")\n",
    "print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"  P-value: {p_value:.4e}\")\n",
    "print(f\"  Degrees of freedom: {dof}\")\n",
    "print(f\"  Significant association: {'Yes' if p_value < 0.05 else 'No'} (α=0.05)\")\n",
    "\n",
    "# Visualize contingency table\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(contingency_table, annot=True, fmt='d', cmap='YlOrRd')\n",
    "plt.title('Text Length Distribution Across Classes')\n",
    "plt.xlabel('Text Length Category')\n",
    "plt.ylabel('Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Extraction Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using FeatureExtractor\n",
    "feature_config = FeatureExtractionConfig(\n",
    "    use_tfidf=True,\n",
    "    use_statistical=True,\n",
    "    tfidf_max_features=1000\n",
    ")\n",
    "\n",
    "extractor = FeatureExtractor(feature_config)\n",
    "\n",
    "# Sample for efficiency\n",
    "sample_texts = train_df['text'].sample(1000, random_state=42).tolist()\n",
    "sample_labels = train_df.loc[train_df['text'].isin(sample_texts), 'label'].tolist()\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting features...\")\n",
    "features = extractor.extract_all_features(sample_texts, fit=True)\n",
    "\n",
    "# Analyze TF-IDF features\n",
    "if 'tfidf' in features:\n",
    "    tfidf_features = features['tfidf']\n",
    "    print(f\"\\nTF-IDF Feature Statistics:\")\n",
    "    print(f\"  Shape: {tfidf_features.shape}\")\n",
    "    print(f\"  Sparsity: {(tfidf_features == 0).mean():.2%}\")\n",
    "    print(f\"  Mean value: {tfidf_features[tfidf_features > 0].mean():.4f}\")\n",
    "\n",
    "# Analyze statistical features\n",
    "if 'statistical' in features:\n",
    "    stat_features = features['statistical']\n",
    "    stat_df = pd.DataFrame(stat_features, columns=[\n",
    "        'char_count', 'word_count', 'sent_count', 'avg_word_len',\n",
    "        'period_count', 'comma_count', 'exclaim_count', 'question_count',\n",
    "        'upper_count', 'capital_word_count', 'digit_count'\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nStatistical Features Summary:\")\n",
    "    print(stat_df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate effect sizes\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.stats.power as smp\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size.\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = group1.var(), group2.var()\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    return (group1.mean() - group2.mean()) / pooled_std\n",
    "\n",
    "print(\"Effect Size Analysis (Cohen's d)\")\n",
    "print(\"=\"*50)\n",
    "print(\"Interpretation: |d| < 0.2 (small), 0.2-0.8 (medium), > 0.8 (large)\\n\")\n",
    "\n",
    "# Calculate pairwise effect sizes for word count\n",
    "for i, class1 in enumerate(AG_NEWS_CLASSES):\n",
    "    for class2 in AG_NEWS_CLASSES[i+1:]:\n",
    "        group1 = train_df[train_df['label_name'] == class1]['word_count']\n",
    "        group2 = train_df[train_df['label_name'] == class2]['word_count']\n",
    "        d = cohens_d(group1, group2)\n",
    "        \n",
    "        magnitude = \"small\" if abs(d) < 0.2 else \"medium\" if abs(d) < 0.8 else \"large\"\n",
    "        print(f\"{class1} vs {class2}: d={d:.3f} ({magnitude})\")\n",
    "\n",
    "# Sample size calculation for future experiments\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample Size Requirements for Future Experiments\")\n",
    "print(\"(Power = 0.8, Alpha = 0.05)\")\n",
    "\n",
    "for effect_size in [0.2, 0.5, 0.8]:\n",
    "    n = smp.tt_ind_solve_power(effect_size=effect_size, alpha=0.05, power=0.8)\n",
    "    print(f\"  Effect size {effect_size}: n={int(np.ceil(n))} per group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Statistical Insights\n",
    "\n",
    "### Key Statistical Findings\n",
    "\n",
    "1. **Distribution Characteristics**:\n",
    "   - Text length distributions are non-normal (rejected by Shapiro-Wilk test)\n",
    "   - Significant variations exist between classes\n",
    "\n",
    "2. **Class Differences**:\n",
    "   - ANOVA reveals significant differences in text characteristics across classes\n",
    "   - Effect sizes range from small to medium\n",
    "\n",
    "3. **Feature Correlations**:\n",
    "   - Strong correlation between word count and character count (expected)\n",
    "   - Weak correlation between text features and class labels\n",
    "\n",
    "4. **Statistical Power**:\n",
    "   - Current dataset size provides adequate power for detecting medium effect sizes\n",
    "   - Sufficient for robust model training and evaluation\n",
    "\n",
    "### Recommendations for Modeling\n",
    "\n",
    "1. **Feature Engineering**: Consider normalized features due to non-normal distributions\n",
    "2. **Stratification**: Use stratified sampling to maintain class balance\n",
    "3. **Evaluation**: Apply non-parametric tests for model comparison\n",
    "4. **Sample Size**: Current size adequate for deep learning approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
