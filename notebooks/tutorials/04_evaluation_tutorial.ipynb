{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Tutorial for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates comprehensive model evaluation techniques following methodologies from:\n",
    "- Sokolova & Lapalme (2009): \"A systematic analysis of performance measures for classification tasks\"\n",
    "- Demšar (2006): \"Statistical Comparisons of Classifiers over Multiple Data Sets\"\n",
    "- Raschka (2018): \"Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Load trained models and test data\n",
    "2. Calculate comprehensive evaluation metrics\n",
    "3. Perform error analysis and visualization\n",
    "4. Conduct statistical significance testing\n",
    "5. Compare multiple model performances\n",
    "6. Generate evaluation reports\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# Data and ML imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, roc_curve, auc,\n",
    "    matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.data.loaders.dataloader import create_dataloaders\n",
    "from src.models.transformers.deberta.deberta_v3 import DeBERTaV3Classifier\n",
    "from src.evaluation.metrics.classification_metrics import ClassificationMetrics\n",
    "from src.evaluation.analysis.error_analysis import ErrorAnalyzer\n",
    "from src.evaluation.analysis.confusion_analysis import ConfusionAnalyzer\n",
    "from src.evaluation.statistical.significance_tests import SignificanceTests\n",
    "from src.evaluation.visualization.performance_plots import PerformancePlotter\n",
    "from src.utils.io_utils import safe_load, safe_save, ensure_dir\n",
    "from src.utils.logging_config import setup_logging\n",
    "from src.utils.reproducibility import set_seed\n",
    "from configs.config_loader import ConfigLoader\n",
    "from configs.constants import (\n",
    "    AG_NEWS_CLASSES,\n",
    "    AG_NEWS_NUM_CLASSES,\n",
    "    ID_TO_LABEL,\n",
    "    LABEL_TO_ID,\n",
    "    MODEL_DIR,\n",
    "    OUTPUT_DIR,\n",
    "    DATA_DIR\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "set_seed(42)\n",
    "logger = setup_logging('evaluation_tutorial')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Model Evaluation Tutorial\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation configuration\n",
    "config_loader = ConfigLoader()\n",
    "\n",
    "# Load model and evaluation configs\n",
    "model_config = config_loader.load_config('models/single/deberta_v3_xlarge.yaml')\n",
    "eval_config = {\n",
    "    'batch_size': 16,\n",
    "    'max_samples': 1000,  # Limit for tutorial\n",
    "    'model_name': 'microsoft/deberta-v3-base',\n",
    "    'max_length': 256,\n",
    "    'metrics': ['accuracy', 'precision', 'recall', 'f1', 'auc', 'mcc', 'kappa'],\n",
    "    'confidence_level': 0.95,\n",
    "    'num_bootstrap': 1000\n",
    "}\n",
    "\n",
    "print(\"Evaluation Configuration:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in eval_config.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"  {key}: {', '.join(value)}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(eval_config['model_name'])\n",
    "\n",
    "# Load test dataset\n",
    "data_config = AGNewsConfig(\n",
    "    data_dir=DATA_DIR / \"processed\",\n",
    "    max_samples=eval_config['max_samples'],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=eval_config['max_length']\n",
    ")\n",
    "\n",
    "print(\"Loading test dataset...\")\n",
    "test_dataset = AGNewsDataset(data_config, split='test')\n",
    "\n",
    "# Create DataLoader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=eval_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"\\nTest dataset loaded:\")\n",
    "print(f\"  Total samples: {len(test_dataset)}\")\n",
    "print(f\"  Number of batches: {len(test_dataloader)}\")\n",
    "print(f\"  Classes: {AG_NEWS_CLASSES}\")\n",
    "\n",
    "# Display sample distribution\n",
    "label_counts = pd.Series(test_dataset.labels).value_counts().sort_index()\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label_id, count in label_counts.items():\n",
    "    label_name = ID_TO_LABEL[label_id]\n",
    "    print(f\"  {label_name}: {count} ({count/len(test_dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "def load_trained_model(checkpoint_path: Optional[Path] = None) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load trained model for evaluation.\n",
    "    \n",
    "    Following model loading best practices from:\n",
    "        PyTorch Documentation: \"Saving and Loading Models\"\n",
    "    \"\"\"\n",
    "    # Initialize model architecture\n",
    "    model = DeBERTaV3Classifier(\n",
    "        model_name=eval_config['model_name'],\n",
    "        num_labels=AG_NEWS_NUM_CLASSES,\n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint if provided\n",
    "    if checkpoint_path and checkpoint_path.exists():\n",
    "        print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"  Loaded model from epoch: {checkpoint.get('epoch', 'unknown')}\")\n",
    "            print(f\"  Best accuracy: {checkpoint.get('best_accuracy', 'unknown')}\")\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "    else:\n",
    "        print(\"Using randomly initialized model for demonstration\")\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Load model\n",
    "checkpoint_path = MODEL_DIR / \"tutorial\" / \"deberta_v3_trained\" / \"checkpoint.pt\"\n",
    "model = load_trained_model(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel loaded:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Model in eval mode: {not model.training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model: nn.Module,\n",
    "                        dataloader: DataLoader,\n",
    "                        device: torch.device) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate predictions on test set.\n",
    "    \n",
    "    Following inference best practices from:\n",
    "        Krishnan et al. (2022): \"Efficient Deep Learning Inference\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating predictions\"):\n",
    "            # Move batch to device\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "            \n",
    "            # Calculate probabilities and predictions\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    return (np.array(all_predictions),\n",
    "            np.array(all_labels),\n",
    "            np.array(all_probabilities))\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions on test set...\")\n",
    "predictions, true_labels, probabilities = generate_predictions(model, test_dataloader, device)\n",
    "\n",
    "print(f\"\\nPredictions generated:\")\n",
    "print(f\"  Predictions shape: {predictions.shape}\")\n",
    "print(f\"  True labels shape: {true_labels.shape}\")\n",
    "print(f\"  Probabilities shape: {probabilities.shape}\")\n",
    "\n",
    "# Quick accuracy check\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"\\nQuick metrics:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Comprehensive Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics calculator\n",
    "metrics_calculator = ClassificationMetrics(num_classes=AG_NEWS_NUM_CLASSES)\n",
    "\n",
    "# Calculate all metrics\n",
    "metrics = metrics_calculator.compute_metrics(\n",
    "    predictions=predictions,\n",
    "    labels=true_labels,\n",
    "    probabilities=probabilities\n",
    ")\n",
    "\n",
    "print(\"Comprehensive Evaluation Metrics:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall metrics\n",
    "print(\"\\nOverall Performance:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "print(f\"  Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
    "print(f\"  Macro Precision: {metrics['macro_precision']:.4f}\")\n",
    "print(f\"  Macro Recall: {metrics['macro_recall']:.4f}\")\n",
    "\n",
    "# Additional metrics\n",
    "mcc = matthews_corrcoef(true_labels, predictions)\n",
    "kappa = cohen_kappa_score(true_labels, predictions)\n",
    "\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"  Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "print(f\"  Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    true_labels, predictions, average=None\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Support':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i in range(AG_NEWS_NUM_CLASSES):\n",
    "    class_name = ID_TO_LABEL[i]\n",
    "    print(f\"{class_name:<15} {precision[i]:<10.4f} {recall[i]:<10.4f} \"\n",
    "          f\"{f1[i]:<10.4f} {support[i]:<10}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    true_labels,\n",
    "    predictions,\n",
    "    target_names=AG_NEWS_CLASSES,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Normalize confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=AG_NEWS_CLASSES,\n",
    "            yticklabels=AG_NEWS_CLASSES,\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=AG_NEWS_CLASSES,\n",
    "            yticklabels=AG_NEWS_CLASSES,\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('Normalized Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "\n",
    "plt.suptitle('Confusion Matrix Analysis', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze confusion patterns\n",
    "print(\"Confusion Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find most confused pairs\n",
    "confusion_pairs = []\n",
    "for i in range(AG_NEWS_NUM_CLASSES):\n",
    "    for j in range(AG_NEWS_NUM_CLASSES):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusion_pairs.append({\n",
    "                'true': ID_TO_LABEL[i],\n",
    "                'predicted': ID_TO_LABEL[j],\n",
    "                'count': cm[i, j],\n",
    "                'percentage': cm_normalized[i, j] * 100\n",
    "            })\n",
    "\n",
    "# Sort by count\n",
    "confusion_pairs.sort(key=lambda x: x['count'], reverse=True)\n",
    "\n",
    "print(\"\\nTop 5 Confusion Pairs:\")\n",
    "for pair in confusion_pairs[:5]:\n",
    "    print(f\"  {pair['true']} -> {pair['predicted']}: \"\n",
    "          f\"{pair['count']} ({pair['percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(predictions: np.ndarray,\n",
    "                  true_labels: np.ndarray,\n",
    "                  probabilities: np.ndarray,\n",
    "                  texts: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform detailed error analysis.\n",
    "    \n",
    "    Following error analysis methodology from:\n",
    "        Wu et al. (2020): \"Errudite: Scalable, Reproducible, and Testable Error Analysis\"\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != true_labels[i]:\n",
    "            # Calculate confidence\n",
    "            confidence = probabilities[i, predictions[i]]\n",
    "            true_prob = probabilities[i, true_labels[i]]\n",
    "            \n",
    "            errors.append({\n",
    "                'index': i,\n",
    "                'text': texts[i][:100] + '...' if len(texts[i]) > 100 else texts[i],\n",
    "                'true_label': ID_TO_LABEL[true_labels[i]],\n",
    "                'predicted_label': ID_TO_LABEL[predictions[i]],\n",
    "                'confidence': confidence,\n",
    "                'true_prob': true_prob,\n",
    "                'confidence_gap': confidence - true_prob,\n",
    "                'text_length': len(texts[i].split())\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(errors)\n",
    "\n",
    "# Perform error analysis\n",
    "error_df = analyze_errors(\n",
    "    predictions,\n",
    "    true_labels,\n",
    "    probabilities,\n",
    "    test_dataset.texts\n",
    ")\n",
    "\n",
    "print(\"Error Analysis Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total errors: {len(error_df)} ({len(error_df)/len(predictions)*100:.1f}%)\")\n",
    "\n",
    "if len(error_df) > 0:\n",
    "    # Error distribution by class\n",
    "    print(\"\\nErrors by True Class:\")\n",
    "    error_by_class = error_df['true_label'].value_counts()\n",
    "    for label, count in error_by_class.items():\n",
    "        total = sum(true_labels == LABEL_TO_ID[label])\n",
    "        print(f\"  {label}: {count}/{total} ({count/total*100:.1f}%)\")\n",
    "    \n",
    "    # High confidence errors\n",
    "    high_conf_errors = error_df[error_df['confidence'] > 0.8]\n",
    "    print(f\"\\nHigh confidence errors (>0.8): {len(high_conf_errors)}\")\n",
    "    \n",
    "    if len(high_conf_errors) > 0:\n",
    "        print(\"\\nExample high confidence errors:\")\n",
    "        for _, row in high_conf_errors.head(3).iterrows():\n",
    "            print(f\"\\n  Text: {row['text']}\")\n",
    "            print(f\"  True: {row['true_label']}, Predicted: {row['predicted_label']}\")\n",
    "            print(f\"  Confidence: {row['confidence']:.3f}\")\n",
    "    \n",
    "    # Error patterns by text length\n",
    "    print(\"\\nError rate by text length:\")\n",
    "    error_df['length_bin'] = pd.qcut(error_df['text_length'], q=3,\n",
    "                                     labels=['Short', 'Medium', 'Long'])\n",
    "    for length_bin in ['Short', 'Medium', 'Long']:\n",
    "        count = len(error_df[error_df['length_bin'] == length_bin])\n",
    "        print(f\"  {length_bin}: {count} errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(y_true: np.ndarray,\n",
    "                                 y_pred: np.ndarray,\n",
    "                                 metric_func: callable,\n",
    "                                 n_bootstrap: int = 1000,\n",
    "                                 confidence_level: float = 0.95) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence intervals.\n",
    "    \n",
    "    Following bootstrap methodology from:\n",
    "        Efron & Tibshirani (1993): \"An Introduction to the Bootstrap\"\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        score = metric_func(y_true[indices], y_pred[indices])\n",
    "        scores.append(score)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    alpha = 1 - confidence_level\n",
    "    \n",
    "    lower = np.percentile(scores, (alpha/2) * 100)\n",
    "    upper = np.percentile(scores, (1 - alpha/2) * 100)\n",
    "    mean = np.mean(scores)\n",
    "    \n",
    "    return mean, lower, upper\n",
    "\n",
    "# Calculate confidence intervals for key metrics\n",
    "print(\"Bootstrap Confidence Intervals:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of bootstrap samples: {eval_config['num_bootstrap']}\")\n",
    "print(f\"Confidence level: {eval_config['confidence_level']*100}%\")\n",
    "print()\n",
    "\n",
    "metrics_to_test = [\n",
    "    ('Accuracy', accuracy_score),\n",
    "    ('Macro F1', lambda y_t, y_p: precision_recall_fscore_support(y_t, y_p, average='macro')[2]),\n",
    "    ('MCC', matthews_corrcoef),\n",
    "    ('Cohen Kappa', cohen_kappa_score)\n",
    "]\n",
    "\n",
    "confidence_intervals = {}\n",
    "for metric_name, metric_func in metrics_to_test:\n",
    "    mean, lower, upper = bootstrap_confidence_interval(\n",
    "        true_labels,\n",
    "        predictions,\n",
    "        metric_func,\n",
    "        n_bootstrap=eval_config['num_bootstrap'],\n",
    "        confidence_level=eval_config['confidence_level']\n",
    "    )\n",
    "    \n",
    "    confidence_intervals[metric_name] = (mean, lower, upper)\n",
    "    print(f\"{metric_name:12}: {mean:.4f} [{lower:.4f}, {upper:.4f}]\")\n",
    "\n",
    "# McNemar's test (for comparing two models)\n",
    "def mcnemar_test(y_true: np.ndarray,\n",
    "                 pred1: np.ndarray,\n",
    "                 pred2: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Perform McNemar's test.\n",
    "    \n",
    "    Following methodology from:\n",
    "        Dietterich (1998): \"Approximate Statistical Tests for Comparing Supervised Classification\"\n",
    "    \"\"\"\n",
    "    # Create contingency table\n",
    "    correct1 = (pred1 == y_true)\n",
    "    correct2 = (pred2 == y_true)\n",
    "    \n",
    "    n00 = np.sum(~correct1 & ~correct2)  # Both wrong\n",
    "    n01 = np.sum(~correct1 & correct2)   # 1 wrong, 2 correct\n",
    "    n10 = np.sum(correct1 & ~correct2)   # 1 correct, 2 wrong\n",
    "    n11 = np.sum(correct1 & correct2)    # Both correct\n",
    "    \n",
    "    # McNemar's statistic\n",
    "    if n01 + n10 == 0:\n",
    "        return 0.0, 1.0\n",
    "    \n",
    "    statistic = (abs(n01 - n10) - 1) ** 2 / (n01 + n10)\n",
    "    p_value = 1 - stats.chi2.cdf(statistic, df=1)\n",
    "    \n",
    "    return statistic, p_value\n",
    "\n",
    "# Simulate second model predictions for demonstration\n",
    "print(\"\\nStatistical Model Comparison (Simulated):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Add small random noise to create second model\n",
    "pred2 = predictions.copy()\n",
    "n_changes = int(len(pred2) * 0.05)  # Change 5% of predictions\n",
    "change_indices = np.random.choice(len(pred2), n_changes, replace=False)\n",
    "pred2[change_indices] = np.random.randint(0, AG_NEWS_NUM_CLASSES, n_changes)\n",
    "\n",
    "statistic, p_value = mcnemar_test(true_labels, predictions, pred2)\n",
    "print(f\"McNemar's test statistic: {statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'} (α=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Per-class F1 scores\n",
    "ax = axes[0, 0]\n",
    "ax.bar(AG_NEWS_CLASSES, f1)\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score by Class')\n",
    "ax.set_ylim([0, 1])\n",
    "for i, v in enumerate(f1):\n",
    "    ax.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# 2. Precision-Recall comparison\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(AG_NEWS_CLASSES))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, precision, width, label='Precision')\n",
    "ax.bar(x + width/2, recall, width, label='Recall')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Precision vs Recall')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(AG_NEWS_CLASSES)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# 3. ROC curves\n",
    "ax = axes[0, 2]\n",
    "for i in range(AG_NEWS_NUM_CLASSES):\n",
    "    y_true_binary = (true_labels == i).astype(int)\n",
    "    y_score = probabilities[:, i]\n",
    "    fpr, tpr, _ = roc_curve(y_true_binary, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, label=f'{ID_TO_LABEL[i]} (AUC={roc_auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confidence distribution\n",
    "ax = axes[1, 0]\n",
    "max_probs = np.max(probabilities, axis=1)\n",
    "ax.hist(max_probs, bins=30, edgecolor='black')\n",
    "ax.set_xlabel('Prediction Confidence')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Prediction Confidence Distribution')\n",
    "ax.axvline(x=0.5, color='r', linestyle='--', label='Threshold')\n",
    "ax.legend()\n",
    "\n",
    "# 5. Error rate by confidence\n",
    "ax = axes[1, 1]\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "confidence_errors = []\n",
    "confidence_counts = []\n",
    "\n",
    "for i in range(len(confidence_bins) - 1):\n",
    "    mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        error_rate = (predictions[mask] != true_labels[mask]).mean()\n",
    "        confidence_errors.append(error_rate)\n",
    "        confidence_counts.append(mask.sum())\n",
    "    else:\n",
    "        confidence_errors.append(0)\n",
    "        confidence_counts.append(0)\n",
    "\n",
    "ax.bar(confidence_bins[:-1], confidence_errors, width=0.08)\n",
    "ax.set_xlabel('Confidence Range')\n",
    "ax.set_ylabel('Error Rate')\n",
    "ax.set_title('Error Rate by Confidence')\n",
    "ax.set_xlim([0, 1])\n",
    "\n",
    "# 6. Sample distribution\n",
    "ax = axes[1, 2]\n",
    "ax.pie(support, labels=AG_NEWS_CLASSES, autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title('Test Set Class Distribution')\n",
    "\n",
    "plt.suptitle('Model Performance Analysis', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance visualizations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report(metrics: Dict[str, Any],\n",
    "                              confidence_intervals: Dict[str, Tuple[float, float, float]],\n",
    "                              error_analysis: pd.DataFrame,\n",
    "                              output_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Generate comprehensive evaluation report.\n",
    "    \n",
    "    Following reporting standards from:\n",
    "        Moreira et al. (2018): \"Standardized Evaluation of Machine Learning Methods\"\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'metadata': {\n",
    "            'model': eval_config['model_name'],\n",
    "            'dataset': 'AG News',\n",
    "            'num_classes': AG_NEWS_NUM_CLASSES,\n",
    "            'test_samples': len(true_labels),\n",
    "            'evaluation_date': pd.Timestamp.now().isoformat()\n",
    "        },\n",
    "        'overall_metrics': {\n",
    "            'accuracy': float(metrics['accuracy']),\n",
    "            'macro_f1': float(metrics['macro_f1']),\n",
    "            'weighted_f1': float(metrics['weighted_f1']),\n",
    "            'macro_precision': float(metrics['macro_precision']),\n",
    "            'macro_recall': float(metrics['macro_recall'])\n",
    "        },\n",
    "        'confidence_intervals': {\n",
    "            metric: {\n",
    "                'mean': float(values[0]),\n",
    "                'lower': float(values[1]),\n",
    "                'upper': float(values[2])\n",
    "            }\n",
    "            for metric, values in confidence_intervals.items()\n",
    "        },\n",
    "        'per_class_metrics': [\n",
    "            {\n",
    "                'class': ID_TO_LABEL[i],\n",
    "                'precision': float(precision[i]),\n",
    "                'recall': float(recall[i]),\n",
    "                'f1': float(f1[i]),\n",
    "                'support': int(support[i])\n",
    "            }\n",
    "            for i in range(AG_NEWS_NUM_CLASSES)\n",
    "        ],\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'error_summary': {\n",
    "            'total_errors': len(error_analysis),\n",
    "            'error_rate': len(error_analysis) / len(true_labels),\n",
    "            'high_confidence_errors': len(error_analysis[error_analysis['confidence'] > 0.8])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    ensure_dir(output_path.parent)\n",
    "    safe_save(report, output_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate and save report\n",
    "report_path = OUTPUT_DIR / \"tutorial\" / \"evaluation\" / \"evaluation_report.json\"\n",
    "saved_report = generate_evaluation_report(\n",
    "    metrics=metrics,\n",
    "    confidence_intervals=confidence_intervals,\n",
    "    error_analysis=error_df,\n",
    "    output_path=report_path\n",
    ")\n",
    "\n",
    "print(\"Evaluation Report Generated:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Report saved to: {saved_report}\")\n",
    "print(f\"File size: {saved_report.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nReport Summary:\")\n",
    "report_data = safe_load(saved_report)\n",
    "print(f\"  Model: {report_data['metadata']['model']}\")\n",
    "print(f\"  Test samples: {report_data['metadata']['test_samples']}\")\n",
    "print(f\"  Accuracy: {report_data['overall_metrics']['accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {report_data['overall_metrics']['macro_f1']:.4f}\")\n",
    "print(f\"  Error rate: {report_data['error_summary']['error_rate']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Next Steps\n",
    "\n",
    "### Evaluation Summary\n",
    "\n",
    "This tutorial demonstrated comprehensive model evaluation techniques:\n",
    "\n",
    "1. **Test Data Loading**: Prepared test dataset with proper preprocessing\n",
    "2. **Model Loading**: Loaded trained DeBERTa-v3 model for evaluation\n",
    "3. **Prediction Generation**: Generated predictions with confidence scores\n",
    "4. **Metrics Calculation**: Computed comprehensive classification metrics\n",
    "5. **Confusion Analysis**: Analyzed confusion patterns between classes\n",
    "6. **Error Analysis**: Identified and characterized prediction errors\n",
    "7. **Statistical Testing**: Performed bootstrap confidence intervals and significance tests\n",
    "8. **Visualization**: Created comprehensive performance visualizations\n",
    "9. **Report Generation**: Generated structured evaluation report\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Multiple Metrics**: Use various metrics to understand different aspects of performance\n",
    "2. **Confidence Intervals**: Bootstrap provides robust confidence estimates\n",
    "3. **Error Patterns**: Analyzing errors reveals model weaknesses\n",
    "4. **Statistical Significance**: Proper testing ensures reliable comparisons\n",
    "5. **Visualization Importance**: Visual analysis complements numerical metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Advanced Evaluation**:\n",
    "   - Implement cross-validation evaluation\n",
    "   - Perform ablation studies\n",
    "   - Conduct robustness testing\n",
    "\n",
    "2. **Model Comparison**:\n",
    "   - Compare multiple model architectures\n",
    "   - Evaluate ensemble methods\n",
    "   - Benchmark against baselines\n",
    "\n",
    "3. **Error Mitigation**:\n",
    "   - Implement targeted data augmentation\n",
    "   - Apply class-specific optimization\n",
    "   - Use ensemble to reduce errors\n",
    "\n",
    "4. **Production Monitoring**:\n",
    "   - Set up continuous evaluation pipeline\n",
    "   - Monitor model drift\n",
    "   - Implement A/B testing framework\n",
    "\n",
    "### References\n",
    "\n",
    "For deeper understanding, consult:\n",
    "- Evaluation documentation: `docs/user_guide/evaluation.md`\n",
    "- Advanced metrics: `src/evaluation/metrics/`\n",
    "- Statistical testing: `notebooks/experiments/statistical_analysis.ipynb`\n",
    "- Visualization tools: `src/evaluation/visualization/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
