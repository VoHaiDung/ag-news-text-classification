{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Tutorial for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers comprehensive model evaluation techniques following methodologies from:\n",
    "- Powers (2011): \"Evaluation: From Precision, Recall and F-measure to ROC, Informedness, Markedness & Correlation\"\n",
    "- Ribeiro et al. (2020): \"Beyond Accuracy: Behavioral Testing of NLP Models with CheckList\"\n",
    "- Card et al. (2020): \"With Little Power Comes Great Responsibility\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Calculate classification metrics\n",
    "2. Perform error analysis\n",
    "3. Generate confusion matrices\n",
    "4. Conduct statistical significance testing\n",
    "5. Analyze model interpretability\n",
    "6. Create comprehensive evaluation reports\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "\n",
    "# Data and ML imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    cohen_kappa_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.evaluation.metrics.classification_metrics import ClassificationMetrics\n",
    "from src.evaluation.analysis.error_analysis import ErrorAnalyzer\n",
    "from src.evaluation.analysis.confusion_analysis import ConfusionAnalyzer\n",
    "from src.evaluation.statistical.significance_tests import SignificanceTests\n",
    "from src.evaluation.visualization.performance_plots import PerformancePlotter\n",
    "from src.utils.io_utils import safe_load, safe_save, ensure_dir\n",
    "from configs.constants import (\n",
    "    AG_NEWS_CLASSES,\n",
    "    AG_NEWS_NUM_CLASSES,\n",
    "    ID_TO_LABEL,\n",
    "    MODEL_DIR\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Model Evaluation Tutorial\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate model predictions (replace with actual model outputs)\n",
    "def generate_sample_predictions(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate sample predictions for demonstration.\n",
    "    In practice, load actual model predictions.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate realistic predictions with some errors\n",
    "    true_labels = np.random.randint(0, AG_NEWS_NUM_CLASSES, n_samples)\n",
    "    \n",
    "    # Add noise to create predictions (90% accuracy)\n",
    "    predictions = true_labels.copy()\n",
    "    error_indices = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)\n",
    "    predictions[error_indices] = np.random.randint(0, AG_NEWS_NUM_CLASSES, len(error_indices))\n",
    "    \n",
    "    # Generate probability scores\n",
    "    probabilities = np.zeros((n_samples, AG_NEWS_NUM_CLASSES))\n",
    "    for i in range(n_samples):\n",
    "        probs = np.random.dirichlet(np.ones(AG_NEWS_NUM_CLASSES))\n",
    "        probs[predictions[i]] = max(probs) + 0.3\n",
    "        probabilities[i] = probs / probs.sum()\n",
    "    \n",
    "    return true_labels, predictions, probabilities\n",
    "\n",
    "# Generate or load predictions\n",
    "print(\"Loading model predictions...\")\n",
    "y_true, y_pred, y_proba = generate_sample_predictions(1000)\n",
    "\n",
    "print(f\"Evaluation set size: {len(y_true)}\")\n",
    "print(f\"Number of classes: {AG_NEWS_NUM_CLASSES}\")\n",
    "print(f\"Class distribution: {np.bincount(y_true)}\")\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "results_df = pd.DataFrame({\n",
    "    'true_label': y_true,\n",
    "    'predicted_label': y_pred,\n",
    "    'true_class': [ID_TO_LABEL[i] for i in y_true],\n",
    "    'predicted_class': [ID_TO_LABEL[i] for i in y_pred],\n",
    "    'correct': y_true == y_pred,\n",
    "    'confidence': y_proba.max(axis=1)\n",
    "})\n",
    "\n",
    "# Add probability columns\n",
    "for i, class_name in enumerate(AG_NEWS_CLASSES):\n",
    "    results_df[f'prob_{class_name}'] = y_proba[:, i]\n",
    "\n",
    "print(\"\\nPrediction sample:\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "metrics_calculator = ClassificationMetrics()\n",
    "\n",
    "print(\"Classification Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Overall Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "print(f\"  Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=None, labels=range(AG_NEWS_NUM_CLASSES)\n",
    ")\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "class_metrics_df = pd.DataFrame({\n",
    "    'Class': AG_NEWS_CLASSES,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "print(class_metrics_df.to_string(index=False))\n",
    "\n",
    "# Macro and weighted averages\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average='macro'\n",
    ")\n",
    "weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average='weighted'\n",
    ")\n",
    
