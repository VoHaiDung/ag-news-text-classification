{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Tutorial for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates comprehensive text preprocessing techniques following methodologies from:\n",
    "- Pennington et al. (2014): \"GloVe: Global Vectors for Word Representation\"\n",
    "- Bojanowski et al. (2017): \"Enriching Word Vectors with Subword Information\"\n",
    "- Kudo & Richardson (2018): \"SentencePiece: A simple and language independent subword tokenizer\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Implement text cleaning and normalization\n",
    "2. Apply tokenization for transformer models\n",
    "3. Prepare data compatible with model training\n",
    "4. Extract features for analysis\n",
    "5. Optimize preprocessing pipeline\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# Data manipulation and NLP imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.data.preprocessing.text_cleaner import TextCleaner, CleaningConfig\n",
    "from src.data.preprocessing.tokenization import Tokenizer, TokenizationConfig\n",
    "from src.data.preprocessing.feature_extraction import FeatureExtractor, FeatureExtractionConfig\n",
    "from src.utils.io_utils import safe_load, safe_save, ensure_dir\n",
    "from src.utils.logging_config import setup_logging\n",
    "from src.utils.reproducibility import set_seed\n",
    "from configs.constants import (\n",
    "    AG_NEWS_CLASSES,\n",
    "    AG_NEWS_NUM_CLASSES,\n",
    "    DATA_DIR,\n",
    "    OUTPUT_DIR\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "set_seed(42)\n",
    "logger = setup_logging('preprocessing_tutorial')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "print(\"Text Preprocessing Tutorial\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News dataset (matching model training configuration)\n",
    "config = AGNewsConfig(\n",
    "    data_dir=DATA_DIR / \"processed\",\n",
    "    max_samples=1000  # Same as model training tutorial\n",
    ")\n",
    "\n",
    "dataset = AGNewsDataset(config, split=\"train\")\n",
    "\n",
    "# Extract sample texts\n",
    "sample_texts = dataset.texts[:10]\n",
    "sample_labels = dataset.labels[:10]\n",
    "\n",
    "print(\"Sample Data Loaded:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Sample texts: {len(sample_texts)}\")\n",
    "\n",
    "# Display first sample\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Label: {sample_labels[0]} ({dataset.label_names[0]})\")\n",
    "print(f\"  Text: {sample_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing for DeBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer (same as in model training)\n",
    "model_name = 'microsoft/deberta-v3-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Tokenizer Configuration:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Max length: 256\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Tokenize sample text\n",
    "sample_text = sample_texts[0]\n",
    "encoding = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenization Example:\")\n",
    "print(f\"  Original text length: {len(sample_text)} chars\")\n",
    "print(f\"  Input IDs shape: {encoding['input_ids'].shape}\")\n",
    "print(f\"  Attention mask shape: {encoding['attention_mask'].shape}\")\n",
    "\n",
    "# Decode to verify\n",
    "decoded = tokenizer.decode(encoding['input_ids'][0], skip_special_tokens=True)\n",
    "print(f\"  Decoded text: {decoded[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_training(\n",
    "    texts: List[str],\n",
    "    labels: List[int],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    max_length: int = 256\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Preprocess texts for model training.\n",
    "    \n",
    "    This function matches the preprocessing used in model training.\n",
    "    \"\"\"\n",
    "    # Tokenize all texts\n",
    "    encoding = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    encoding['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "# Preprocess batch of samples\n",
    "batch_size = 8  # Same as model training\n",
    "batch_texts = dataset.texts[:batch_size]\n",
    "batch_labels = dataset.labels[:batch_size]\n",
    "\n",
    "batch_encoding = preprocess_for_training(\n",
    "    batch_texts,\n",
    "    batch_labels,\n",
    "    tokenizer,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "print(\"Batch Preprocessing Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Input IDs shape: {batch_encoding['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {batch_encoding['attention_mask'].shape}\")\n",
    "print(f\"Labels shape: {batch_encoding['labels'].shape}\")\n",
    "\n",
    "# Check padding statistics\n",
    "padding_tokens = (batch_encoding['input_ids'] == tokenizer.pad_token_id).sum()\n",
    "total_tokens = batch_encoding['input_ids'].numel()\n",
    "print(f\"\\nPadding Statistics:\")\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "print(f\"  Padding tokens: {padding_tokens}\")\n",
    "print(f\"  Padding ratio: {padding_tokens/total_tokens:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Cleaning Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different cleaning strategies for preprocessing\n",
    "from src.data.preprocessing.text_cleaner import get_minimal_cleaner, get_aggressive_cleaner\n",
    "\n",
    "# Minimal cleaning (recommended for transformers)\n",
    "minimal_cleaner = get_minimal_cleaner()\n",
    "\n",
    "# Aggressive cleaning (optional)\n",
    "aggressive_cleaner = get_aggressive_cleaner()\n",
    "\n",
    "print(\"Text Cleaning Comparison:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_text = \"Check out this AMAZING deal at https://example.com! Only $99.99 #BestDeal\"\n",
    "\n",
    "print(f\"Original text:\")\n",
    "print(f\"  {test_text}\")\n",
    "\n",
    "minimal_cleaned = minimal_cleaner.clean(test_text)\n",
    "print(f\"\\nMinimal cleaning (for transformers):\")\n",
    "print(f\"  {minimal_cleaned}\")\n",
    "\n",
    "aggressive_cleaned = aggressive_cleaner.clean(test_text)\n",
    "print(f\"\\nAggressive cleaning (optional):\")\n",
    "print(f\"  {aggressive_cleaned}\")\n",
    "\n",
    "# Apply to AG News sample\n",
    "sample_cleaned = minimal_cleaner.clean(sample_texts[0])\n",
    "print(f\"\\nAG News sample (minimal cleaning):\")\n",
    "print(f\"  Before: {sample_texts[0][:100]}...\")\n",
    "print(f\"  After:  {sample_cleaned[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for data analysis\n",
    "feature_config = FeatureExtractionConfig(\n",
    "    extract_length_features=True,\n",
    "    extract_readability_features=True,\n",
    "    extract_pos_features=False  # Skip for speed\n",
    ")\n",
    "\n",
    "feature_extractor = FeatureExtractor(feature_config)\n",
    "\n",
    "# Extract features from samples\n",
    "features_list = []\n",
    "for text in sample_texts[:5]:\n",
    "    features = feature_extractor.extract(text)\n",
    "    features_list.append(features)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "features_df = pd.DataFrame(features_list)\n",
    "\n",
    "print(\"Extracted Features Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(features_df.describe().round(2))\n",
    "\n",
    "# Feature correlation with labels\n",
    "print(\"\\nFeature Statistics by Class:\")\n",
    "for i in range(min(5, len(sample_labels))):\n",
    "    label_name = dataset.label_names[i]\n",
    "    print(f\"\\n{label_name}:\")\n",
    "    print(f\"  Word count: {features_list[i]['word_count']}\")\n",
    "    print(f\"  Char count: {features_list[i]['char_count']}\")\n",
    "    print(f\"  Avg word length: {features_list[i].get('avg_word_length', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DataLoader Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class PreprocessedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset with integrated preprocessing for model training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[int], \n",
    "                 tokenizer: AutoTokenizer, max_length: int = 256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "preprocessed_dataset = PreprocessedDataset(\n",
    "    texts=dataset.texts[:100],\n",
    "    labels=dataset.labels[:100],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    preprocessed_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Test DataLoader\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "print(\"DataLoader Integration Test:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset size: {len(preprocessed_dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  Input IDs: {batch['input_ids'].shape}\")\n",
    "print(f\"  Attention mask: {batch['attention_mask'].shape}\")\n",
    "print(f\"  Labels: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preprocessing Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_preprocessing(texts: List[str], tokenizer: AutoTokenizer) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Benchmark preprocessing performance.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Single text processing\n",
    "    start = time.time()\n",
    "    for text in texts:\n",
    "        _ = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    results['sequential'] = time.time() - start\n",
    "    \n",
    "    # Batch processing\n",
    "    start = time.time()\n",
    "    _ = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    results['batch'] = time.time() - start\n",
    "    \n",
    "    results['speedup'] = results['sequential'] / results['batch']\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark with different batch sizes\n",
    "test_sizes = [10, 50, 100]\n",
    "\n",
    "print(\"Preprocessing Performance Benchmark:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for size in test_sizes:\n",
    "    test_texts = dataset.texts[:size]\n",
    "    results = benchmark_preprocessing(test_texts, tokenizer)\n",
    "    \n",
    "    print(f\"\\nBatch size: {size}\")\n",
    "    print(f\"  Sequential: {results['sequential']:.3f} seconds\")\n",
    "    print(f\"  Batch: {results['batch']:.3f} seconds\")\n",
    "    print(f\"  Speedup: {results['speedup']:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and save data for model training\n",
    "def save_preprocessed_dataset(\n",
    "    texts: List[str],\n",
    "    labels: List[int],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    output_path: Path,\n",
    "    max_length: int = 256\n",
    "):\n",
    "    \"\"\"\n",
    "    Save preprocessed data compatible with model training.\n",
    "    \"\"\"\n",
    "    ensure_dir(output_path.parent)\n",
    "    \n",
    "    # Preprocess all texts\n",
    "    print(\"Preprocessing texts...\")\n",
    "    encoding = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create dataset dictionary\n",
    "    dataset_dict = {\n",
    "        'input_ids': encoding['input_ids'],\n",
    "        'attention_mask': encoding['attention_mask'],\n",
    "        'labels': torch.tensor(labels, dtype=torch.long),\n",
    "        'metadata': {\n",
    "            'num_samples': len(texts),\n",
    "            'max_length': max_length,\n",
    "            'tokenizer': model_name,\n",
    "            'preprocessing_date': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to disk\n",
    "    torch.save(dataset_dict, output_path)\n",
    "    \n",
    "    print(f\"\\nPreprocessed data saved to: {output_path}\")\n",
    "    print(f\"  File size: {output_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Save preprocessed data\n",
    "output_file = OUTPUT_DIR / \"tutorial\" / \"preprocessed_agnews.pt\"\n",
    "saved_path = save_preprocessed_dataset(\n",
    "    texts=dataset.texts[:500],\n",
    "    labels=dataset.labels[:500],\n",
    "    tokenizer=tokenizer,\n",
    "    output_path=output_file,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# Verify saved data\n",
    "loaded_data = torch.load(saved_path)\n",
    "print(\"\\nVerification of saved data:\")\n",
    "print(f\"  Input IDs shape: {loaded_data['input_ids'].shape}\")\n",
    "print(f\"  Attention mask shape: {loaded_data['attention_mask'].shape}\")\n",
    "print(f\"  Labels shape: {loaded_data['labels'].shape}\")\n",
    "print(f\"  Metadata: {loaded_data['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Preprocessing for Transformers**:\n",
    "   - Use tokenizer from target model (DeBERTa-v3)\n",
    "   - Maintain consistent max_length (256 tokens)\n",
    "   - Apply minimal text cleaning to preserve information\n",
    "\n",
    "2. **Batch Processing**:\n",
    "   - Batch tokenization is significantly faster\n",
    "   - Use same batch size as training (8)\n",
    "   - Enable padding and truncation\n",
    "\n",
    "3. **Data Compatibility**:\n",
    "   - Ensure preprocessing matches model training setup\n",
    "   - Save preprocessed data for reproducibility\n",
    "   - Include metadata for tracking\n",
    "\n",
    "4. **Performance Optimization**:\n",
    "   - Use DataLoader with multiple workers\n",
    "   - Enable pin_memory for GPU training\n",
    "   - Cache preprocessed data when possible\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always use the same tokenizer** for preprocessing and model training\n",
    "2. **Maintain consistent parameters** (max_length, padding strategy)\n",
    "3. **Validate preprocessed data** before training\n",
    "4. **Monitor preprocessing time** for optimization\n",
    "5. **Save preprocessed versions** for reproducibility\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Continue to: `03_model_training_basics.ipynb` for model training\n",
    "- Explore: `04_evaluation_tutorial.ipynb` for model evaluation\n",
    "- Review: Documentation at `docs/user_guide/data_preparation.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
