{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Tutorial for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates comprehensive text preprocessing techniques following methodologies from:\n",
    "- Pennington et al. (2014): \"GloVe: Global Vectors for Word Representation\"\n",
    "- Bojanowski et al. (2017): \"Enriching Word Vectors with Subword Information\"\n",
    "- Kudo & Richardson (2018): \"SentencePiece: A simple and language independent subword tokenizer\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Implement text cleaning and normalization\n",
    "2. Apply various tokenization strategies\n",
    "3. Extract linguistic features\n",
    "4. Handle special characters and noise\n",
    "5. Prepare text for model input\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# Data manipulation and NLP imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.data.preprocessing.text_cleaner import TextCleaner, CleaningConfig\n",
    "from src.data.preprocessing.tokenization import Tokenizer, TokenizationConfig\n",
    "from src.data.preprocessing.feature_extraction import FeatureExtractor, FeatureExtractionConfig\n",
    "from src.data.preprocessing.sliding_window import SlidingWindowProcessor\n",
    "from src.data.preprocessing.prompt_formatter import PromptFormatter\n",
    "from src.utils.io_utils import safe_load, safe_save, ensure_dir\n",
    "from src.utils.logging_config import setup_logging\n",
    "from configs.constants import (\n",
    "    AG_NEWS_CLASSES,\n",
    "    DATA_DIR,\n",
    "    OUTPUT_DIR\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "logger = setup_logging('preprocessing_tutorial')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "print(\"Text Preprocessing Tutorial\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News dataset\n",
    "config = AGNewsConfig(\n",
    "    data_dir=DATA_DIR / \"processed\",\n",
    "    max_samples=1000  # Limit for tutorial\n",
    ")\n",
    "\n",
    "dataset = AGNewsDataset(config, split=\"train\")\n",
    "\n",
    "# Extract sample texts\n",
    "sample_texts = dataset.texts[:10]\n",
    "sample_labels = dataset.labels[:10]\n",
    "\n",
    "print(\"Sample Data Loaded:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Sample texts: {len(sample_texts)}\")\n",
    "\n",
    "# Display first sample\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(f\"  Label: {sample_labels[0]} ({dataset.label_names[0]})\")\n",
    "print(f\"  Text: {sample_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text cleaner\n",
    "cleaning_config = CleaningConfig(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=False,\n",
    "    remove_digits=False,\n",
    "    remove_urls=True,\n",
    "    remove_emails=True,\n",
    "    remove_html_tags=True,\n",
    "    remove_special_chars=False,\n",
    "    normalize_whitespace=True\n",
    ")\n",
    "\n",
    "cleaner = TextCleaner(cleaning_config)\n",
    "\n",
    "# Clean sample texts\n",
    "cleaned_texts = [cleaner.clean(text) for text in sample_texts]\n",
    "\n",
    "print(\"Text Cleaning Examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Original: {sample_texts[i][:150]}...\")\n",
    "    print(f\"  Cleaned:  {cleaned_texts[i][:150]}...\")\n",
    "    \n",
    "    # Calculate reduction\n",
    "    orig_len = len(sample_texts[i])\n",
    "    clean_len = len(cleaned_texts[i])\n",
    "    reduction = (1 - clean_len/orig_len) * 100\n",
    "    print(f\"  Length reduction: {reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_normalize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply advanced text normalization techniques.\n",
    "    \n",
    "    Following normalization practices from:\n",
    "        Jurafsky & Martin (2023): \"Speech and Language Processing\"\n",
    "    \"\"\"\n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove accents\n",
    "    text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "    \n",
    "    # Expand contractions\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "    \n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply advanced normalization\n",
    "normalized_texts = [advanced_normalize(text) for text in cleaned_texts]\n",
    "\n",
    "print(\"Advanced Normalization Examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_text = \"It won't be easy, but we'll succeed! Café résumé naïve.\"\n",
    "normalized = advanced_normalize(test_text)\n",
    "\n",
    "print(f\"Original:   {test_text}\")\n",
    "print(f\"Normalized: {normalized}\")\n",
    "\n",
    "# Apply to sample\n",
    "print(f\"\\nSample text normalization:\")\n",
    "print(f\"  Before: {cleaned_texts[0][:100]}...\")\n",
    "print(f\"  After:  {normalized_texts[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different tokenization approaches\n",
    "print(\"Tokenization Strategies Comparison:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sample_text = normalized_texts[0]\n",
    "\n",
    "# 1. Whitespace tokenization\n",
    "whitespace_tokens = sample_text.split()\n",
    "print(f\"\\n1. Whitespace tokenization:\")\n",
    "print(f\"   Tokens: {len(whitespace_tokens)}\")\n",
    "print(f\"   Sample: {whitespace_tokens[:10]}\")\n",
    "\n",
    "# 2. NLTK tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(sample_text)\n",
    "print(f\"\\n2. NLTK tokenization:\")\n",
    "print(f\"   Tokens: {len(nltk_tokens)}\")\n",
    "print(f\"   Sample: {nltk_tokens[:10]}\")\n",
    "\n",
    "# 3. Subword tokenization (BPE)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "subword_tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"\\n3. Subword tokenization (BERT):\")\n",
    "print(f\"   Tokens: {len(subword_tokens)}\")\n",
    "print(f\"   Sample: {subword_tokens[:10]}\")\n",
    "\n",
    "# 4. Character-level tokenization\n",
    "char_tokens = list(sample_text)\n",
    "print(f\"\\n4. Character tokenization:\")\n",
    "print(f\"   Tokens: {len(char_tokens)}\")\n",
    "print(f\"   Sample: {char_tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linguistic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_linguistic_features(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract linguistic features from text.\n",
    "    \n",
    "    Following feature engineering practices from:\n",
    "        Zheng & Casari (2018): \"Feature Engineering for Machine Learning\"\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    features['char_count'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    features['sentence_count'] = text.count('.') + text.count('!') + text.count('?')\n",
    "    features['avg_word_length'] = np.mean([len(word) for word in text.split()])\n",
    "    \n",
    "    # Vocabulary richness\n",
    "    words = text.lower().split()\n",
    "    features['unique_words'] = len(set(words))\n",
    "    features['lexical_diversity'] = features['unique_words'] / len(words) if words else 0\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    pos_counts = Counter([tag for word, tag in pos_tags])\n",
    "    \n",
    "    features['noun_count'] = sum(count for tag, count in pos_counts.items() if tag.startswith('NN'))\n",
    "    features['verb_count'] = sum(count for tag, count in pos_counts.items() if tag.startswith('VB'))\n",
    "    features['adj_count'] = sum(count for tag, count in pos_counts.items() if tag.startswith('JJ'))\n",
    "    \n",
    "    # Special patterns\n",
    "    features['has_numbers'] = bool(re.search(r'\\d', text))\n",
    "    features['has_urls'] = bool(re.search(r'https?://', text))\n",
    "    features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features from samples\n",
    "print(\"Linguistic Feature Extraction:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(3):\n",
    "    features = extract_linguistic_features(sample_texts[i])\n",
    "    \n",
    "    print(f\"\\nSample {i+1} features:\")\n",
    "    for key, value in list(features.items())[:8]:  # Show first 8 features\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key:20}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {key:20}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stop Words and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text: str, custom_stopwords: Optional[set] = None) -> str:\n",
    "    \"\"\"\n",
    "    Remove stopwords from text.\n",
    "    \n",
    "    Following stopword removal strategies from:\n",
    "        Manning et al. (2008): \"Introduction to Information Retrieval\"\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Combine default and custom stopwords\n",
    "    all_stopwords = stop_words.copy()\n",
    "    if custom_stopwords:\n",
    "        all_stopwords.update(custom_stopwords)\n",
    "    \n",
    "    # Filter words\n",
    "    filtered_words = [word for word in words if word not in all_stopwords]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stopword removal\n",
    "print(\"Stopword Removal Examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Add domain-specific stopwords for news\n",
    "news_stopwords = {'said', 'says', 'according', 'reported', 'news'}\n",
    "\n",
    "for i in range(2):\n",
    "    original = normalized_texts[i]\n",
    "    filtered = remove_stopwords(original, news_stopwords)\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Original ({len(original.split())} words):\")\n",
    "    print(f\"    {original[:150]}...\")\n",
    "    print(f\"  Filtered ({len(filtered.split())} words):\")\n",
    "    print(f\"    {filtered[:150]}...\")\n",
    "    print(f\"  Reduction: {(1 - len(filtered.split())/len(original.split()))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Prepare texts for vectorization\n",
    "texts_for_vectorization = [remove_stopwords(advanced_normalize(text)) \n",
    "                          for text in dataset.texts[:100]]\n",
    "\n",
    "print(\"Text Vectorization Methods:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Bag of Words\n",
    "count_vectorizer = CountVectorizer(max_features=100)\n",
    "bow_matrix = count_vectorizer.fit_transform(texts_for_vectorization)\n",
    "\n",
    "print(\"\\n1. Bag of Words (BoW):\")\n",
    "print(f\"   Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(f\"   Matrix shape: {bow_matrix.shape}\")\n",
    "print(f\"   Sparsity: {(bow_matrix.nnz / np.prod(bow_matrix.shape))*100:.2f}%\")\n",
    "\n",
    "# 2. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts_for_vectorization)\n",
    "\n",
    "print(\"\\n2. TF-IDF:\")\n",
    "print(f\"   Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"   Matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"   Sparsity: {(tfidf_matrix.nnz / np.prod(tfidf_matrix.shape))*100:.2f}%\")\n",
    "\n",
    "# Show top features\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "top_indices = tfidf_scores.argsort()[-10:][::-1]\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF features:\")\n",
    "for idx in top_indices:\n",
    "    print(f\"   {feature_names[idx]:20} : {tfidf_scores[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Transformer Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different transformer tokenizers\n",
    "print(\"Transformer Tokenization Comparison:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sample_text = sample_texts[0]\n",
    "\n",
    "tokenizers = {\n",
    "    'BERT': 'bert-base-uncased',\n",
    "    'RoBERTa': 'roberta-base',\n",
    "    'DeBERTa': 'microsoft/deberta-v3-base'\n",
    "}\n",
    "\n",
    "for name, model_name in tokenizers.items():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        sample_text,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "    \n",
    "    # Count non-padding tokens\n",
    "    non_padding = sum(1 for token in tokens if token not in ['[PAD]', '<pad>'])\n",
    "    \n",
    "    print(f\"\\n{name} Tokenizer:\")\n",
    "    print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"  Tokens (non-padding): {non_padding}\")\n",
    "    print(f\"  First 10 tokens: {tokens[:10]}\")\n",
    "    print(f\"  Special tokens: {list(tokenizer.special_tokens_map.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for AG News.\n",
    "    \n",
    "    Following pipeline design from:\n",
    "        Pedregosa et al. (2011): \"Scikit-learn: Machine Learning in Python\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lowercase: bool = True,\n",
    "                 remove_stopwords: bool = True,\n",
    "                 normalize: bool = True,\n",
    "                 tokenizer_name: str = 'bert-base-uncased'):\n",
    "        \n",
    "        self.lowercase = lowercase\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.normalize = normalize\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def process(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process single text through pipeline.\"\"\"\n",
    "        \n",
    "        # Step 1: Basic cleaning\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Step 2: Normalization\n",
    "        if self.normalize:\n",
    "            text = advanced_normalize(text)\n",
    "        \n",
    "        # Step 3: Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            words = text.split()\n",
    "            text = ' '.join([w for w in words if w not in self.stop_words])\n",
    "        \n",
    "        # Step 4: Tokenization\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'processed_text': text,\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'num_tokens': encoding['attention_mask'].sum().item()\n",
    "        }\n",
    "\n",
    "# Create and test pipeline\n",
    "pipeline = PreprocessingPipeline()\n",
    "\n",
    "print(\"Preprocessing Pipeline Test:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Process samples\n",
    "for i in range(3):\n",
    "    result = pipeline.process(sample_texts[i])\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Original length: {len(sample_texts[i])} chars\")\n",
    "    print(f\"  Processed length: {len(result['processed_text'])} chars\")\n",
    "    print(f\"  Number of tokens: {result['num_tokens']}\")\n",
    "    print(f\"  Input shape: {result['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocess(texts: List[str], \n",
    "                    batch_size: int = 32,\n",
    "                    show_progress: bool = True) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Batch preprocessing for efficiency.\n",
    "    \n",
    "    Following batch processing optimization from:\n",
    "        Howard & Ruder (2018): \"Universal Language Model Fine-tuning for Text Classification\"\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    \n",
    "    # Process in batches\n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    iterator = range(0, len(texts), batch_size)\n",
    "    if show_progress:\n",
    "        iterator = tqdm(iterator, desc=\"Processing batches\", total=num_batches)\n",
    "    \n",
    "    for i in iterator:\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Batch tokenization\n",
    "        encoding = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        all_input_ids.append(encoding['input_ids'])\n",
    "        all_attention_masks.append(encoding['attention_mask'])\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    return {\n",
    "        'input_ids': torch.cat(all_input_ids, dim=0),\n",
    "        'attention_mask': torch.cat(all_attention_masks, dim=0)\n",
    "    }\n",
    "\n",
    "# Process dataset in batches\n",
    "print(\"Batch Processing Performance:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import time\n",
    "\n",
    "# Test with different batch sizes\n",
    "test_texts = dataset.texts[:200]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    start_time = time.time()\n",
    "    result = batch_preprocess(test_texts, batch_size=batch_size, show_progress=False)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nBatch size {batch_size}:\")\n",
    "    print(f\"  Processing time: {elapsed:.2f} seconds\")\n",
    "    print(f\"  Throughput: {len(test_texts)/elapsed:.0f} samples/sec\")\n",
    "    print(f\"  Output shape: {result['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data for future use\n",
    "def save_preprocessed_data(texts: List[str], \n",
    "                          labels: List[int],\n",
    "                          output_path: Path):\n",
    "    \"\"\"\n",
    "    Save preprocessed data efficiently.\n",
    "    \n",
    "    Following data serialization best practices from:\n",
    "        PyTorch Documentation: \"Saving and Loading Models\"\n",
    "    \"\"\"\n",
    "    ensure_dir(output_path.parent)\n",
    "    \n",
    "    # Preprocess all texts\n",
    "    print(\"Preprocessing texts...\")\n",
    "    processed = batch_preprocess(texts, batch_size=32)\n",
    "    \n",
    "    # Create dataset dictionary\n",
    "    dataset_dict = {\n",
    "        'input_ids': processed['input_ids'],\n",
    "        'attention_mask': processed['attention_mask'],\n",
    "        'labels': torch.tensor(labels, dtype=torch.long),\n",
    "        'metadata': {\n",
    "            'num_samples': len(texts),\n",
    "            'max_length': 256,\n",
    "            'tokenizer': 'bert-base-uncased',\n",
    "            'preprocessing_date': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to disk\n",
    "    torch.save(dataset_dict, output_path)\n",
    "    \n",
    "    print(f\"\\nPreprocessed data saved to: {output_path}\")\n",
    "    print(f\"  File size: {output_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Save sample preprocessed data\n",
    "output_file = OUTPUT_DIR / \"tutorial\" / \"preprocessed_sample.pt\"\n",
    "saved_path = save_preprocessed_data(\n",
    "    texts=dataset.texts[:500],\n",
    "    labels=dataset.labels[:500],\n",
    "    output_path=output_file\n",
    ")\n",
    "\n",
    "# Verify saved data\n",
    "loaded_data = torch.load(saved_path)\n",
    "print(\"\\nVerification of saved data:\")\n",
    "print(f\"  Input IDs shape: {loaded_data['input_ids'].shape}\")\n",
    "print(f\"  Attention mask shape: {loaded_data['attention_mask'].shape}\")\n",
    "print(f\"  Labels shape: {loaded_data['labels'].shape}\")\n",
    "print(f\"  Metadata: {loaded_data['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusions and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Text Cleaning**:\n",
    "   - Remove noise while preserving information\n",
    "   - Balance between aggressive and minimal cleaning\n",
    "   - Consider domain-specific requirements\n",
    "\n",
    "2. **Tokenization**:\n",
    "   - Choose appropriate tokenization for model type\n",
    "   - Subword tokenization handles OOV words better\n",
    "   - Consider computational cost vs. accuracy\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Extract both statistical and linguistic features\n",
    "   - Combine multiple feature types for robustness\n",
    "   - Validate feature importance\n",
    "\n",
    "4. **Efficiency**:\n",
    "   - Use batch processing for large datasets\n",
    "   - Cache preprocessed data\n",
    "   - Optimize pipeline for production\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always preserve original data** for reproducibility\n",
    "2. **Document preprocessing steps** clearly\n",
    "3. **Validate preprocessing** impact on model performance\n",
    "4. **Use consistent preprocessing** across train/val/test\n",
    "5. **Monitor preprocessing time** for optimization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Continue to: `03_model_training_basics.ipynb` for model training\n",
    "- Explore: `05_prompt_engineering.ipynb` for advanced formatting\n",
    "- Review: Documentation at `docs/user_guide/data_preparation.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
