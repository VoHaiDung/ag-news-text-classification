{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Tutorial for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates comprehensive text preprocessing techniques following methodologies from:\n",
    "- Jurafsky & Martin (2023): \"Speech and Language Processing\"\n",
    "- Manning et al. (2008): \"Introduction to Information Retrieval\"\n",
    "- Devlin et al. (2019): \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand text preprocessing pipeline components\n",
    "2. Apply various cleaning and normalization techniques\n",
    "3. Implement tokenization strategies for transformer models\n",
    "4. Compare preprocessing effects on model performance\n",
    "5. Create custom preprocessing pipelines\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Callable\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "import spacy\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Project setup\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Project imports\n",
    "from src.data.preprocessing.text_cleaner import (\n",
    "    TextCleaner, \n",
    "    CleaningConfig,\n",
    "    get_minimal_cleaner,\n",
    "    get_standard_cleaner,\n",
    "    get_aggressive_cleaner\n",
    ")\n",
    "from src.data.preprocessing.tokenization import (\n",
    "    TokenizerWrapper,\n",
    "    TokenizationConfig\n",
    ")\n",
    "from src.data.preprocessing.feature_extraction import (\n",
    "    FeatureExtractor,\n",
    "    FeatureConfig\n",
    ")\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.utils.reproducibility import set_seed\n",
    "from configs.constants import DATA_DIR, AG_NEWS_CLASSES\n",
    "\n",
    "# Set random seed\n",
    "set_seed(42)\n",
    "\n",
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find('punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"Environment ready for preprocessing tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data for preprocessing\n",
    "dataset_config = AGNewsConfig(\n",
    "    data_dir=DATA_DIR / \"processed\",\n",
    "    max_samples=1000,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "try:\n",
    "    dataset = AGNewsDataset(dataset_config, split=\"train\")\n",
    "    texts = dataset.texts[:100]  # Use first 100 samples\n",
    "    labels = dataset.labels[:100]\n",
    "    print(f\"Loaded {len(texts)} sample texts\")\n",
    "except:\n",
    "    # Fallback sample texts\n",
    "    texts = [\n",
    "        \"Wall Street #1 stocks rise; S&P 500 hits NEW record HIGH!!!\",\n",
    "        \"Scientists discover potential cure for COVID-19 using mRNA technology.\",\n",
    "        \"Manchester United defeats Chelsea 3-2 in Premier League match.\",\n",
    "        \"Apple Inc. announces new iPhone 15 with revolutionary features.\",\n",
    "        \"NASA's Mars rover finds evidence of ancient water on the Red Planet.\"\n",
    "    ]\n",
    "    labels = [2, 3, 1, 2, 3]  # Business, Sci/Tech, Sports, Business, Sci/Tech\n",
    "    print(f\"Using {len(texts)} fallback sample texts\")\n",
    "\n",
    "# Display sample texts\n",
    "print(\"\\nSample texts before preprocessing:\")\n",
    "for i, text in enumerate(texts[:3]):\n",
    "    print(f\"{i+1}. {text[:100]}...\" if len(text) > 100 else f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_cleaning_steps(text: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Demonstrate individual cleaning steps.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of cleaning steps and results\n",
    "    \"\"\"\n",
    "    steps = {}\n",
    "    \n",
    "    # Original\n",
    "    steps['original'] = text\n",
    "    \n",
    "    # Lowercase\n",
    "    steps['lowercase'] = text.lower()\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    steps['no_html'] = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    steps['no_urls'] = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    steps['no_emails'] = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    steps['no_punctuation'] = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    steps['no_numbers'] = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    steps['normalized_whitespace'] = ' '.join(text.split())\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    steps['ascii_only'] = ''.join(char for char in text if ord(char) < 128)\n",
    "    \n",
    "    return steps\n",
    "\n",
    "# Demonstrate on sample text\n",
    "sample_text = \"Check out this URL: https://example.com! Email us at info@example.com. Price: $99.99 #awesome\"\n",
    "cleaning_steps = demonstrate_cleaning_steps(sample_text)\n",
    "\n",
    "print(\"Text Cleaning Steps Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "for step_name, result in cleaning_steps.items():\n",
    "    print(f\"\\n{step_name}:\")\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Text Cleaner Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different cleaning configurations\n",
    "cleaning_configs = {\n",
    "    'minimal': get_minimal_cleaner(),\n",
    "    'standard': get_standard_cleaner(),\n",
    "    'aggressive': get_aggressive_cleaner()\n",
    "}\n",
    "\n",
    "# Apply different cleaning strategies\n",
    "test_text = texts[0] if texts else sample_text\n",
    "\n",
    "print(\"Comparing Cleaning Strategies\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nOriginal text ({len(test_text)} chars):\")\n",
    "print(f\"  {test_text}\")\n",
    "\n",
    "cleaning_results = {}\n",
    "for strategy_name, cleaner in cleaning_configs.items():\n",
    "    cleaned = cleaner.clean(test_text)\n",
    "    cleaning_results[strategy_name] = cleaned\n",
    "    \n",
    "    print(f\"\\n{strategy_name.capitalize()} cleaning ({len(cleaned)} chars):\")\n",
    "    print(f\"  {cleaned}\")\n",
    "    print(f\"  Character reduction: {(1 - len(cleaned)/len(test_text))*100:.1f}%\")\n",
    "\n",
    "# Measure cleaning speed\n",
    "print(\"\\nCleaning Speed Comparison:\")\n",
    "for strategy_name, cleaner in cleaning_configs.items():\n",
    "    start_time = time.time()\n",
    "    for text in texts[:100]:\n",
    "        _ = cleaner.clean(text)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"  {strategy_name}: {elapsed*1000:.2f}ms for 100 texts\")\n",
    "    print(f\"    ({elapsed*10:.2f}ms per text)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different tokenization methods\n",
    "test_sentence = \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\"\n",
    "\n",
    "print(\"Tokenization Methods Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nOriginal: {test_sentence}\")\n",
    "print()\n",
    "\n",
    "# 1. Simple whitespace tokenization\n",
    "whitespace_tokens = test_sentence.split()\n",
    "print(f\"Whitespace tokenization ({len(whitespace_tokens)} tokens):\")\n",
    "print(f\"  {whitespace_tokens}\")\n",
    "\n",
    "# 2. NLTK word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk_tokens = word_tokenize(test_sentence)\n",
    "print(f\"\\nNLTK tokenization ({len(nltk_tokens)} tokens):\")\n",
    "print(f\"  {nltk_tokens}\")\n",
    "\n",
    "# 3. Transformer tokenization (BERT)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_tokens = bert_tokenizer.tokenize(test_sentence.lower())\n",
    "print(f\"\\nBERT tokenization ({len(bert_tokens)} tokens):\")\n",
    "print(f\"  {bert_tokens}\")\n",
    "\n",
    "# 4. Subword tokenization example\n",
    "complex_word = \"unbelievably\"\n",
    "print(f\"\\nSubword tokenization of '{complex_word}':\")\n",
    "print(f\"  BERT: {bert_tokenizer.tokenize(complex_word)}\")\n",
    "\n",
    "# Demonstrate handling of special cases\n",
    "special_cases = [\n",
    "    \"don't\",\n",
    "    \"U.S.A.\",\n",
    "    \"email@example.com\",\n",
    "    \"#hashtag\",\n",
    "    \"$100.50\"\n",
    "]\n",
    "\n",
    "print(\"\\nSpecial Cases Handling:\")\n",
    "for case in special_cases:\n",
    "    print(f\"  '{case}' -> {word_tokenize(case)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformer Model Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization for different transformer models\n",
    "from src.data.preprocessing.tokenization import TokenizerWrapper, TokenizationConfig\n",
    "\n",
    "# Create tokenization config\n",
    "tokenizer_config = TokenizationConfig(\n",
    "    model_name='bert-base-uncased',\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Initialize tokenizer wrapper\n",
    "tokenizer_wrapper = TokenizerWrapper(tokenizer_config)\n",
    "\n",
    "# Tokenize sample texts\n",
    "sample_texts_for_tokenization = [\n",
    "    \"This is a short text.\",\n",
    "    \"This is a much longer text that might need to be truncated if it exceeds the maximum length limit set in the configuration.\",\n",
    "    texts[0] if texts else \"Default text for tokenization example.\"\n",
    "]\n",
    "\n",
    "print(\"Transformer Tokenization Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(sample_texts_for_tokenization):\n",
    "    # Tokenize\n",
    "    encoded = tokenizer_wrapper.encode_single(text)\n",
    "    \n",
    "    print(f\"\\nText {i+1}: {text[:50]}...\" if len(text) > 50 else f\"\\nText {i+1}: {text}\")\n",
    "    print(f\"  Original length: {len(text)} characters\")\n",
    "    print(f\"  Token IDs shape: {encoded['input_ids'].shape}\")\n",
    "    print(f\"  Attention mask shape: {encoded['attention_mask'].shape}\")\n",
    "    \n",
    "    # Decode back to text\n",
    "    decoded = tokenizer_wrapper.tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)\n",
    "    print(f\"  Decoded: {decoded[:50]}...\" if len(decoded) > 50 else f\"  Decoded: {decoded}\")\n",
    "    \n",
    "    # Show special tokens\n",
    "    tokens_with_special = tokenizer_wrapper.tokenizer.convert_ids_to_tokens(encoded['input_ids'][0][:20])\n",
    "    print(f\"  First 20 tokens: {tokens_with_special}\")\n",
    "\n",
    "# Batch tokenization\n",
    "print(\"\\nBatch Tokenization:\")\n",
    "batch_encoded = tokenizer_wrapper.encode_batch(sample_texts_for_tokenization)\n",
    "print(f\"  Batch input_ids shape: {batch_encoded['input_ids'].shape}\")\n",
    "print(f\"  Batch attention_mask shape: {batch_encoded['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract various features from text\n",
    "from src.data.preprocessing.feature_extraction import FeatureExtractor, FeatureConfig\n",
    "\n",
    "# Create feature extraction config\n",
    "feature_config = FeatureConfig(\n",
    "    extract_length_features=True,\n",
    "    extract_pos_features=True,\n",
    "    extract_entity_features=True,\n",
    "    extract_sentiment_features=False,  # Requires additional models\n",
    "    extract_readability_features=True\n",
    ")\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor(feature_config)\n",
    "\n",
    "# Extract features from sample texts\n",
    "test_texts_for_features = [\n",
    "    \"Apple Inc. announced record profits in Q4 2024.\",\n",
    "    \"The team won the championship after a thrilling final match.\",\n",
    "    \"Researchers at MIT developed a new quantum computing algorithm.\"\n",
    "]\n",
    "\n",
    "print(\"Feature Extraction Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for text in test_texts_for_features:\n",
    "    features = feature_extractor.extract(text)\n",
    "    \n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Features:\")\n",
    "    \n",
    "    # Display features in organized way\n",
    "    if 'length_features' in features:\n",
    "        print(\"  Length features:\")\n",
    "        for key, value in features['length_features'].items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "    \n",
    "    if 'pos_features' in features:\n",
    "        print(\"  POS tag distribution:\")\n",
    "        for tag, count in list(features['pos_features'].items())[:5]:\n",
    "            print(f\"    {tag}: {count}\")\n",
    "    \n",
    "    if 'entities' in features:\n",
    "        print(f\"  Named entities: {features['entities']}\")\n",
    "    \n",
    "    if 'readability' in features:\n",
    "        print(f\"  Readability score: {features['readability']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPreprocessingPipeline:\n",
    "    \"\"\"\n",
    "    Custom preprocessing pipeline for AG News classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 cleaning_strategy: str = 'standard',\n",
    "                 tokenizer_model: str = 'bert-base-uncased',\n",
    "                 max_length: int = 128):\n",
    "        \"\"\"\n",
    "        Initialize preprocessing pipeline.\n",
    "        \n",
    "        Args:\n",
    "            cleaning_strategy: Cleaning strategy to use\n",
    "            tokenizer_model: Tokenizer model name\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        # Initialize cleaner\n",
    "        if cleaning_strategy == 'minimal':\n",
    "            self.cleaner = get_minimal_cleaner()\n",
    "        elif cleaning_strategy == 'aggressive':\n",
    "            self.cleaner = get_aggressive_cleaner()\n",
    "        else:\n",
    "            self.cleaner = get_standard_cleaner()\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Track statistics\n",
    "        self.stats = {\n",
    "            'texts_processed': 0,\n",
    "            'avg_original_length': 0,\n",
    "            'avg_cleaned_length': 0,\n",
    "            'avg_token_count': 0\n",
    "        }\n",
    "    \n",
    "    def preprocess(self, text: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Apply full preprocessing pipeline.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with preprocessed data\n",
    "        \"\"\"\n",
    "        # Original text stats\n",
    "        original_length = len(text)\n",
    "        \n",
    "        # Clean text\n",
    "        cleaned_text = self.cleaner.clean(text)\n",
    "        cleaned_length = len(cleaned_text)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            cleaned_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Count actual tokens (excluding padding)\n",
    "        token_count = (encoded['attention_mask'][0] == 1).sum().item()\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['texts_processed'] += 1\n",
    "        n = self.stats['texts_processed']\n",
    "        self.stats['avg_original_length'] = (\n",
    "            (self.stats['avg_original_length'] * (n-1) + original_length) / n\n",
    "        )\n",
    "        self.stats['avg_cleaned_length'] = (\n",
    "            (self.stats['avg_cleaned_length'] * (n-1) + cleaned_length) / n\n",
    "        )\n",
    "        self.stats['avg_token_count'] = (\n",
    "            (self.stats['avg_token_count'] * (n-1) + token_count) / n\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'input_ids': encoded['input_ids'],\n",
    "            'attention_mask': encoded['attention_mask'],\n",
    "            'token_count': token_count,\n",
    "            'original_length': original_length,\n",
    "            'cleaned_length': cleaned_length\n",
    "        }\n",
    "    \n",
    "    def preprocess_batch(self, texts: List[str]) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Preprocess batch of texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with batch preprocessed data\n",
    "        \"\"\"\n",
    "        results = [self.preprocess(text) for text in texts]\n",
    "        \n",
    "        # Stack tensors\n",
    "        import torch\n",
    "        batch_result = {\n",
    "            'input_ids': torch.cat([r['input_ids'] for r in results]),\n",
    "            'attention_mask': torch.cat([r['attention_mask'] for r in results]),\n",
    "            'cleaned_texts': [r['cleaned_text'] for r in results],\n",
    "            'token_counts': [r['token_count'] for r in results]\n",
    "        }\n",
    "        \n",
    "        return batch_result\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, float]:\n",
    "        \"\"\"Get preprocessing statistics.\"\"\"\n",
    "        return self.stats.copy()\n",
    "\n",
    "# Create and test custom pipeline\n",
    "pipeline = CustomPreprocessingPipeline(\n",
    "    cleaning_strategy='standard',\n",
    "    tokenizer_model='bert-base-uncased',\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Process sample texts\n",
    "print(\"Custom Pipeline Processing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(texts[:3]):\n",
    "    result = pipeline.preprocess(text)\n",
    "    \n",
    "    print(f\"\\nText {i+1}:\")\n",
    "    print(f\"  Original ({result['original_length']} chars): {text[:50]}...\")\n",
    "    print(f\"  Cleaned ({result['cleaned_length']} chars): {result['cleaned_text'][:50]}...\")\n",
    "    print(f\"  Tokens: {result['token_count']}\")\n",
    "    print(f\"  Input shape: {result['input_ids'].shape}\")\n",
    "\n",
    "# Display statistics\n",
    "stats = pipeline.get_statistics()\n",
    "print(\"\\nPipeline Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preprocessing Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze impact of different preprocessing strategies\n",
    "strategies = ['minimal', 'standard', 'aggressive']\n",
    "analysis_results = []\n",
    "\n",
    "print(\"Preprocessing Impact Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for strategy in strategies:\n",
    "    # Create pipeline\n",
    "    pipeline = CustomPreprocessingPipeline(cleaning_strategy=strategy)\n",
    "    \n",
    "    # Process texts\n",
    "    processing_times = []\n",
    "    for text in texts[:50]:  # Use first 50 texts\n",
    "        start_time = time.time()\n",
    "        result = pipeline.preprocess(text)\n",
    "        processing_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Get statistics\n",
    "    stats = pipeline.get_statistics()\n",
    "    \n",
    "    # Store results\n",
    "    analysis_results.append({\n",
    "        'strategy': strategy,\n",
    "        'avg_original_length': stats['avg_original_length'],\n",
    "        'avg_cleaned_length': stats['avg_cleaned_length'],\n",
    "        'avg_token_count': stats['avg_token_count'],\n",
    "        'reduction_ratio': 1 - stats['avg_cleaned_length'] / stats['avg_original_length'],\n",
    "        'avg_processing_time': np.mean(processing_times) * 1000  # Convert to ms\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "df_results = pd.DataFrame(analysis_results)\n",
    "print(\"\\nComparative Analysis:\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"\"\"\n",
    "1. Minimal cleaning: Best for transformer models that handle raw text well\n",
    "2. Standard cleaning: Balanced approach for most use cases\n",
    "3. Aggressive cleaning: Useful for classical ML models or noisy data\n",
    "4. Consider domain-specific requirements (e.g., keeping numbers for financial news)\n",
    "5. Test different strategies on validation set to find optimal approach\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Preprocessing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimal preprocessing configuration\n",
    "optimal_config = {\n",
    "    'cleaning': {\n",
    "        'strategy': 'standard',\n",
    "        'lowercase': True,\n",
    "        'remove_urls': True,\n",
    "        'remove_emails': True,\n",
    "        'remove_html': True,\n",
    "        'remove_special_chars': False,\n",
    "        'remove_numbers': False,\n",
    "        'remove_punctuation': False,\n",
    "        'normalize_whitespace': True\n",
    "    },\n",
    "    'tokenization': {\n",
    "        'model_name': 'bert-base-uncased',\n",
    "        'max_length': 128,\n",
    "        'padding': 'max_length',\n",
    "        'truncation': True,\n",
    "        'return_tensors': 'pt'\n",
    "    },\n",
    "    'features': {\n",
    "        'extract_length_features': True,\n",
    "        'extract_pos_features': False,\n",
    "        'extract_entity_features': True,\n",
    "        'extract_readability_features': False\n",
    "    },\n",
    "    'analysis': df_results.to_dict('records')\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "import json\n",
    "config_path = PROJECT_ROOT / \"outputs\" / \"preprocessing\" / \"optimal_config.json\"\n",
    "config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(optimal_config, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Preprocessing configuration saved to: {config_path}\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\"\"\"\n",
    "Key Preprocessing Insights:\n",
    "1. Text cleaning reduces noise but may remove useful information\n",
    "2. Transformer tokenizers handle subwords effectively\n",
    "3. Feature extraction can provide additional signals for models\n",
    "4. Pipeline efficiency is important for large-scale processing\n",
    "5. Different tasks may require different preprocessing strategies\n",
    "\n",
    "Next Steps:\n",
    "- Apply preprocessing to full dataset\n",
    "- Test impact on model performance\n",
    "- Optimize pipeline for production use\n",
    "- Consider domain-specific preprocessing needs\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
