{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup for AG News Text Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides comprehensive environment setup following best practices from:\n",
    "- Sculley et al. (2015): \"Hidden Technical Debt in Machine Learning Systems\"\n",
    "- Amershi et al. (2019): \"Software Engineering for Machine Learning: A Case Study\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Verify system requirements\n",
    "2. Install and configure dependencies\n",
    "3. Validate GPU/TPU availability\n",
    "4. Setup project paths and configurations\n",
    "5. Test core module imports\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Requirements Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# System information\n",
    "print(\"System Information\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"CPU cores: {os.cpu_count()}\")\n",
    "\n",
    "# Check Python version\n",
    "required_python = (3, 8)\n",
    "current_python = sys.version_info[:2]\n",
    "\n",
    "if current_python < required_python:\n",
    "    raise RuntimeError(\n",
    "        f\"Python {required_python[0]}.{required_python[1]}+ required. \"\n",
    "        f\"Current: {current_python[0]}.{current_python[1]}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\nPython version check: PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU/Hardware Acceleration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "def check_gpu_availability() -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Check GPU availability following CUDA best practices.\n",
    "    \n",
    "    References:\n",
    "        NVIDIA CUDA Programming Guide v11.0\n",
    "    \"\"\"\n",
    "    gpu_info = {\n",
    "        'cuda_available': False,\n",
    "        'gpu_count': 0,\n",
    "        'gpu_names': [],\n",
    "        'cuda_version': None,\n",
    "        'cudnn_version': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        gpu_info['cuda_available'] = torch.cuda.is_available()\n",
    "        \n",
    "        if gpu_info['cuda_available']:\n",
    "            gpu_info['gpu_count'] = torch.cuda.device_count()\n",
    "            gpu_info['cuda_version'] = torch.version.cuda\n",
    "            \n",
    "            for i in range(gpu_info['gpu_count']):\n",
    "                gpu_info['gpu_names'].append(torch.cuda.get_device_name(i))\n",
    "            \n",
    "            # Check cuDNN\n",
    "            gpu_info['cudnn_version'] = torch.backends.cudnn.version()\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"PyTorch not installed. Install with: pip install torch\")\n",
    "    \n",
    "    return gpu_info\n",
    "\n",
    "# Check GPU\n",
    "gpu_info = check_gpu_availability()\n",
    "print(\"\\nGPU Configuration\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if gpu_info['cuda_available']:\n",
    "    print(f\"CUDA Available: Yes\")\n",
    "    print(f\"CUDA Version: {gpu_info['cuda_version']}\")\n",
    "    print(f\"cuDNN Version: {gpu_info['cudnn_version']}\")\n",
    "    print(f\"Number of GPUs: {gpu_info['gpu_count']}\")\n",
    "    for i, name in enumerate(gpu_info['gpu_names']):\n",
    "        print(f\"  GPU {i}: {name}\")\n",
    "else:\n",
    "    print(\"CUDA Available: No\")\n",
    "    print(\"Training will use CPU (slower)\")\n",
    "\n",
    "# Memory information\n",
    "try:\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"\\nSystem Memory:\")\n",
    "    print(f\"  Total: {memory.total / (1024**3):.1f} GB\")\n",
    "    print(f\"  Available: {memory.available / (1024**3):.1f} GB\")\n",
    "    print(f\"  Used: {memory.percent:.1f}%\")\n",
    "except ImportError:\n",
    "    print(\"\\npsutil not installed. Install with: pip install psutil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup project paths\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Define critical directories\n",
    "critical_dirs = {\n",
    "    'root': PROJECT_ROOT,\n",
    "    'src': PROJECT_ROOT / \"src\",\n",
    "    'configs': PROJECT_ROOT / \"configs\",\n",
    "    'data': PROJECT_ROOT / \"data\",\n",
    "    'outputs': PROJECT_ROOT / \"outputs\",\n",
    "    'scripts': PROJECT_ROOT / \"scripts\",\n",
    "    'notebooks': PROJECT_ROOT / \"notebooks\"\n",
    "}\n",
    "\n",
    "print(\"Project Structure Verification\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(\"\\nDirectory Status:\")\n",
    "\n",
    "missing_dirs = []\n",
    "for name, path in critical_dirs.items():\n",
    "    exists = path.exists()\n",
    "    status = \"[EXISTS]\" if exists else \"[MISSING]\"\n",
    "    print(f\"  {status:10} {name:10} : {path.relative_to(PROJECT_ROOT.parent)}\")\n",
    "    if not exists and name != 'root':\n",
    "        missing_dirs.append(path)\n",
    "\n",
    "# Create missing directories\n",
    "if missing_dirs:\n",
    "    print(\"\\nCreating missing directories...\")\n",
    "    for dir_path in missing_dirs:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  Created: {dir_path.relative_to(PROJECT_ROOT.parent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and install required packages\n",
    "def check_package_installation() -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Check installation status of required packages.\n",
    "    \n",
    "    Following dependency management practices from:\n",
    "        PEP 508 - Dependency specification for Python Software Packages\n",
    "    \"\"\"\n",
    "    required_packages = {\n",
    "        'numpy': 'numpy',\n",
    "        'pandas': 'pandas',\n",
    "        'torch': 'torch',\n",
    "        'transformers': 'transformers',\n",
    "        'datasets': 'datasets',\n",
    "        'scikit-learn': 'sklearn',\n",
    "        'matplotlib': 'matplotlib',\n",
    "        'seaborn': 'seaborn',\n",
    "        'tqdm': 'tqdm',\n",
    "        'pyyaml': 'yaml'\n",
    "    }\n",
    "    \n",
    "    installation_status = {}\n",
    "    \n",
    "    for package_name, import_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(import_name)\n",
    "            installation_status[package_name] = True\n",
    "        except ImportError:\n",
    "            installation_status[package_name] = False\n",
    "    \n",
    "    return installation_status\n",
    "\n",
    "print(\"Package Installation Status\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "package_status = check_package_installation()\n",
    "missing_packages = []\n",
    "\n",
    "for package, installed in package_status.items():\n",
    "    status = \"[INSTALLED]\" if installed else \"[MISSING]\"\n",
    "    print(f\"  {status:12} {package}\")\n",
    "    if not installed:\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(\"\\nMissing packages detected!\")\n",
    "    print(\"Install with:\")\n",
    "    print(f\"  pip install {' '.join(missing_packages)}\")\n",
    "    print(\"\\nOr install all requirements:\")\n",
    "    print(f\"  pip install -r {PROJECT_ROOT}/requirements/base.txt\")\n",
    "else:\n",
    "    print(\"\\nAll required packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Core Module Import Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test core module imports\n",
    "print(\"Core Module Import Test\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import_tests = [\n",
    "    ('Core', 'src.core.registry'),\n",
    "    ('Core', 'src.core.factory'),\n",
    "    ('Data', 'src.data.datasets.ag_news'),\n",
    "    ('Data', 'src.data.preprocessing.text_cleaner'),\n",
    "    ('Models', 'src.models.base.base_model'),\n",
    "    ('Training', 'src.training.trainers.base_trainer'),\n",
    "    ('Evaluation', 'src.evaluation.metrics.classification_metrics'),\n",
    "    ('Utils', 'src.utils.io_utils'),\n",
    "    ('Config', 'configs.constants'),\n",
    "    ('API', 'src.api.rest.app'),\n",
    "    ('Services', 'src.services.core.prediction_service')\n",
    "]\n",
    "\n",
    "failed_imports = []\n",
    "\n",
    "for category, module_path in import_tests:\n",
    "    try:\n",
    "        __import__(module_path)\n",
    "        print(f\"  [OK] {category:10} : {module_path}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"  [FAIL] {category:10} : {module_path}\")\n",
    "        print(f\"         Error: {str(e)}\")\n",
    "        failed_imports.append((module_path, str(e)))\n",
    "\n",
    "if failed_imports:\n",
    "    print(\"\\nWarning: Some modules failed to import.\")\n",
    "    print(\"This may affect certain functionalities.\")\n",
    "else:\n",
    "    print(\"\\nAll core modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration Loading Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration loading\n",
    "from configs.config_loader import ConfigLoader\n",
    "from configs.constants import DATA_DIR, MODEL_DIR, OUTPUT_DIR\n",
    "\n",
    "print(\"Configuration Loading Test\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize config loader\n",
    "config_loader = ConfigLoader()\n",
    "\n",
    "# Test loading different configs\n",
    "test_configs = [\n",
    "    ('Training', 'training/standard/base_training.yaml'),\n",
    "    ('Model', 'models/single/deberta_v3_xlarge.yaml'),\n",
    "    ('Data', 'data/preprocessing/standard.yaml'),\n",
    "    ('Environment', 'environments/dev.yaml')\n",
    "]\n",
    "\n",
    "for config_type, config_path in test_configs:\n",
    "    try:\n",
    "        config = config_loader.load_config(config_path)\n",
    "        print(f\"  [LOADED] {config_type:12} : Success\")\n",
    "        print(f\"           Keys: {list(config.keys())[:3]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] {config_type:12} : Failed\")\n",
    "        print(f\"          Error: {str(e)}\")\n",
    "\n",
    "# Verify critical paths\n",
    "print(\"\\nCritical Paths:\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  Model Directory: {MODEL_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Download Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if AG News dataset is available\n",
    "print(\"Dataset Availability Check\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "\n",
    "# Check for processed data\n",
    "processed_data_path = DATA_DIR / \"processed\"\n",
    "raw_data_path = DATA_DIR / \"raw\" / \"ag_news\"\n",
    "\n",
    "print(f\"Checking data directories:\")\n",
    "print(f\"  Raw data: {raw_data_path}\")\n",
    "print(f\"    Exists: {raw_data_path.exists()}\")\n",
    "print(f\"  Processed data: {processed_data_path}\")\n",
    "print(f\"    Exists: {processed_data_path.exists()}\")\n",
    "\n",
    "# Try to load a small sample\n",
    "try:\n",
    "    config = AGNewsConfig(\n",
    "        data_dir=processed_data_path,\n",
    "        max_samples=100  # Load only 100 samples for testing\n",
    "    )\n",
    "    \n",
    "    dataset = AGNewsDataset(config, split=\"train\")\n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"  Samples loaded: {len(dataset)}\")\n",
    "    print(f\"  Classes: {dataset.num_classes}\")\n",
    "    print(f\"  Label names: {dataset.class_names}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nDataset not found or corrupted.\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(f\"\\nTo download the dataset, run:\")\n",
    "    print(f\"  python {PROJECT_ROOT}/scripts/setup/download_all_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Environment Variables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"Environment Variables Setup\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load environment variables\n",
    "env_file = PROJECT_ROOT / \".env\"\n",
    "env_example = PROJECT_ROOT / \".env.example\"\n",
    "\n",
    "if env_file.exists():\n",
    "    load_dotenv(env_file)\n",
    "    print(f\"Loaded environment from: {env_file}\")\n",
    "elif env_example.exists():\n",
    "    print(f\"No .env file found. Copy from .env.example:\")\n",
    "    print(f\"  cp {env_example} {env_file}\")\n",
    "else:\n",
    "    print(\"No environment file found.\")\n",
    "\n",
    "# Set critical environment variables\n",
    "critical_env_vars = {\n",
    "    'PYTHONPATH': str(PROJECT_ROOT),\n",
    "    'PROJECT_ROOT': str(PROJECT_ROOT),\n",
    "    'CUDA_VISIBLE_DEVICES': '0',  # Use first GPU by default\n",
    "    'TOKENIZERS_PARALLELISM': 'false',  # Avoid tokenizer warnings\n",
    "    'TRANSFORMERS_CACHE': str(PROJECT_ROOT / \"cache\" / \"transformers\")\n",
    "}\n",
    "\n",
    "print(\"\\nSetting environment variables:\")\n",
    "for var_name, var_value in critical_env_vars.items():\n",
    "    if var_name not in os.environ:\n",
    "        os.environ[var_name] = var_value\n",
    "        print(f\"  Set {var_name}\")\n",
    "    else:\n",
    "        print(f\"  {var_name} already set\")\n",
    "\n",
    "# Verify environment\n",
    "print(\"\\nEnvironment verification:\")\n",
    "print(f\"  PYTHONPATH includes project: {str(PROJECT_ROOT) in os.environ.get('PYTHONPATH', '')}\")\n",
    "print(f\"  GPU access configured: {'CUDA_VISIBLE_DEVICES' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive check\n",
    "def final_environment_validation() -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Perform final validation of environment setup.\n",
    "    \"\"\"\n",
    "    checks = {\n",
    "        'python_version': sys.version_info >= (3, 8),\n",
    "        'project_structure': all(p.exists() for p in critical_dirs.values()),\n",
    "        'core_imports': len(failed_imports) == 0,\n",
    "        'packages_installed': len(missing_packages) == 0,\n",
    "        'gpu_available': gpu_info['cuda_available'],\n",
    "        'configs_loadable': True,  # Simplified check\n",
    "        'data_accessible': (DATA_DIR / \"processed\").exists() or (DATA_DIR / \"raw\").exists()\n",
    "    }\n",
    "    \n",
    "    return checks\n",
    "\n",
    "print(\"Final Environment Validation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "validation_results = final_environment_validation()\n",
    "all_passed = all(validation_results.values())\n",
    "\n",
    "for check_name, passed in validation_results.items():\n",
    "    status = \"[PASS]\" if passed else \"[FAIL]\"\n",
    "    requirement = \"Required\" if check_name != 'gpu_available' else \"Optional\"\n",
    "    print(f\"  {status:7} {check_name:20} [{requirement}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_passed or (not validation_results['gpu_available'] and \n",
    "                  all(v for k, v in validation_results.items() if k != 'gpu_available')):\n",
    "    print(\"Environment setup complete! You can proceed with the tutorials.\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Run: notebooks/tutorials/01_data_loading_basics.ipynb\")\n",
    "    print(\"  2. Run: notebooks/tutorials/02_preprocessing_tutorial.ipynb\")\n",
    "    print(\"  3. Run: notebooks/tutorials/03_model_training_basics.ipynb\")\n",
    "else:\n",
    "    print(\"Some required components are missing.\")\n",
    "    print(\"Please address the issues marked with [FAIL] above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **CUDA/GPU Not Detected**:\n",
    "   - Verify NVIDIA drivers: `nvidia-smi`\n",
    "   - Check PyTorch CUDA version matches system CUDA\n",
    "   - Reinstall PyTorch with CUDA support\n",
    "\n",
    "2. **Import Errors**:\n",
    "   - Ensure PYTHONPATH includes project root\n",
    "   - Check for missing `__init__.py` files\n",
    "   - Verify package installations\n",
    "\n",
    "3. **Memory Issues**:\n",
    "   - Reduce batch size in configs\n",
    "   - Use gradient accumulation\n",
    "   - Enable mixed precision training\n",
    "\n",
    "4. **Dataset Download Failures**:\n",
    "   - Check internet connectivity\n",
    "   - Verify Hugging Face Hub access\n",
    "   - Use manual download scripts\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "For additional support:\n",
    "1. Check project documentation: `docs/troubleshooting.md`\n",
    "2. Review GitHub issues: https://github.com/VoHaiDung/ag-news-text-classification\n",
    "3. Contact: vohaidung.work@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
