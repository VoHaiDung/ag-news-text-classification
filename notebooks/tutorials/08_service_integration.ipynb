{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service Integration for AG News Text Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates comprehensive service integration patterns following methodologies from:\n",
    "- Newman (2015): \"Building Microservices: Designing Fine-Grained Systems\"\n",
    "- Richardson (2018): \"Microservices Patterns: With Examples in Java\"\n",
    "- Kleppmann (2017): \"Designing Data-Intensive Applications\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Integrate core classification services\n",
    "2. Implement service orchestration\n",
    "3. Configure message queuing systems\n",
    "4. Set up caching strategies\n",
    "5. Connect storage services\n",
    "6. Enable monitoring and alerting\n",
    "7. Deploy notification systems\n",
    "8. Manage service pipelines\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from enum import Enum\n",
    "import warnings\n",
    "\n",
    "# Service and infrastructure imports\n",
    "import redis\n",
    "import celery\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import boto3\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.services.base_service import BaseService\n",
    "from src.services.service_registry import ServiceRegistry\n",
    "from src.services.core.prediction_service import PredictionService\n",
    "from src.services.core.training_service import TrainingService\n",
    "from src.services.core.data_service import DataService\n",
    "from src.services.core.model_management_service import ModelManagementService\n",
    "from src.services.orchestration.workflow_orchestrator import WorkflowOrchestrator\n",
    "from src.services.orchestration.pipeline_manager import PipelineManager\n",
    "from src.services.orchestration.job_scheduler import JobScheduler\n",
    "from src.services.orchestration.state_manager import StateManager\n",
    "from src.services.caching.cache_service import CacheService\n",
    "from src.services.caching.redis_cache import RedisCache\n",
    "from src.services.queue.task_queue import TaskQueue\n",
    "from src.services.queue.message_broker import MessageBroker\n",
    "from src.services.monitoring.metrics_service import MetricsService\n",
    "from src.services.monitoring.health_service import HealthService\n",
    "from src.services.notification.notification_service import NotificationService\n",
    "from src.services.storage.storage_service import StorageService\n",
    "from src.utils.service_utils import (\n",
    "    create_service_client,\n",
    "    handle_service_error,\n",
    "    service_health_check\n",
    ")\n",
    "from src.utils.logging_config import setup_logging\n",
    "from configs.config_loader import ConfigLoader\n",
    "from configs.constants import AG_NEWS_CLASSES\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "logger = setup_logging('service_integration_tutorial')\n",
    "\n",
    "# Service Configuration\n",
    "SERVICE_CONFIG = {\n",
    "    'redis_host': 'localhost',\n",
    "    'redis_port': 6379,\n",
    "    'kafka_broker': 'localhost:9092',\n",
    "    'celery_broker': 'redis://localhost:6379/0',\n",
    "    'prometheus_port': 9090,\n",
    "    'service_timeout': 30,\n",
    "    'max_retries': 3\n",
    "}\n",
    "\n",
    "print(\"Service Integration Tutorial\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Redis: {SERVICE_CONFIG['redis_host']}:{SERVICE_CONFIG['redis_port']}\")\n",
    "print(f\"Kafka Broker: {SERVICE_CONFIG['kafka_broker']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Service Registry and Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ServiceInfo:\n",
    "    \"\"\"\n",
    "    Service information for registry.\n",
    "    \n",
    "    Following service discovery patterns from:\n",
    "        Burns (2018): \"Designing Distributed Systems\"\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "    host: str\n",
    "    port: int\n",
    "    protocol: str\n",
    "    status: str = 'unknown'\n",
    "    health_endpoint: str = '/health'\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class ServiceDiscovery:\n",
    "    \"\"\"\n",
    "    Service discovery and registration.\n",
    "    \n",
    "    Following patterns from:\n",
    "        HashiCorp Consul documentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.services: Dict[str, ServiceInfo] = {}\n",
    "        self.registry = ServiceRegistry()\n",
    "    \n",
    "    def register_service(self, service_info: ServiceInfo) -> bool:\n",
    "        \"\"\"Register a service.\"\"\"\n",
    "        try:\n",
    "            # Check if service is healthy\n",
    "            if self._health_check(service_info):\n",
    "                service_info.status = 'healthy'\n",
    "                self.services[service_info.name] = service_info\n",
    "                self.registry.register(service_info.name, service_info)\n",
    "                logger.info(f\"Service registered: {service_info.name}\")\n",
    "                return True\n",
    "            else:\n",
    "                service_info.status = 'unhealthy'\n",
    "                logger.warning(f\"Service unhealthy: {service_info.name}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to register service: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def discover_service(self, service_name: str) -> Optional[ServiceInfo]:\n",
    "        \"\"\"Discover a service by name.\"\"\"\n",
    "        if service_name in self.services:\n",
    "            service = self.services[service_name]\n",
    "            # Check current health\n",
    "            if self._health_check(service):\n",
    "                return service\n",
    "        return None\n",
    "    \n",
    "    def list_services(self, status: Optional[str] = None) -> List[ServiceInfo]:\n",
    "        \"\"\"List all registered services.\"\"\"\n",
    "        services = list(self.services.values())\n",
    "        if status:\n",
    "            services = [s for s in services if s.status == status]\n",
    "        return services\n",
    "    \n",
    "    def _health_check(self, service_info: ServiceInfo) -> bool:\n",
    "        \"\"\"Check service health.\"\"\"\n",
    "        # Simulate health check\n",
    "        import random\n",
    "        return random.random() > 0.1  # 90% healthy\n",
    "\n",
    "\n",
    "# Initialize service discovery\n",
    "service_discovery = ServiceDiscovery()\n",
    "\n",
    "# Register core services\n",
    "core_services = [\n",
    "    ServiceInfo(\n",
    "        name='prediction-service',\n",
    "        version='1.0.0',\n",
    "        host='localhost',\n",
    "        port=8001,\n",
    "        protocol='http',\n",
    "        metadata={'model': 'deberta-v3', 'max_batch_size': 32}\n",
    "    ),\n",
    "    ServiceInfo(\n",
    "        name='training-service',\n",
    "        version='1.0.0',\n",
    "        host='localhost',\n",
    "        port=8002,\n",
    "        protocol='http',\n",
    "        metadata={'gpu_enabled': True, 'max_parallel_jobs': 4}\n",
    "    ),\n",
    "    ServiceInfo(\n",
    "        name='data-service',\n",
    "        version='1.0.0',\n",
    "        host='localhost',\n",
    "        port=8003,\n",
    "        protocol='http',\n",
    "        metadata={'storage_backend': 's3', 'cache_enabled': True}\n",
    "    ),\n",
    "    ServiceInfo(\n",
    "        name='model-management',\n",
    "        version='1.0.0',\n",
    "        host='localhost',\n",
    "        port=8004,\n",
    "        protocol='http',\n",
    "        metadata={'model_registry': 'mlflow', 'versioning': True}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Registering Services:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for service in core_services:\n",
    "    success = service_discovery.register_service(service)\n",
    "    status = \"[OK]\" if success else \"[FAIL]\"\n",
    "    print(f\"{status} {service.name}: {service.host}:{service.port} [{service.status}]\")\n",
    "\n",
    "# List registered services\n",
    "print(\"\\nRegistered Services:\")\n",
    "registered = service_discovery.list_services()\n",
    "for service in registered:\n",
    "    print(f\"  - {service.name} (v{service.version}): {service.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Service Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServiceOrchestrator:\n",
    "    \"\"\"\n",
    "    Orchestrate multiple services for complex workflows.\n",
    "    \n",
    "    Following orchestration patterns from:\n",
    "        Hohpe & Woolf (2003): \"Enterprise Integration Patterns\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, service_discovery: ServiceDiscovery):\n",
    "        self.service_discovery = service_discovery\n",
    "        self.workflows = {}\n",
    "        self.pipeline_manager = PipelineManager()\n",
    "        self.state_manager = StateManager()\n",
    "    \n",
    "    def create_workflow(self, name: str, steps: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Create a service workflow.\"\"\"\n",
    "        workflow_id = f\"workflow_{name}_{int(time.time())}\"\n",
    "        \n",
    "        workflow = {\n",
    "            'id': workflow_id,\n",
    "            'name': name,\n",
    "            'steps': steps,\n",
    "            'status': 'created',\n",
    "            'created_at': time.time()\n",
    "        }\n",
    "        \n",
    "        self.workflows[workflow_id] = workflow\n",
    "        self.state_manager.initialize_state(workflow_id)\n",
    "        \n",
    "        return workflow_id\n",
    "    \n",
    "    async def execute_workflow(self, workflow_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a workflow asynchronously.\"\"\"\n",
    "        workflow = self.workflows.get(workflow_id)\n",
    "        if not workflow:\n",
    "            raise ValueError(f\"Workflow not found: {workflow_id}\")\n",
    "        \n",
    "        workflow['status'] = 'running'\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            for step_idx, step in enumerate(workflow['steps']):\n",
    "                service_name = step['service']\n",
    "                operation = step['operation']\n",
    "                params = step.get('params', {})\n",
    "                \n",
    "                # Discover service\n",
    "                service_info = self.service_discovery.discover_service(service_name)\n",
    "                if not service_info:\n",
    "                    raise Exception(f\"Service not available: {service_name}\")\n",
    "                \n",
    "                # Execute service operation\n",
    "                result = await self._execute_service_operation(\n",
    "                    service_info, operation, params\n",
    "                )\n",
    "                \n",
    "                results[f\"step_{step_idx}\"] = result\n",
    "                self.state_manager.update_state(\n",
    "                    workflow_id, f\"step_{step_idx}\", result\n",
    "                )\n",
    "            \n",
    "            workflow['status'] = 'completed'\n",
    "            workflow['results'] = results\n",
    "            \n",
    "        except Exception as e:\n",
    "            workflow['status'] = 'failed'\n",
    "            workflow['error'] = str(e)\n",
    "            logger.error(f\"Workflow failed: {e}\")\n",
    "        \n",
    "        return workflow\n",
    "    \n",
    "    async def _execute_service_operation(\n",
    "        self,\n",
    "        service_info: ServiceInfo,\n",
    "        operation: str,\n",
    "        params: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a service operation.\"\"\"\n",
    "        # Simulate service call\n",
    "        await asyncio.sleep(0.5)  # Simulate network delay\n",
    "        \n",
    "        return {\n",
    "            'service': service_info.name,\n",
    "            'operation': operation,\n",
    "            'status': 'success',\n",
    "            'result': f\"Processed {operation} on {service_info.name}\",\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator = ServiceOrchestrator(service_discovery)\n",
    "\n",
    "# Define classification workflow\n",
    "classification_workflow = [\n",
    "    {\n",
    "        'service': 'data-service',\n",
    "        'operation': 'load_data',\n",
    "        'params': {'dataset': 'ag_news', 'split': 'test'}\n",
    "    },\n",
    "    {\n",
    "        'service': 'prediction-service',\n",
    "        'operation': 'batch_predict',\n",
    "        'params': {'model_id': 'deberta-v3', 'batch_size': 32}\n",
    "    },\n",
    "    {\n",
    "        'service': 'model-management',\n",
    "        'operation': 'log_predictions',\n",
    "        'params': {'experiment_name': 'tutorial'}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create and execute workflow\n",
    "print(\"\\nWorkflow Orchestration:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "workflow_id = orchestrator.create_workflow(\n",
    "    'classification_pipeline',\n",
    "    classification_workflow\n",
    ")\n",
    "\n",
    "print(f\"Created workflow: {workflow_id}\")\n",
    "print(\"\\nExecuting workflow steps:\")\n",
    "\n",
    "# Execute workflow (simulated)\n",
    "async def run_workflow():\n",
    "    result = await orchestrator.execute_workflow(workflow_id)\n",
    "    return result\n",
    "\n",
    "# Run async workflow\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "if loop.is_running():\n",
    "    # For Jupyter notebooks\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "workflow_result = asyncio.run(run_workflow())\n",
    "\n",
    "# Display results\n",
    "for step_name, step_result in workflow_result.get('results', {}).items():\n",
    "    print(f\"\\n{step_name}:\")\n",
    "    print(f\"  Service: {step_result['service']}\")\n",
    "    print(f\"  Operation: {step_result['operation']}\")\n",
    "    print(f\"  Status: {step_result['status']}\")\n",
    "\n",
    "print(f\"\\nWorkflow Status: {workflow_result['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Message Queue Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageQueueService:\n",
    "    \"\"\"\n",
    "    Message queue service for asynchronous processing.\n",
    "    \n",
    "    Following message queue patterns from:\n",
    "        Gregor & Hohpe (2003): \"Enterprise Integration Patterns\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, broker_url: str):\n",
    "        self.broker_url = broker_url\n",
    "        self.queues = {}\n",
    "        self.consumers = {}\n",
    "        self.message_broker = MessageBroker(broker_url)\n",
    "    \n",
    "    def create_queue(self, queue_name: str, config: Dict[str, Any] = None) -> bool:\n",
    "        \"\"\"Create a message queue.\"\"\"\n",
    "        try:\n",
    "            queue_config = config or {\n",
    "                'durable': True,\n",
    "                'max_priority': 10,\n",
    "                'ttl': 3600\n",
    "            }\n",
    "            \n",
    "            self.queues[queue_name] = {\n",
    "                'name': queue_name,\n",
    "                'config': queue_config,\n",
    "                'messages': [],\n",
    "                'created_at': time.time()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Queue created: {queue_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create queue: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def publish_message(\n",
    "        self,\n",
    "        queue_name: str,\n",
    "        message: Dict[str, Any],\n",
    "        priority: int = 5\n",
    "    ) -> str:\n",
    "        \"\"\"Publish message to queue.\"\"\"\n",
    "        if queue_name not in self.queues:\n",
    "            raise ValueError(f\"Queue not found: {queue_name}\")\n",
    "        \n",
    "        message_id = f\"msg_{int(time.time() * 1000)}\"\n",
    "        \n",
    "        message_wrapper = {\n",
    "            'id': message_id,\n",
    "            'payload': message,\n",
    "            'priority': priority,\n",
    "            'timestamp': time.time(),\n",
    "            'retries': 0\n",
    "        }\n",
    "        \n",
    "        self.queues[queue_name]['messages'].append(message_wrapper)\n",
    "        logger.info(f\"Message published to {queue_name}: {message_id}\")\n",
    "        \n",
    "        return message_id\n",
    "    \n",
    "    def consume_message(self, queue_name: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Consume message from queue.\"\"\"\n",
    "        if queue_name not in self.queues:\n",
    "            raise ValueError(f\"Queue not found: {queue_name}\")\n",
    "        \n",
    "        messages = self.queues[queue_name]['messages']\n",
    "        if messages:\n",
    "            # Sort by priority and timestamp\n",
    "            messages.sort(key=lambda x: (-x['priority'], x['timestamp']))\n",
    "            message = messages.pop(0)\n",
    "            logger.info(f\"Message consumed from {queue_name}: {message['id']}\")\n",
    "            return message\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def register_consumer(\n",
    "        self,\n",
    "        queue_name: str,\n",
    "        handler: callable,\n",
    "        auto_ack: bool = True\n",
    "    ):\n",
    "        \"\"\"Register a message consumer.\"\"\"\n",
    "        if queue_name not in self.queues:\n",
    "            raise ValueError(f\"Queue not found: {queue_name}\")\n",
    "        \n",
    "        consumer = {\n",
    "            'queue': queue_name,\n",
    "            'handler': handler,\n",
    "            'auto_ack': auto_ack,\n",
    "            'active': True\n",
    "        }\n",
    "        \n",
    "        consumer_id = f\"consumer_{len(self.consumers)}\"\n",
    "        self.consumers[consumer_id] = consumer\n",
    "        \n",
    "        return consumer_id\n",
    "\n",
    "\n",
    "# Create message queue service\n",
    "mq_service = MessageQueueService(SERVICE_CONFIG['celery_broker'])\n",
    "\n",
    "# Create queues for different services\n",
    "queues = [\n",
    "    'classification_requests',\n",
    "    'training_jobs',\n",
    "    'data_processing',\n",
    "    'model_updates'\n",
    "]\n",
    "\n",
    "print(\"Message Queue Setup:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for queue_name in queues:\n",
    "    success = mq_service.create_queue(queue_name)\n",
    "    print(f\"Created queue: {queue_name} - {'Success' if success else 'Failed'}\")\n",
    "\n",
    "# Publish test messages\n",
    "print(\"\\nPublishing Messages:\")\n",
    "\n",
    "test_messages = [\n",
    "    {\n",
    "        'queue': 'classification_requests',\n",
    "        'message': {'text': 'Sample news article', 'model_id': 'deberta-v3'},\n",
    "        'priority': 8\n",
    "    },\n",
    "    {\n",
    "        'queue': 'training_jobs',\n",
    "        'message': {'dataset': 'ag_news', 'epochs': 3, 'model_type': 'roberta'},\n",
    "        'priority': 5\n",
    "    },\n",
    "    {\n",
    "        'queue': 'data_processing',\n",
    "        'message': {'action': 'augment', 'dataset_id': 'train_001'},\n",
    "        'priority': 3\n",
    "    }\n",
    "]\n",
    "\n",
    "for msg_config in test_messages:\n",
    "    msg_id = mq_service.publish_message(\n",
    "        msg_config['queue'],\n",
    "        msg_config['message'],\n",
    "        msg_config['priority']\n",
    "    )\n",
    "    print(f\"  Published to {msg_config['queue']}: {msg_id}\")\n",
    "\n",
    "# Consume messages\n",
    "print(\"\\nConsuming Messages:\")\n",
    "\n",
    "for queue_name in ['classification_requests', 'training_jobs']:\n",
    "    message = mq_service.consume_message(queue_name)\n",
    "    if message:\n",
    "        print(f\"\\n{queue_name}:\")\n",
    "        print(f\"  ID: {message['id']}\")\n",
    "        print(f\"  Priority: {message['priority']}\")\n",
    "        print(f\"  Payload: {message['payload']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Caching Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachingService:\n",
    "    \"\"\"\n",
    "    Distributed caching service.\n",
    "    \n",
    "    Following caching strategies from:\n",
    "        Fitzpatrick (2004): \"Distributed Caching with Memcached\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_config: Dict[str, Any]):\n",
    "        self.config = cache_config\n",
    "        self.cache_stores = {}\n",
    "        self.stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0\n",
    "        }\n",
    "        self._initialize_stores()\n",
    "    \n",
    "    def _initialize_stores(self):\n",
    "        \"\"\"Initialize cache stores.\"\"\"\n",
    "        # Create different cache levels\n",
    "        self.cache_stores['L1'] = {}  # In-memory cache\n",
    "        self.cache_stores['L2'] = {}  # Redis cache (simulated)\n",
    "        self.cache_stores['L3'] = {}  # Disk cache (simulated)\n",
    "    \n",
    "    def get(\n",
    "        self,\n",
    "        key: str,\n",
    "        cache_level: str = 'L1'\n",
    "    ) -> Optional[Any]:\n",
    "        \"\"\"Get value from cache.\"\"\"\n",
    "        if cache_level in self.cache_stores:\n",
    "            if key in self.cache_stores[cache_level]:\n",
    "                self.stats['hits'] += 1\n",
    "                entry = self.cache_stores[cache_level][key]\n",
    "                \n",
    "                # Check TTL\n",
    "                if time.time() < entry['expires_at']:\n",
    "                    # Update access time\n",
    "                    entry['last_accessed'] = time.time()\n",
    "                    entry['access_count'] += 1\n",
    "                    return entry['value']\n",
    "                else:\n",
    "                    # Expired, remove from cache\n",
    "                    self._evict(key, cache_level)\n",
    "        \n",
    "        self.stats['misses'] += 1\n",
    "        return None\n",
    "    \n",
    "    def set(\n",
    "        self,\n",
    "        key: str,\n",
    "        value: Any,\n",
    "        ttl: int = 3600,\n",
    "        cache_level: str = 'L1'\n",
    "    ) -> bool:\n",
    "        \"\"\"Set value in cache.\"\"\"\n",
    "        if cache_level not in self.cache_stores:\n",
    "            return False\n",
    "        \n",
    "        entry = {\n",
    "            'value': value,\n",
    "            'created_at': time.time(),\n",
    "            'expires_at': time.time() + ttl,\n",
    "            'last_accessed': time.time(),\n",
    "            'access_count': 0,\n",
    "            'size': sys.getsizeof(value)\n",
    "        }\n",
    "        \n",
    "        self.cache_stores[cache_level][key] = entry\n",
    "        \n",
    "        # Check cache size and evict if necessary\n",
    "        self._check_cache_size(cache_level)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def invalidate(self, key: str, cache_level: Optional[str] = None) -> bool:\n",
    "        \"\"\"Invalidate cache entry.\"\"\"\n",
    "        invalidated = False\n",
    "        \n",
    "        if cache_level:\n",
    "            if cache_level in self.cache_stores and key in self.cache_stores[cache_level]:\n",
    "                del self.cache_stores[cache_level][key]\n",
    "                invalidated = True\n",
    "        else:\n",
    "            # Invalidate from all levels\n",
    "            for level in self.cache_stores:\n",
    "                if key in self.cache_stores[level]:\n",
    "                    del self.cache_stores[level][key]\n",
    "                    invalidated = True\n",
    "        \n",
    "        return invalidated\n",
    "    \n",
    "    def _evict(self, key: str, cache_level: str):\n",
    "        \"\"\"Evict entry from cache.\"\"\"\n",
    "        if key in self.cache_stores[cache_level]:\n",
    "            del self.cache_stores[cache_level][key]\n",
    "            self.stats['evictions'] += 1\n",
    "    \n",
    "    def _check_cache_size(self, cache_level: str):\n",
    "        \"\"\"Check and manage cache size.\"\"\"\n",
    "        max_entries = {'L1': 100, 'L2': 1000, 'L3': 10000}\n",
    "        \n",
    "        if len(self.cache_stores[cache_level]) > max_entries.get(cache_level, 100):\n",
    "            # LRU eviction\n",
    "            entries = self.cache_stores[cache_level]\n",
    "            sorted_keys = sorted(\n",
    "                entries.keys(),\n",
    "                key=lambda k: entries[k]['last_accessed']\n",
    "            )\n",
    "            \n",
    "            # Evict oldest 10%\n",
    "            evict_count = len(sorted_keys) // 10\n",
    "            for key in sorted_keys[:evict_count]:\n",
    "                self._evict(key, cache_level)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total_ops = self.stats['hits'] + self.stats['misses']\n",
    "        hit_rate = self.stats['hits'] / total_ops if total_ops > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'hits': self.stats['hits'],\n",
    "            'misses': self.stats['misses'],\n",
    "            'hit_rate': hit_rate,\n",
    "            'evictions': self.stats['evictions'],\n",
    "            'cache_sizes': {\n",
    "                level: len(store) for level, store in self.cache_stores.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize caching service\n",
    "cache_config = {\n",
    "    'redis_host': SERVICE_CONFIG['redis_host'],\n",
    "    'redis_port': SERVICE_CONFIG['redis_port'],\n",
    "    'default_ttl': 3600,\n",
    "    'max_memory': '1GB'\n",
    "}\n",
    "\n",
    "cache_service = CachingService(cache_config)\n",
    "\n",
    "print(\"Caching Service Demo:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test caching operations\n",
    "test_data = [\n",
    "    ('model_predictions_1', {'predictions': [0, 1, 2, 3], 'confidence': [0.9, 0.8, 0.95, 0.7]}),\n",
    "    ('dataset_meta', {'name': 'ag_news', 'size': 120000, 'classes': 4}),\n",
    "    ('feature_cache_1', np.random.randn(100, 768).tolist()),\n",
    "]\n",
    "\n",
    "# Set cache entries\n",
    "print(\"Setting cache entries:\")\n",
    "for key, value in test_data:\n",
    "    success = cache_service.set(key, value, ttl=1800)\n",
    "    print(f\"  {key}: {'Cached' if success else 'Failed'}\")\n",
    "\n",
    "# Test cache hits and misses\n",
    "print(\"\\nTesting cache access:\")\n",
    "test_keys = ['model_predictions_1', 'dataset_meta', 'non_existent_key']\n",
    "\n",
    "for key in test_keys:\n",
    "    value = cache_service.get(key)\n",
    "    status = 'HIT' if value is not None else 'MISS'\n",
    "    print(f\"  {key}: {status}\")\n",
    "\n",
    "# Multi-level caching\n",
    "print(\"\\nMulti-level caching:\")\n",
    "cache_service.set('hot_data', {'value': 'frequently_accessed'}, cache_level='L1')\n",
    "cache_service.set('warm_data', {'value': 'occasionally_accessed'}, cache_level='L2')\n",
    "cache_service.set('cold_data', {'value': 'rarely_accessed'}, cache_level='L3')\n",
    "\n",
    "# Display cache statistics\n",
    "stats = cache_service.get_stats()\n",
    "print(\"\\nCache Statistics:\")\n",
    "print(f\"  Hits: {stats['hits']}\")\n",
    "print(f\"  Misses: {stats['misses']}\")\n",
    "print(f\"  Hit Rate: {stats['hit_rate']:.2%}\")\n",
    "print(f\"  Evictions: {stats['evictions']}\")\n",
    "print(\"\\nCache Sizes:\")\n",
    "for level, size in stats['cache_sizes'].items():\n",
    "    print(f\"  {level}: {size} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Storage Service Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StorageServiceIntegration:\n",
    "    \"\"\"\n",
    "    Unified storage service for multiple backends.\n",
    "    \n",
    "    Following storage patterns from:\n",
    "        AWS Well-Architected Framework - Storage Pillar\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.backends = {}\n",
    "        self.default_backend = 'local'\n",
    "        self._initialize_backends()\n",
    "    \n",
    "    def _initialize_backends(self):\n",
    "        \"\"\"Initialize storage backends.\"\"\"\n",
    "        # Local file storage\n",
    "        self.backends['local'] = {\n",
    "            'type': 'filesystem',\n",
    "            'base_path': PROJECT_ROOT / 'data',\n",
    "            'available': True\n",
    "        }\n",
    "        \n",
    "        # S3 storage (simulated)\n",
    "        self.backends['s3'] = {\n",
    "            'type': 's3',\n",
    "            'bucket': 'ag-news-models',\n",
    "            'region': 'us-west-2',\n",
    "            'available': False  # Would be True if configured\n",
    "        }\n",
    "        \n",
    "        # GCS storage (simulated)\n",
    "        self.backends['gcs'] = {\n",
    "            'type': 'gcs',\n",
    "            'bucket': 'ag-news-storage',\n",
    "            'project': 'ml-project',\n",
    "            'available': False  # Would be True if configured\n",
    "        }\n",
    "    \n",
    "    def store(\n",
    "        self,\n",
    "        key: str,\n",
    "        data: Any,\n",
    "        backend: Optional[str] = None,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Store data in specified backend.\"\"\"\n",
    "        backend = backend or self.default_backend\n",
    "        \n",
    "        if backend not in self.backends:\n",
    "            raise ValueError(f\"Unknown backend: {backend}\")\n",
    "        \n",
    "        if not self.backends[backend]['available']:\n",
    "            logger.warning(f\"Backend {backend} not available, using local\")\n",
    "            backend = 'local'\n",
    "        \n",
    "        # Simulate storage operation\n",
    "        storage_info = {\n",
    "            'key': key,\n",
    "            'backend': backend,\n",
    "            'size': sys.getsizeof(data),\n",
    "            'timestamp': time.time(),\n",
    "            'metadata': metadata or {},\n",
    "            'location': self._get_storage_location(backend, key)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Stored {key} in {backend}\")\n",
    "        return storage_info\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        key: str,\n",
    "        backend: Optional[str] = None\n",
    "    ) -> Optional[Any]:\n",
    "        \"\"\"Retrieve data from storage.\"\"\"\n",
    "        backend = backend or self.default_backend\n",
    "        \n",
    "        # Simulate retrieval\n",
    "        logger.info(f\"Retrieved {key} from {backend}\")\n",
    "        \n",
    "        # Return simulated data\n",
    "        return {'data': f\"Retrieved {key}\", 'backend': backend}\n",
    "    \n",
    "    def list_objects(\n",
    "        self,\n",
    "        prefix: str = '',\n",
    "        backend: Optional[str] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"List objects in storage.\"\"\"\n",
    "        backend = backend or self.default_backend\n",
    "        \n",
    "        # Simulate object listing\n",
    "        objects = [\n",
    "            {'key': f'{prefix}model_v1.pt', 'size': 524288000, 'modified': time.time() - 86400},\n",
    "            {'key': f'{prefix}model_v2.pt', 'size': 524288000, 'modified': time.time() - 3600},\n",
    "            {'key': f'{prefix}dataset.json', 'size': 10485760, 'modified': time.time() - 7200},\n",
    "        ]\n",
    "        \n",
    "        return [obj for obj in objects if obj['key'].startswith(prefix)]\n",
    "    \n",
    "    def _get_storage_location(self, backend: str, key: str) -> str:\n",
    "        \"\"\"Get storage location URL.\"\"\"\n",
    "        if backend == 'local':\n",
    "            return f\"file://{self.backends[backend]['base_path']}/{key}\"\n",
    "        elif backend == 's3':\n",
    "            return f\"s3://{self.backends[backend]['bucket']}/{key}\"\n",
    "        elif backend == 'gcs':\n",
    "            return f\"gs://{self.backends[backend]['bucket']}/{key}\"\n",
    "        else:\n",
    "            return f\"{backend}://{key}\"\n",
    "    \n",
    "    def get_storage_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get storage statistics.\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for backend_name, backend_config in self.backends.items():\n",
    "            stats[backend_name] = {\n",
    "                'type': backend_config['type'],\n",
    "                'available': backend_config['available'],\n",
    "                'objects_count': len(self.list_objects(backend=backend_name)),\n",
    "                'total_size': sum(\n",
    "                    obj['size'] for obj in self.list_objects(backend=backend_name)\n",
    "                )\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "# Initialize storage service\n",
    "storage_service = StorageServiceIntegration()\n",
    "\n",
    "print(\"Storage Service Integration:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Store different types of data\n",
    "storage_operations = [\n",
    "    {\n",
    "        'key': 'models/deberta_v3_final.pt',\n",
    "        'data': {'model_weights': 'simulated_weights'},\n",
    "        'backend': 'local',\n",
    "        'metadata': {'accuracy': 0.95, 'version': '1.0'}\n",
    "    },\n",
    "    {\n",
    "        'key': 'datasets/ag_news_processed.json',\n",
    "        'data': {'dataset': 'processed_data'},\n",
    "        'backend': 'local',\n",
    "        'metadata': {'samples': 120000, 'format': 'json'}\n",
    "    },\n",
    "    {\n",
    "        'key': 'checkpoints/epoch_5.pt',\n",
    "        'data': {'checkpoint': 'training_state'},\n",
    "        'backend': 's3',  # Will fallback to local\n",
    "        'metadata': {'epoch': 5, 'loss': 0.234}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Storing objects:\")\n",
    "for op in storage_operations:\n",
    "    result = storage_service.store(\n",
    "        op['key'],\n",
    "        op['data'],\n",
    "        op['backend'],\n",
    "        op['metadata']\n",
    "    )\n",
    "    print(f\"  {result['key']}: {result['location']}\")\n",
    "\n",
    "# List stored objects\n",
    "print(\"\\nListing objects:\")\n",
    "for prefix in ['models/', 'datasets/', 'checkpoints/']:\n",
    "    objects = storage_service.list_objects(prefix)\n",
    "    print(f\"\\n{prefix}\")\n",
    "    for obj in objects:\n",
    "        size_mb = obj['size'] / (1024 * 1024)\n",
    "        print(f\"  {obj['key']}: {size_mb:.1f} MB\")\n",
    "\n",
    "# Storage statistics\n",
    "stats = storage_service.get_storage_stats()\n",
    "print(\"\\nStorage Statistics:\")\n",
    "for backend, backend_stats in stats.items():\n",
    "    print(f\"\\n{backend}:\")\n",
    "    print(f\"  Type: {backend_stats['type']}\")\n",
    "    print(f\"  Available: {backend_stats['available']}\")\n",
    "    print(f\"  Objects: {backend_stats['objects_count']}\")\n",
    "    print(f\"  Total Size: {backend_stats['total_size'] / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitoring and Alerting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitoringService:\n",
    "    \"\"\"\n",
    "    Comprehensive monitoring and alerting service.\n",
    "    \n",
    "    Following monitoring practices from:\n",
    "        Google SRE Book - \"Monitoring Distributed Systems\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        self.alerts = []\n",
    "        self.thresholds = self._default_thresholds()\n",
    "        self._initialize_metrics()\n",
    "    \n",
    "    def _initialize_metrics(self):\n",
    "        \"\"\"Initialize metric collectors.\"\"\"\n",
    "        self.metrics['service_health'] = {}\n",
    "        self.metrics['performance'] = {}\n",
    "        self.metrics['errors'] = {}\n",
    "        self.metrics['business'] = {}\n",
    "    \n",
    "    def _default_thresholds(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Define default alert thresholds.\"\"\"\n",
    "        return {\n",
    "            'latency': {'warning': 500, 'critical': 1000},  # ms\n",
    "            'error_rate': {'warning': 0.01, 'critical': 0.05},\n",
    "            'cpu_usage': {'warning': 70, 'critical': 90},  # %\n",
    "            'memory_usage': {'warning': 80, 'critical': 95},  # %\n",
    "            'queue_depth': {'warning': 100, 'critical': 500}\n",
    "        }\n",
    "    \n",
    "    def record_metric(\n",
    "        self,\n",
    "        category: str,\n",
    "        name: str,\n",
    "        value: float,\n",
    "        tags: Optional[Dict[str, str]] = None\n",
    "    ):\n",
    "        \"\"\"Record a metric value.\"\"\"\n",
    "        if category not in self.metrics:\n",
    "            self.metrics[category] = {}\n",
    "        \n",
    "        if name not in self.metrics[category]:\n",
    "            self.metrics[category][name] = []\n",
    "        \n",
    "        metric_point = {\n",
    "            'value': value,\n",
    "            'timestamp': time.time(),\n",
    "            'tags': tags or {}\n",
    "        }\n",
    "        \n",
    "        self.metrics[category][name].append(metric_point)\n",
    "        \n",
    "        # Check thresholds\n",
    "        self._check_thresholds(name, value)\n",
    "    \n",
    "    def _check_thresholds(self, metric_name: str, value: float):\n",
    "        \"\"\"Check if metric exceeds thresholds.\"\"\"\n",
    "        if metric_name in self.thresholds:\n",
    "            thresholds = self.thresholds[metric_name]\n",
    "            \n",
    "            if value >= thresholds['critical']:\n",
    "                self._create_alert(\n",
    "                    'CRITICAL',\n",
    "                    metric_name,\n",
    "                    value,\n",
    "                    thresholds['critical']\n",
    "                )\n",
    "            elif value >= thresholds['warning']:\n",
    "                self._create_alert(\n",
    "                    'WARNING',\n",
    "                    metric_name,\n",
    "                    value,\n",
    "                    thresholds['warning']\n",
    "                )\n",
    "    \n",
    "    def _create_alert(self, severity: str, metric: str, value: float, threshold: float):\n",
    "        \"\"\"Create an alert.\"\"\"\n",
    "        alert = {\n",
    "            'id': f\"alert_{int(time.time() * 1000)}\",\n",
    "            'severity': severity,\n",
    "            'metric': metric,\n",
    "            'value': value,\n",
    "            'threshold': threshold,\n",
    "            'timestamp': time.time(),\n",
    "            'status': 'active'\n",
    "        }\n",
    "        \n",
    "        self.alerts.append(alert)\n",
    "        logger.warning(\n",
    "            f\"{severity} Alert: {metric} = {value:.2f} (threshold: {threshold})\"\n",
    "        )\n",
    "    \n",
    "    def get_metrics_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of all metrics.\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for category, metrics in self.metrics.items():\n",
    "            summary[category] = {}\n",
    "            \n",
    "            for metric_name, points in metrics.items():\n",
    "                if points:\n",
    "                    values = [p['value'] for p in points[-100:]]  # Last 100 points\n",
    "                    summary[category][metric_name] = {\n",
    "                        'current': values[-1] if values else 0,\n",
    "                        'avg': np.mean(values) if values else 0,\n",
    "                        'min': np.min(values) if values else 0,\n",
    "                        'max': np.max(values) if values else 0,\n",
    "                        'p95': np.percentile(values, 95) if values else 0\n",
    "                    }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def get_active_alerts(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get active alerts.\"\"\"\n",
    "        return [alert for alert in self.alerts if alert['status'] == 'active']\n",
    "\n",
    "\n",
    "# Initialize monitoring service\n",
    "monitoring = MonitoringService()\n",
    "\n",
    "print(\"Monitoring Service:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simulate metric collection\n",
    "print(\"Recording metrics:\")\n",
    "\n",
    "# Service metrics\n",
    "service_metrics = [\n",
    "    ('performance', 'latency', 250, {'service': 'prediction'}),\n",
    "    ('performance', 'latency', 750, {'service': 'training'}),  # Will trigger warning\n",
    "    ('performance', 'throughput', 150, {'service': 'prediction'}),\n",
    "    ('errors', 'error_rate', 0.008, {'service': 'api'}),\n",
    "    ('errors', 'error_rate', 0.06, {'service': 'storage'}),  # Will trigger critical\n",
    "    ('service_health', 'cpu_usage', 65, {'node': 'worker-1'}),\n",
    "    ('service_health', 'memory_usage', 72, {'node': 'worker-1'}),\n",
    "    ('business', 'predictions_per_minute', 450, {}),\n",
    "    ('business', 'models_trained', 5, {})\n",
    "]\n",
    "\n",
    "for category, name, value, tags in service_metrics:\n",
    "    monitoring.record_metric(category, name, value, tags)\n",
    "    print(f\"  {category}.{name}: {value}\")\n",
    "\n",
    "# Get metrics summary\n",
    "summary = monitoring.get_metrics_summary()\n",
    "\n",
    "print(\"\\nMetrics Summary:\")\n",
    "for category, metrics in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric_name, stats in metrics.items():\n",
    "        print(f\"  {metric_name}:\")\n",
    "        print(f\"    Current: {stats['current']:.2f}\")\n",
    "        print(f\"    Average: {stats['avg']:.2f}\")\n",
    "        print(f\"    P95: {stats['p95']:.2f}\")\n",
    "\n",
    "# Display active alerts\n",
    "active_alerts = monitoring.get_active_alerts()\n",
    "\n",
    "print(\"\\nActive Alerts:\")\n",
    "if active_alerts:\n",
    "    for alert in active_alerts:\n",
    "        print(f\"  [{alert['severity']}] {alert['metric']}: {alert['value']:.2f} > {alert['threshold']}\")\n",
    "else:\n",
    "    print(\"  No active alerts\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Metric categories\n",
    "categories = list(summary.keys())\n",
    "metric_counts = [len(metrics) for metrics in summary.values()]\n",
    "\n",
    "axes[0].bar(categories, metric_counts)\n",
    "axes[0].set_xlabel('Category')\n",
    "axes[0].set_ylabel('Number of Metrics')\n",
    "axes[0].set_title('Metrics by Category')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Alert severity distribution\n",
    "severities = ['WARNING', 'CRITICAL']\n",
    "severity_counts = [\n",
    "    sum(1 for a in active_alerts if a['severity'] == s)\n",
    "    for s in severities\n",
    "]\n",
    "\n",
    "axes[1].bar(severities, severity_counts, color=['orange', 'red'])\n",
    "axes[1].set_xlabel('Severity')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Active Alerts by Severity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Notification Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotificationServiceIntegration:\n",
    "    \"\"\"\n",
    "    Multi-channel notification service.\n",
    "    \n",
    "    Following notification patterns from:\n",
    "        Martin Fowler - \"Event Notification Pattern\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.channels = {}\n",
    "        self.subscriptions = {}\n",
    "        self.notification_history = []\n",
    "        self._initialize_channels()\n",
    "    \n",
    "    def _initialize_channels(self):\n",
    "        \"\"\"Initialize notification channels.\"\"\"\n",
    "        self.channels['email'] = {\n",
    "            'type': 'email',\n",
    "            'enabled': True,\n",
    "            'config': {'smtp_server': 'localhost', 'port': 587}\n",
    "        }\n",
    "        \n",
    "        self.channels['slack'] = {\n",
    "            'type': 'slack',\n",
    "            'enabled': True,\n",
    "            'config': {'webhook_url': 'https://hooks.slack.com/...'}\n",
    "        }\n",
    "        \n",
    "        self.channels['webhook'] = {\n",
    "            'type': 'webhook',\n",
    "            'enabled': True,\n",
    "            'config': {'endpoints': []}\n",
    "        }\n",
    "        \n",
    "        self.channels['sms'] = {\n",
    "            'type': 'sms',\n",
    "            'enabled': False,\n",
    "            'config': {'provider': 'twilio'}\n",
    "        }\n",
    "    \n",
    "    def subscribe(\n",
    "        self,\n",
    "        event_type: str,\n",
    "        channel: str,\n",
    "        recipient: str,\n",
    "        filters: Optional[Dict[str, Any]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Subscribe to notifications.\"\"\"\n",
    "        subscription_id = f\"sub_{int(time.time() * 1000)}\"\n",
    "        \n",
    "        subscription = {\n",
    "            'id': subscription_id,\n",
    "            'event_type': event_type,\n",
    "            'channel': channel,\n",
    "            'recipient': recipient,\n",
    "            'filters': filters or {},\n",
    "            'created_at': time.time(),\n",
    "            'active': True\n",
    "        }\n",
    "        \n",
    "        if event_type not in self.subscriptions:\n",
    "            self.subscriptions[event_type] = []\n",
    "        \n",
    "        self.subscriptions[event_type].append(subscription)\n",
    "        \n",
    "        return subscription_id\n",
    "    \n",
    "    def notify(\n",
    "        self,\n",
    "        event_type: str,\n",
    "        title: str,\n",
    "        message: str,\n",
    "        severity: str = 'info',\n",
    "        data: Optional[Dict[str, Any]] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Send notifications for an event.\"\"\"\n",
    "        notification_ids = []\n",
    "        \n",
    "        # Get relevant subscriptions\n",
    "        subscriptions = self.subscriptions.get(event_type, [])\n",
    "        \n",
    "        for subscription in subscriptions:\n",
    "            if not subscription['active']:\n",
    "                continue\n",
    "            \n",
    "            # Apply filters\n",
    "            if not self._apply_filters(subscription['filters'], data):\n",
    "                continue\n",
    "            \n",
    "            # Send notification\n",
    "            notification_id = self._send_notification(\n",
    "                subscription['channel'],\n",
    "                subscription['recipient'],\n",
    "                title,\n",
    "                message,\n",
    "                severity,\n",
    "                data\n",
    "            )\n",
    "            \n",
    "            notification_ids.append(notification_id)\n",
    "        \n",
    "        return notification_ids\n",
    "    \n",
    "    def _send_notification(\n",
    "        self,\n",
    "        channel: str,\n",
    "        recipient: str,\n",
    "        title: str,\n",
    "        message: str,\n",
    "        severity: str,\n",
    "        data: Optional[Dict[str, Any]]\n",
    "    ) -> str:\n",
    "        \"\"\"Send notification through specified channel.\"\"\"\n",
    "        notification_id = f\"notif_{int(time.time() * 1000)}\"\n",
    "        \n",
    "        notification = {\n",
    "            'id': notification_id,\n",
    "            'channel': channel,\n",
    "            'recipient': recipient,\n",
    "            'title': title,\n",
    "            'message': message,\n",
    "            'severity': severity,\n",
    "            'data': data,\n",
    "            'timestamp': time.time(),\n",
    "            'status': 'sent'\n",
    "        }\n",
    "        \n",
    "        self.notification_history.append(notification)\n",
    "        \n",
    "        # Simulate sending\n",
    "        logger.info(f\"Notification sent via {channel} to {recipient}\")\n",
    "        \n",
    "        return notification_id\n",
    "    \n",
    "    def _apply_filters(\n",
    "        self,\n",
    "        filters: Dict[str, Any],\n",
    "        data: Optional[Dict[str, Any]]\n",
    "    ) -> bool:\n",
    "        \"\"\"Apply subscription filters.\"\"\"\n",
    "        if not filters:\n",
    "            return True\n",
    "        \n",
    "        if not data:\n",
    "            return False\n",
    "        \n",
    "        for key, value in filters.items():\n",
    "            if key not in data or data[key] != value:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_notification_history(\n",
    "        self,\n",
    "        limit: int = 10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get notification history.\"\"\"\n",
    "        return self.notification_history[-limit:]\n",
    "\n",
    "\n",
    "# Initialize notification service\n",
    "notification_service = NotificationServiceIntegration()\n",
    "\n",
    "print(\"Notification Service:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Setup subscriptions\n",
    "subscriptions = [\n",
    "    ('model_training_complete', 'email', 'admin@example.com', None),\n",
    "    ('model_training_complete', 'slack', '#ml-team', None),\n",
    "    ('error_alert', 'email', 'oncall@example.com', {'severity': 'critical'}),\n",
    "    ('error_alert', 'slack', '#alerts', None),\n",
    "    ('prediction_batch_complete', 'webhook', 'https://api.example.com/webhook', None)\n",
    "]\n",
    "\n",
    "print(\"Creating subscriptions:\")\n",
    "for event, channel, recipient, filters in subscriptions:\n",
    "    sub_id = notification_service.subscribe(event, channel, recipient, filters)\n",
    "    print(f\"  {event} -> {channel}: {recipient}\")\n",
    "\n",
    "# Send notifications\n",
    "print(\"\\nSending notifications:\")\n",
    "\n",
    "# Model training complete\n",
    "notif_ids = notification_service.notify(\n",
    "    'model_training_complete',\n",
    "    'Training Complete',\n",
    "    'DeBERTa-v3 training completed successfully',\n",
    "    'success',\n",
    "    {'model': 'deberta-v3', 'accuracy': 0.95, 'duration': '2h 15m'}\n",
    ")\n",
    "print(f\"  Sent {len(notif_ids)} notifications for training completion\")\n",
    "\n",
    "# Error alert\n",
    "notif_ids = notification_service.notify(\n",
    "    'error_alert',\n",
    "    'Critical Error',\n",
    "    'Storage service experiencing high error rate',\n",
    "    'critical',\n",
    "    {'service': 'storage', 'error_rate': 0.06, 'severity': 'critical'}\n",
    ")\n",
    "print(f\"  Sent {len(notif_ids)} notifications for error alert\")\n",
    "\n",
    "# Get notification history\n",
    "history = notification_service.get_notification_history(limit=5)\n",
    "\n",
    "print(\"\\nNotification History:\")\n",
    "for notif in history:\n",
    "    print(f\"\\n  [{notif['severity'].upper()}] {notif['title']}\")\n",
    "    print(f\"    Channel: {notif['channel']}\")\n",
    "    print(f\"    Recipient: {notif['recipient']}\")\n",
    "    print(f\"    Status: {notif['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Service Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServicePipelineIntegration:\n",
    "    \"\"\"\n",
    "    End-to-end service pipeline integration.\n",
    "    \n",
    "    Following pipeline patterns from:\n",
    "        Fowler (2014): \"Microservices\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        service_discovery: ServiceDiscovery,\n",
    "        orchestrator: ServiceOrchestrator,\n",
    "        cache_service: CachingService,\n",
    "        monitoring: MonitoringService\n",
    "    ):\n",
    "        self.service_discovery = service_discovery\n",
    "        self.orchestrator = orchestrator\n",
    "        self.cache_service = cache_service\n",
    "        self.monitoring = monitoring\n",
    "        self.pipelines = {}\n",
    "    \n",
    "    def create_pipeline(\n",
    "        self,\n",
    "        name: str,\n",
    "        stages: List[Dict[str, Any]]\n",
    "    ) -> str:\n",
    "        \"\"\"Create an integrated service pipeline.\"\"\"\n",
    "        pipeline_id = f\"pipeline_{name}_{int(time.time())}\"\n",
    "        \n",
    "        pipeline = {\n",
    "            'id': pipeline_id,\n",
    "            'name': name,\n",
    "            'stages': stages,\n",
    "            'status': 'created',\n",
    "            'created_at': time.time()\n",
    "        }\n",
    "        \n",
    "        self.pipelines[pipeline_id] = pipeline\n",
    "        return pipeline_id\n",
    "    \n",
    "    async def execute_pipeline(\n",
    "        self,\n",
    "        pipeline_id: str,\n",
    "        input_data: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Execute integrated pipeline.\"\"\"\n",
    "        pipeline = self.pipelines.get(pipeline_id)\n",
    "        if not pipeline:\n",
    "            raise ValueError(f\"Pipeline not found: {pipeline_id}\")\n",
    "        \n",
    "        pipeline['status'] = 'running'\n",
    "        stage_results = {}\n",
    "        current_data = input_data\n",
    "        \n",
    "        for stage_idx, stage in enumerate(pipeline['stages']):\n",
    "            stage_name = stage['name']\n",
    "            \n",
    "            # Check cache\n",
    "            cache_key = f\"{pipeline_id}_{stage_name}_{hash(str(current_data))}\"\n",
    "            cached_result = self.cache_service.get(cache_key)\n",
    "            \n",
    "            if cached_result:\n",
    "                self.monitoring.record_metric(\n",
    "                    'pipeline', 'cache_hits', 1,\n",
    "                    {'pipeline': pipeline['name'], 'stage': stage_name}\n",
    "                )\n",
    "                stage_result = cached_result\n",
    "            else:\n",
    "                # Execute stage\n",
    "                stage_result = await self._execute_stage(\n",
    "                    stage, current_data\n",
    "                )\n",
    "                \n",
    "                # Cache result\n",
    "                self.cache_service.set(cache_key, stage_result, ttl=3600)\n",
    "            \n",
    "            stage_results[stage_name] = stage_result\n",
    "            current_data = stage_result.get('output', current_data)\n",
    "        \n",
    "        pipeline['status'] = 'completed'\n",
    "        pipeline['results'] = stage_results\n",
    "        \n",
    "        return pipeline\n",
    "    \n",
    "    async def _execute_stage(\n",
    "        self,\n",
    "        stage: Dict[str, Any],\n",
    "        input_data: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a pipeline stage.\"\"\"\n",
    "        # Simulate stage execution\n",
    "        await asyncio.sleep(0.2)\n",
    "        \n",
    "        return {\n",
    "            'stage': stage['name'],\n",
    "            'input': input_data,\n",
    "            'output': {'processed': True, 'data': f\"Processed {stage['name']}\"},\n",
    "            'metrics': {'latency': 200, 'items_processed': 100}\n",
    "        }\n",
    "\n",
    "\n",
    "# Create integrated pipeline\n",
    "pipeline_integration = ServicePipelineIntegration(\n",
    "    service_discovery,\n",
    "    orchestrator,\n",
    "    cache_service,\n",
    "    monitoring\n",
    ")\n",
    "\n",
    "# Define classification pipeline\n",
    "classification_pipeline_stages = [\n",
    "    {'name': 'data_ingestion', 'service': 'data-service'},\n",
    "    {'name': 'preprocessing', 'service': 'data-service'},\n",
    "    {'name': 'feature_extraction', 'service': 'prediction-service'},\n",
    "    {'name': 'model_inference', 'service': 'prediction-service'},\n",
    "    {'name': 'postprocessing', 'service': 'prediction-service'},\n",
    "    {'name': 'result_storage', 'service': 'storage-service'}\n",
    "]\n",
    "\n",
    "print(\"Service Pipeline Integration:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline_id = pipeline_integration.create_pipeline(\n",
    "    'classification_pipeline',\n",
    "    classification_pipeline_stages\n",
    ")\n",
    "\n",
    "print(f\"Created pipeline: {pipeline_id}\")\n",
    "\n",
    "# Execute pipeline\n",
    "async def run_pipeline():\n",
    "    input_data = {\n",
    "        'texts': ['Sample news article'],\n",
    "        'model_id': 'deberta-v3'\n",
    "    }\n",
    "    \n",
    "    result = await pipeline_integration.execute_pipeline(\n",
    "        pipeline_id,\n",
    "        input_data\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run pipeline\n",
    "pipeline_result = asyncio.run(run_pipeline())\n",
    "\n",
    "print(f\"\\nPipeline Status: {pipeline_result['status']}\")\n",
    "print(\"\\nStage Results:\")\n",
    "\n",
    "for stage_name, result in pipeline_result['results'].items():\n",
    "    print(f\"\\n  {stage_name}:\")\n",
    "    print(f\"    Output: {result['output']}\")\n",
    "    if 'metrics' in result:\n",
    "        print(f\"    Metrics: {result['metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Service Health Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive service health dashboard\n",
    "print(\"Service Health Dashboard:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Service status\n",
    "print(\"\\n[SERVICE STATUS]\")\n",
    "print(\"-\"*70)\n",
    "services = service_discovery.list_services()\n",
    "for service in services:\n",
    "    status_icon = \"[HEALTHY]\" if service.status == 'healthy' else \"[UNHEALTHY]\"\n",
    "    print(f\"{status_icon} {service.name:20} {service.status:10} {service.host}:{service.port}\")\n",
    "\n",
    "# Queue status\n",
    "print(\"\\n[MESSAGE QUEUES]\")\n",
    "print(\"-\"*70)\n",
    "for queue_name, queue_data in mq_service.queues.items():\n",
    "    message_count = len(queue_data['messages'])\n",
    "    print(f\"  {queue_name:25} Messages: {message_count:3}\")\n",
    "\n",
    "# Cache statistics\n",
    "cache_stats = cache_service.get_stats()\n",
    "print(\"\\n[CACHE PERFORMANCE]\")\n",
    "print(\"-\"*70)\n",
    "print(f\"  Hit Rate:     {cache_stats['hit_rate']:.1%}\")\n",
    "print(f\"  Total Hits:   {cache_stats['hits']}\")\n",
    "print(f\"  Total Misses: {cache_stats['misses']}\")\n",
    "print(f\"  Evictions:    {cache_stats['evictions']}\")\n",
    "\n",
    "# Monitoring metrics\n",
    "metrics_summary = monitoring.get_metrics_summary()\n",
    "print(\"\\n[SYSTEM METRICS]\")\n",
    "print(\"-\"*70)\n",
    "if 'performance' in metrics_summary:\n",
    "    for metric, stats in metrics_summary['performance'].items():\n",
    "        print(f\"  {metric:20} Current: {stats['current']:8.2f}  P95: {stats['p95']:8.2f}\")\n",
    "\n",
    "# Active alerts\n",
    "active_alerts = monitoring.get_active_alerts()\n",
    "print(\"\\n[ACTIVE ALERTS]\")\n",
    "print(\"-\"*70)\n",
    "if active_alerts:\n",
    "    for alert in active_alerts:\n",
    "        severity_indicator = \"[CRITICAL]\" if alert['severity'] == 'CRITICAL' else \"[WARNING]\"\n",
    "        print(f\"{severity_indicator} {alert['metric']:15} = {alert['value']:.2f}\")\n",
    "else:\n",
    "    print(\"  No active alerts\")\n",
    "\n",
    "# Storage usage\n",
    "storage_stats = storage_service.get_storage_stats()\n",
    "print(\"\\n[STORAGE USAGE]\")\n",
    "print(\"-\"*70)\n",
    "for backend, stats in storage_stats.items():\n",
    "    if stats['available']:\n",
    "        size_gb = stats['total_size'] / (1024**3)\n",
    "        print(f\"  {backend:10} Objects: {stats['objects_count']:5}  Size: {size_gb:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions and Next Steps\n",
    "\n",
    "### Service Integration Summary\n",
    "\n",
    "This tutorial demonstrated comprehensive service integration patterns:\n",
    "\n",
    "1. **Service Discovery**: Registry and health checking\n",
    "2. **Orchestration**: Workflow management and execution\n",
    "3. **Message Queuing**: Asynchronous processing with priorities\n",
    "4. **Caching**: Multi-level caching with TTL and eviction\n",
    "5. **Storage**: Unified interface for multiple backends\n",
    "6. **Monitoring**: Metrics collection and alerting\n",
    "7. **Notifications**: Multi-channel event notifications\n",
    "8. **Pipeline Integration**: End-to-end service pipelines\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Service Decoupling**: Independent services with clear interfaces\n",
    "2. **Resilience Patterns**: Health checks, retries, circuit breakers\n",
    "3. **Performance Optimization**: Caching, batching, async processing\n",
    "4. **Observability**: Comprehensive monitoring and alerting\n",
    "5. **Scalability**: Distributed architecture with queue-based processing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Advanced Orchestration**:\n",
    "   - Implement Saga pattern for distributed transactions\n",
    "   - Add workflow versioning and rollback\n",
    "   - Implement dynamic service mesh\n",
    "\n",
    "2. **Performance Enhancement**:\n",
    "   - Add distributed tracing\n",
    "   - Implement adaptive caching strategies\n",
    "   - Use connection pooling\n",
    "\n",
    "3. **Reliability Improvements**:\n",
    "   - Implement chaos engineering tests\n",
    "   - Add automated failover\n",
    "   - Create disaster recovery procedures\n",
    "\n",
    "4. **Production Deployment**:\n",
    "   - Container orchestration with Kubernetes\n",
    "   - Service mesh with Istio\n",
    "   - Continuous deployment pipelines\n",
    "\n",
    "### References\n",
    "\n",
    "For deeper understanding, consult:\n",
    "- Service documentation: `docs/developer_guide/service_development.md`\n",
    "- Architecture patterns: `docs/architecture/patterns/`\n",
    "- Operations guide: `docs/operations/runbooks/`\n",
    "- Monitoring setup: `monitoring/dashboards/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
