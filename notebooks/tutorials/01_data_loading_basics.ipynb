{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Basics for AG News Text Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates fundamental data loading techniques following methodologies from:\n",
    "- Zhang et al. (2015): \"Character-level Convolutional Networks for Text Classification\"\n",
    "- Joulin et al. (2017): \"Bag of Tricks for Efficient Text Classification\"\n",
    "- Wolf et al. (2020): \"Transformers: State-of-the-Art Natural Language Processing\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Load AG News dataset from multiple sources\n",
    "2. Understand data structure and format\n",
    "3. Create efficient data pipelines\n",
    "4. Implement data validation\n",
    "5. Optimize loading performance\n",
    "6. Save processed data for training\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig, create_ag_news_datasets\n",
    "from src.data.datasets.external_news import ExternalNewsDataset\n",
    "from src.data.datasets.combined_dataset import CombinedDataset\n",
    "from src.data.loaders.dataloader import create_dataloaders, DataLoaderConfig\n",
    "from src.data.loaders.dynamic_batching import DynamicBatchSampler\n",
    "from src.data.loaders.prefetch_loader import PrefetchLoader\n",
    "from src.utils.io_utils import safe_load, safe_save, ensure_dir\n",
    "from src.utils.logging_config import setup_logging\n",
    "from src.utils.reproducibility import set_seed\n",
    "from configs.config_loader import ConfigLoader\n",
    "from configs.constants import (\n",
    "    AG_NEWS_CLASSES,\n",
    "    AG_NEWS_NUM_CLASSES,\n",
    "    LABEL_TO_ID,\n",
    "    ID_TO_LABEL,\n",
    "    DATA_DIR\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "set_seed(42)\n",
    "logger = setup_logging('data_loading_tutorial')\n",
    "\n",
    "print(\"Data Loading Tutorial\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data loading configuration\n",
    "config_loader = ConfigLoader()\n",
    "\n",
    "# Load data config\n",
    "data_config = config_loader.load_config('data/preprocessing/standard.yaml')\n",
    "\n",
    "# Tutorial configuration\n",
    "tutorial_config = {\n",
    "    'max_samples': 1000,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': torch.cuda.is_available(),\n",
    "    'use_cache': True,\n",
    "    'validate_data': True\n",
    "}\n",
    "\n",
    "print(\"Data Loading Configuration:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in tutorial_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading from Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load from processed files\n",
    "def load_from_processed() -> Tuple[AGNewsDataset, AGNewsDataset, AGNewsDataset]:\n",
    "    \"\"\"\n",
    "    Load AG News dataset from preprocessed files.\n",
    "    \n",
    "    Following data loading best practices from:\n",
    "        TensorFlow Data Validation (TFDV) documentation\n",
    "    \"\"\"\n",
    "    config = AGNewsConfig(\n",
    "        data_dir=DATA_DIR / \"processed\",\n",
    "        max_samples=tutorial_config['max_samples'],\n",
    "        validate_labels=tutorial_config['validate_data'],\n",
    "        use_cache=tutorial_config['use_cache']\n",
    "    )\n",
    "    \n",
    "    train_dataset = AGNewsDataset(config, split=\"train\")\n",
    "    val_dataset = AGNewsDataset(config, split=\"validation\")\n",
    "    test_dataset = AGNewsDataset(config, split=\"test\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "print(\"Loading from processed files...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    train_dataset, val_dataset, test_dataset = load_from_processed()\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Loading completed in {load_time:.2f} seconds\")\n",
    "    print(f\"\\nDataset sizes:\")\n",
    "    print(f\"  Train: {len(train_dataset):,} samples\")\n",
    "    print(f\"  Validation: {len(val_dataset):,} samples\")\n",
    "    print(f\"  Test: {len(test_dataset):,} samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading processed data: {e}\")\n",
    "    print(\"Please run data preparation scripts first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading from Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Load from Hugging Face datasets\n",
    "def load_from_huggingface() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load AG News dataset from Hugging Face Hub.\n",
    "    \n",
    "    References:\n",
    "        Lhoest et al. (2021): \"Datasets: A Community Library for Natural Language Processing\"\n",
    "    \"\"\"\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(\"Downloading from Hugging Face Hub...\")\n",
    "    dataset = load_dataset('ag_news')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Try loading from Hugging Face\n",
    "try:\n",
    "    hf_dataset = load_from_huggingface()\n",
    "    \n",
    "    print(\"\\nHugging Face dataset structure:\")\n",
    "    print(f\"  Splits: {list(hf_dataset.keys())}\")\n",
    "    print(f\"  Features: {hf_dataset['train'].features}\")\n",
    "    print(f\"  Train size: {len(hf_dataset['train']):,}\")\n",
    "    print(f\"  Test size: {len(hf_dataset['test']):,}\")\n",
    "    \n",
    "    # Show sample\n",
    "    sample = hf_dataset['train'][0]\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(f\"  Label: {sample['label']} ({ID_TO_LABEL[sample['label']]})\")\n",
    "    print(f\"  Text: {sample['text'][:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load from Hugging Face: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset wrapper\n",
    "class AGNewsCustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset implementation for AG News.\n",
    "    \n",
    "    Following PyTorch dataset design patterns from:\n",
    "        Paszke et al. (2019): \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer=None, max_length: int = 256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.tokenizer:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'text': text,\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "# Create sample custom dataset\n",
    "sample_texts = train_dataset.texts[:100]\n",
    "sample_labels = train_dataset.labels[:100]\n",
    "\n",
    "custom_dataset = AGNewsCustomDataset(\n",
    "    texts=sample_texts,\n",
    "    labels=sample_labels\n",
    ")\n",
    "\n",
    "print(\"Custom Dataset Created:\")\n",
    "print(f\"  Size: {len(custom_dataset)}\")\n",
    "print(f\"  Sample item keys: {list(custom_dataset[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DataLoader Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DataLoader\n",
    "def create_optimized_dataloader(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 4\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create optimized DataLoader with best practices.\n",
    "    \n",
    "    Following optimization strategies from:\n",
    "        Li et al. (2020): \"PyTorch Distributed: Experiences on Accelerating Data Parallel Training\"\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=False,\n",
    "        prefetch_factor=2 if num_workers > 0 else 2,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = create_optimized_dataloader(\n",
    "    custom_dataset,\n",
    "    batch_size=tutorial_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=tutorial_config['num_workers']\n",
    ")\n",
    "\n",
    "print(\"DataLoader Configuration:\")\n",
    "print(f\"  Batch size: {tutorial_config['batch_size']}\")\n",
    "print(f\"  Number of batches: {len(train_loader)}\")\n",
    "print(f\"  Pin memory: {torch.cuda.is_available()}\")\n",
    "print(f\"  Number of workers: {tutorial_config['num_workers']}\")\n",
    "\n",
    "# Test DataLoader\n",
    "print(\"\\nTesting DataLoader...\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"  Batch keys: {list(batch.keys())}\")\n",
    "print(f\"  Batch labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory-Efficient Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement memory-efficient loading\n",
    "class LazyAGNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazy loading dataset to minimize memory usage.\n",
    "    \n",
    "    Following memory optimization techniques from:\n",
    "        Chen et al. (2016): \"Training Deep Nets with Sublinear Memory Cost\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_file: Path, max_samples: Optional[int] = None):\n",
    "        self.data_file = data_file\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "        # Only load metadata\n",
    "        self._load_metadata()\n",
    "        \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load only essential metadata.\"\"\"\n",
    "        # Simulate loading metadata\n",
    "        self.length = self.max_samples if self.max_samples else 120000\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Load data on-demand.\"\"\"\n",
    "        # Simulate loading single item from disk\n",
    "        text = f\"Sample news article {idx}\"\n",
    "        label = idx % AG_NEWS_NUM_CLASSES\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# Create lazy dataset\n",
    "lazy_dataset = LazyAGNewsDataset(\n",
    "    data_file=DATA_DIR / \"processed\" / \"train.json\",\n",
    "    max_samples=10000\n",
    ")\n",
    "\n",
    "print(\"Lazy Loading Dataset:\")\n",
    "print(f\"  Dataset size: {len(lazy_dataset):,}\")\n",
    "print(f\"  Memory efficient: Yes\")\n",
    "print(f\"  On-demand loading: Yes\")\n",
    "\n",
    "# Memory usage comparison\n",
    "import sys\n",
    "regular_size = sys.getsizeof(train_dataset.texts) + sys.getsizeof(train_dataset.labels)\n",
    "lazy_size = sys.getsizeof(lazy_dataset)\n",
    "\n",
    "print(f\"\\nMemory Usage Comparison:\")\n",
    "print(f\"  Regular dataset: ~{regular_size / 1024**2:.2f} MB\")\n",
    "print(f\"  Lazy dataset: ~{lazy_size / 1024**2:.2f} MB\")\n",
    "print(f\"  Reduction: {(1 - lazy_size/regular_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(dataset: Dataset) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform comprehensive data validation.\n",
    "    \n",
    "    Following validation practices from:\n",
    "        Breck et al. (2019): \"Data Validation for Machine Learning\"\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        'passed': True,\n",
    "        'issues': [],\n",
    "        'statistics': {}\n",
    "    }\n",
    "    \n",
    "    # Check dataset size\n",
    "    if len(dataset) == 0:\n",
    "        validation_results['passed'] = False\n",
    "        validation_results['issues'].append(\"Dataset is empty\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    labels = [dataset[i]['labels'].item() if torch.is_tensor(dataset[i]['labels']) \n",
    "              else dataset[i]['labels'] for i in range(min(1000, len(dataset)))]\n",
    "    \n",
    "    label_counts = pd.Series(labels).value_counts()\n",
    "    validation_results['statistics']['label_distribution'] = label_counts.to_dict()\n",
    "    \n",
    "    # Check for invalid labels\n",
    "    invalid_labels = [l for l in labels if l < 0 or l >= AG_NEWS_NUM_CLASSES]\n",
    "    if invalid_labels:\n",
    "        validation_results['passed'] = False\n",
    "        validation_results['issues'].append(f\"Found {len(invalid_labels)} invalid labels\")\n",
    "    \n",
    "    # Check for missing data\n",
    "    sample_issues = 0\n",
    "    for i in range(min(100, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        if 'text' in sample and not sample['text']:\n",
    "            sample_issues += 1\n",
    "    \n",
    "    if sample_issues > 0:\n",
    "        validation_results['issues'].append(f\"Found {sample_issues} samples with empty text\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate datasets\n",
    "print(\"Dataset Validation Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, dataset in [(\"Custom\", custom_dataset), (\"Lazy\", lazy_dataset)]:\n",
    "    results = validate_dataset(dataset)\n",
    "    \n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    print(f\"  Validation passed: {results['passed']}\")\n",
    "    \n",
    "    if results['issues']:\n",
    "        print(f\"  Issues found:\")\n",
    "        for issue in results['issues']:\n",
    "            print(f\"    - {issue}\")\n",
    "    else:\n",
    "        print(f\"  No issues found\")\n",
    "    \n",
    "    if 'label_distribution' in results['statistics']:\n",
    "        print(f\"  Label distribution: {results['statistics']['label_distribution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Processing and Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Custom collation function for batching.\n",
    "    \n",
    "    Following batching strategies from:\n",
    "        Micikevicius et al. (2018): \"Mixed Precision Training\"\n",
    "    \"\"\"\n",
    "    # Handle text data\n",
    "    if 'text' in batch[0]:\n",
    "        texts = [item['text'] for item in batch]\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        \n",
    "        return {\n",
    "            'texts': texts,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    # Handle tokenized data\n",
    "    elif 'input_ids' in batch[0]:\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unknown batch format\")\n",
    "\n",
    "# Create DataLoader with custom collation\n",
    "custom_loader = DataLoader(\n",
    "    custom_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Process a batch\n",
    "batch = next(iter(custom_loader))\n",
    "\n",
    "print(\"Custom Batch Processing:\")\n",
    "print(f\"  Batch keys: {list(batch.keys())}\")\n",
    "print(f\"  Batch size: {len(batch['labels'])}\")\n",
    "print(f\"  Labels shape: {batch['labels'].shape}\")\n",
    "print(f\"  Labels: {batch['labels'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_dataloader(dataloader: DataLoader, num_batches: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Benchmark DataLoader performance.\n",
    "    \n",
    "    Following benchmarking methodology from:\n",
    "        PyTorch Performance Tuning Guide\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    times = []\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = next(iter(dataloader))\n",
    "    \n",
    "    # Benchmark\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        # Simulate processing\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.cuda(non_blocking=True) if torch.is_tensor(v) else v \n",
    "                    for k, v in batch.items()}\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        times.append(end - start)\n",
    "    \n",
    "    return {\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times),\n",
    "        'throughput': len(dataloader.dataset) / sum(times) if times else 0\n",
    "    }\n",
    "\n",
    "# Benchmark different configurations\n",
    "configurations = [\n",
    "    {'batch_size': 8, 'num_workers': 0},\n",
    "    {'batch_size': 16, 'num_workers': 2},\n",
    "    {'batch_size': 32, 'num_workers': 4}\n",
    "]\n",
    "\n",
    "print(\"DataLoader Performance Benchmarking:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for config in configurations:\n",
    "    loader = DataLoader(\n",
    "        custom_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    metrics = benchmark_dataloader(loader, num_batches=50)\n",
    "    \n",
    "    print(f\"\\nConfiguration: batch_size={config['batch_size']}, workers={config['num_workers']}\")\n",
    "    print(f\"  Mean time per batch: {metrics['mean_time']*1000:.2f} ms\")\n",
    "    print(f\"  Throughput: {metrics['throughput']:.0f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save and Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "def save_processed_data(dataset: Dataset, filepath: Path):\n",
    "    \"\"\"\n",
    "    Save processed dataset for future use.\n",
    "    \n",
    "    Following serialization best practices from:\n",
    "        PyTorch Serialization Semantics Documentation\n",
    "    \"\"\"\n",
    "    ensure_dir(filepath.parent)\n",
    "    \n",
    "    data_dict = {\n",
    "        'texts': [],\n",
    "        'labels': [],\n",
    "        'metadata': {\n",
    "            'num_classes': AG_NEWS_NUM_CLASSES,\n",
    "            'class_names': AG_NEWS_CLASSES,\n",
    "            'processed_date': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Extract data\n",
    "    for i in range(len(dataset)):\n",
    "        item = dataset[i]\n",
    "        if 'text' in item:\n",
    "            data_dict['texts'].append(item['text'])\n",
    "        data_dict['labels'].append(\n",
    "            item['labels'].item() if torch.is_tensor(item['labels']) else item['labels']\n",
    "        )\n",
    "    \n",
    "    # Save\n",
    "    safe_save(data_dict, filepath)\n",
    "    print(f\"Data saved to: {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Save sample data\n",
    "from src.utils.io_utils import OUTPUT_DIR\n",
    "output_path = OUTPUT_DIR / \"tutorial\" / \"sample_data.json\"\n",
    "saved_path = save_processed_data(custom_dataset, output_path)\n",
    "\n",
    "# Load saved data\n",
    "loaded_data = safe_load(saved_path)\n",
    "print(f\"\\nLoaded data verification:\")\n",
    "print(f\"  Number of texts: {len(loaded_data['texts'])}\")\n",
    "print(f\"  Number of labels: {len(loaded_data['labels'])}\")\n",
    "print(f\"  Metadata: {loaded_data['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Next Steps\n",
    "\n",
    "### Data Loading Summary\n",
    "\n",
    "This tutorial demonstrated fundamental data loading concepts:\n",
    "\n",
    "1. **Environment Setup**: Configured data loading environment with necessary libraries\n",
    "2. **Multiple Sources**: Loaded data from processed files and Hugging Face Hub\n",
    "3. **Custom Datasets**: Implemented PyTorch Dataset classes for flexibility\n",
    "4. **DataLoader Configuration**: Created optimized data loaders with proper settings\n",
    "5. **Memory Efficiency**: Implemented lazy loading for large datasets\n",
    "6. **Data Validation**: Performed comprehensive validation checks\n",
    "7. **Batch Processing**: Configured custom collation functions\n",
    "8. **Performance Benchmarking**: Evaluated loading performance\n",
    "9. **Data Persistence**: Saved and loaded processed data\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Multiple Loading Methods**: Different sources require different approaches\n",
    "2. **Memory Management**: Lazy loading essential for large datasets\n",
    "3. **Validation Importance**: Always validate data before training\n",
    "4. **Performance Optimization**: Proper DataLoader configuration significantly impacts speed\n",
    "5. **Reproducibility**: Save processed data for consistent experiments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Advanced Data Loading**:\n",
    "   - Implement streaming data loaders\n",
    "   - Use distributed data loading\n",
    "   - Apply data sharding strategies\n",
    "\n",
    "2. **Optimization**:\n",
    "   - Profile data loading bottlenecks\n",
    "   - Implement prefetching strategies\n",
    "   - Use memory mapping for large files\n",
    "\n",
    "3. **Integration**:\n",
    "   - Connect with preprocessing pipeline\n",
    "   - Implement data augmentation\n",
    "   - Create data versioning system\n",
    "\n",
    "4. **Production**:\n",
    "   - Build scalable data pipelines\n",
    "   - Implement real-time data loading\n",
    "   - Monitor data quality metrics\n",
    "\n",
    "### References\n",
    "\n",
    "For deeper understanding, consult:\n",
    "- Data loading documentation: `docs/user_guide/data_preparation.md`\n",
    "- Preprocessing: `notebooks/tutorials/02_preprocessing_tutorial.ipynb`\n",
    "- Model training: `notebooks/tutorials/03_model_training_basics.ipynb`\n",
    "- Advanced techniques: `notebooks/tutorials/05_prompt_engineering.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
