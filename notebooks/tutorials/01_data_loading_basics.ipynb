{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Basics for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates fundamental data loading techniques following best practices from:\n",
    "- PyTorch DataLoader documentation and design patterns\n",
    "- Hugging Face Datasets library architecture\n",
    "- Efficient data pipeline design principles\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand the AG News dataset structure\n",
    "2. Load data using custom dataset classes\n",
    "3. Implement efficient batch loading\n",
    "4. Handle data transformations and preprocessing\n",
    "5. Create reproducible data splits\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project setup\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Project imports\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.data.loaders.dataloader import create_dataloader, DataLoaderConfig\n",
    "from src.utils.reproducibility import set_seed\n",
    "from configs.constants import (\n",
    "    AG_NEWS_CLASSES,\n",
    "    AG_NEWS_NUM_CLASSES,\n",
    "    LABEL_TO_ID,\n",
    "    ID_TO_LABEL,\n",
    "    DATA_DIR\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding AG News Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"AG News Dataset Information\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of classes: {AG_NEWS_NUM_CLASSES}\")\n",
    "print(f\"Class names: {AG_NEWS_CLASSES}\")\n",
    "print(\"\\nClass to ID mapping:\")\n",
    "for class_name, class_id in LABEL_TO_ID.items():\n",
    "    print(f\"  {class_name}: {class_id}\")\n",
    "\n",
    "# Dataset structure\n",
    "print(\"\\nExpected Dataset Structure:\")\n",
    "print(\"\"\"\n",
    "Each sample contains:\n",
    "  - text: The news article text (title + description)\n",
    "  - label: Integer label (0-3)\n",
    "  - label_name: String label (World, Sports, Business, Sci/Tech)\n",
    "  \n",
    "Data splits:\n",
    "  - Train: 120,000 samples (30,000 per class)\n",
    "  - Test: 7,600 samples (1,900 per class)\n",
    "  - No official validation set (we create one from training data)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset configuration\n",
    "dataset_config = AGNewsConfig(\n",
    "    data_dir=DATA_DIR / \"processed\",\n",
    "    max_samples=1000,  # Limit samples for tutorial\n",
    "    use_cache=True,\n",
    "    validate_labels=True\n",
    ")\n",
    "\n",
    "print(\"Loading AG News dataset...\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data directory: {dataset_config.data_dir}\")\n",
    "print(f\"  Max samples: {dataset_config.max_samples}\")\n",
    "print(f\"  Use cache: {dataset_config.use_cache}\")\n",
    "\n",
    "# Load training dataset\n",
    "try:\n",
    "    train_dataset = AGNewsDataset(dataset_config, split=\"train\")\n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"Number of samples: {len(train_dataset)}\")\n",
    "    \n",
    "    # Get first sample\n",
    "    first_sample = train_dataset[0]\n",
    "    print(f\"\\nFirst sample:\")\n",
    "    print(f\"  Text: {first_sample['text'][:100]}...\")\n",
    "    print(f\"  Label: {first_sample['label']}\")\n",
    "    print(f\"  Label name: {first_sample['label_name']}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nError: Dataset files not found.\")\n",
    "    print(f\"Please run the data preparation script first:\")\n",
    "    print(f\"  python scripts/data_preparation/prepare_ag_news.py\")\n",
    "    \n",
    "    # Alternative: Load from Hugging Face\n",
    "    print(\"\\nAlternatively, loading from Hugging Face...\")\n",
    "    from datasets import load_dataset\n",
    "    hf_dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "    print(f\"Loaded {len(hf_dataset)} samples from Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader configuration\n",
    "loader_config = DataLoaderConfig(\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(\"Creating DataLoader...\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Batch size: {loader_config.batch_size}\")\n",
    "print(f\"  Shuffle: {loader_config.shuffle}\")\n",
    "print(f\"  Num workers: {loader_config.num_workers}\")\n",
    "print(f\"  Pin memory: {loader_config.pin_memory}\")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = create_dataloader(train_dataset, loader_config)\n",
    "\n",
    "print(f\"\\nDataLoader created!\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "print(f\"Samples per batch: {loader_config.batch_size}\")\n",
    "\n",
    "# Test iteration\n",
    "print(\"\\nTesting batch iteration...\")\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(f\"Batch keys: {batch.keys()}\")\n",
    "        print(f\"Text shape: {len(batch['text'])}\")\n",
    "        print(f\"Labels shape: {batch['label'].shape}\")\n",
    "        print(f\"Label names: {batch['label_name'][:5]}\")\n",
    "    if i >= 2:  # Show only first 3 batches\n",
    "        break\n",
    "print(f\"Iterated through {i+1} batches successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAGNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple custom dataset implementation for AG News.\n",
    "    \n",
    "    This demonstrates the core PyTorch Dataset interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[int]):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text samples\n",
    "            labels: List of integer labels\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        assert len(texts) == len(labels), \"Texts and labels must have same length\"\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Args:\n",
    "            idx: Sample index\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing text and label\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'text': self.texts[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'label_name': ID_TO_LABEL[self.labels[idx]]\n",
    "        }\n",
    "\n",
    "# Create sample data\n",
    "sample_texts = [\n",
    "    \"Stock market reaches new heights today\",\n",
    "    \"Scientists discover new planet in solar system\",\n",
    "    \"Football team wins championship\",\n",
    "    \"New technology breakthrough announced\"\n",
    "]\n",
    "sample_labels = [2, 3, 1, 3]  # Business, Sci/Tech, Sports, Sci/Tech\n",
    "\n",
    "# Create custom dataset\n",
    "simple_dataset = SimpleAGNewsDataset(sample_texts, sample_labels)\n",
    "\n",
    "print(f\"Custom dataset created with {len(simple_dataset)} samples\")\n",
    "print(\"\\nSamples:\")\n",
    "for i in range(len(simple_dataset)):\n",
    "    sample = simple_dataset[i]\n",
    "    print(f\"{i+1}. {sample['label_name']}: {sample['text']}\")\n",
    "\n",
    "# Create DataLoader for custom dataset\n",
    "simple_loader = DataLoader(simple_dataset, batch_size=2, shuffle=True)\n",
    "print(f\"\\nDataLoader created with batch size 2\")\n",
    "\n",
    "for batch in simple_loader:\n",
    "    print(f\"Batch texts: {batch['text']}\")\n",
    "    print(f\"Batch labels: {batch['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Train/Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_split(dataset: Dataset, val_ratio: float = 0.1, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Create train/validation split from a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Original dataset\n",
    "        val_ratio: Ratio of validation samples\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_dataset, val_dataset)\n",
    "    \"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    train_size = total_size - val_size\n",
    "    \n",
    "    # Create random split\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Create train/validation split\n",
    "print(\"Creating train/validation split...\")\n",
    "train_subset, val_subset = create_train_val_split(train_dataset, val_ratio=0.2)\n",
    "\n",
    "print(f\"Original dataset size: {len(train_dataset)}\")\n",
    "print(f\"Training subset size: {len(train_subset)} ({len(train_subset)/len(train_dataset)*100:.1f}%)\")\n",
    "print(f\"Validation subset size: {len(val_subset)} ({len(val_subset)/len(train_dataset)*100:.1f}%)\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_indices = set(train_subset.indices)\n",
    "val_indices = set(val_subset.indices)\n",
    "overlap = train_indices.intersection(val_indices)\n",
    "print(f\"\\nOverlap between train and validation: {len(overlap)} samples\")\n",
    "\n",
    "if len(overlap) == 0:\n",
    "    print(\"Split verification: PASSED (no overlap)\")\n",
    "else:\n",
    "    print(\"Split verification: FAILED (overlap detected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Efficient Batch Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different loading strategies\n",
    "print(\"Comparing DataLoader Performance\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test configurations\n",
    "configs = [\n",
    "    {\"batch_size\": 16, \"num_workers\": 0, \"pin_memory\": False},\n",
    "    {\"batch_size\": 16, \"num_workers\": 2, \"pin_memory\": False},\n",
    "    {\"batch_size\": 32, \"num_workers\": 2, \"pin_memory\": False},\n",
    "    {\"batch_size\": 32, \"num_workers\": 4, \"pin_memory\": torch.cuda.is_available()},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        pin_memory=config[\"pin_memory\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Measure loading time\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= 10:  # Test only first 10 batches\n",
    "            break\n",
    "        # Simulate some processing\n",
    "        _ = batch['label'].numpy()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    result = {\n",
    "        **config,\n",
    "        \"time\": elapsed_time,\n",
    "        \"batches_per_second\": 10 / elapsed_time\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"Config: batch_size={config['batch_size']}, \"\n",
    "          f\"workers={config['num_workers']}, \"\n",
    "          f\"pin_memory={config['pin_memory']}\")\n",
    "    print(f\"  Time: {elapsed_time:.3f}s\")\n",
    "    print(f\"  Throughput: {result['batches_per_second']:.1f} batches/sec\")\n",
    "    print()\n",
    "\n",
    "# Find best configuration\n",
    "best_config = max(results, key=lambda x: x['batches_per_second'])\n",
    "print(f\"Best configuration:\")\n",
    "print(f\"  Batch size: {best_config['batch_size']}\")\n",
    "print(f\"  Workers: {best_config['num_workers']}\")\n",
    "print(f\"  Pin memory: {best_config['pin_memory']}\")\n",
    "print(f\"  Throughput: {best_config['batches_per_second']:.1f} batches/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory-Efficient Loading for Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyAGNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazy-loading dataset that reads data on-demand.\n",
    "    \n",
    "    This is memory-efficient for large datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_file: Path):\n",
    "        \"\"\"\n",
    "        Initialize lazy dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_file: Path to data file (JSON lines format)\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        \n",
    "        # Count lines without loading entire file\n",
    "        self._length = 0\n",
    "        if data_file.exists():\n",
    "            with open(data_file, 'r') as f:\n",
    "                for _ in f:\n",
    "                    self._length += 1\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self._length\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Load a single sample from disk.\n",
    "        \n",
    "        Args:\n",
    "            idx: Sample index\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing sample data\n",
    "        \"\"\"\n",
    "        with open(self.data_file, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i == idx:\n",
    "                    sample = json.loads(line)\n",
    "                    return {\n",
    "                        'text': sample['text'],\n",
    "                        'label': sample['label'],\n",
    "                        'label_name': ID_TO_LABEL[sample['label']]\n",
    "                    }\n",
    "        \n",
    "        raise IndexError(f\"Index {idx} out of range\")\n",
    "\n",
    "# Demonstrate lazy loading concept\n",
    "print(\"Lazy Loading Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "Lazy loading is useful when:\n",
    "1. Dataset is too large to fit in memory\n",
    "2. You want to minimize memory usage\n",
    "3. Data is stored in a streamable format\n",
    "\n",
    "Trade-offs:\n",
    "- Pros: Low memory usage, can handle huge datasets\n",
    "- Cons: Slower data access, more I/O operations\n",
    "\n",
    "Best practices:\n",
    "- Use memory mapping for large files\n",
    "- Cache frequently accessed samples\n",
    "- Use efficient file formats (HDF5, Arrow, etc.)\n",
    "\"\"\")\n",
    "\n",
    "# Create sample data file for demonstration\n",
    "sample_data_file = PROJECT_ROOT / \"data\" / \"sample_lazy.jsonl\"\n",
    "sample_data_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write sample data\n",
    "with open(sample_data_file, 'w') as f:\n",
    "    for i, (text, label) in enumerate(zip(sample_texts, sample_labels)):\n",
    "        json.dump({\"text\": text, \"label\": label}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Create lazy dataset\n",
    "lazy_dataset = LazyAGNewsDataset(sample_data_file)\n",
    "print(f\"\\nLazy dataset created with {len(lazy_dataset)} samples\")\n",
    "print(\"Memory usage: Minimal (data not loaded)\")\n",
    "\n",
    "# Access samples on-demand\n",
    "print(\"\\nAccessing samples on-demand:\")\n",
    "for i in range(min(3, len(lazy_dataset))):\n",
    "    sample = lazy_dataset[i]\n",
    "    print(f\"  Sample {i}: {sample['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Loading Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_dataloader(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int = 32,\n",
    "    training: bool = True\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create an optimized DataLoader with best practices.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PyTorch dataset\n",
    "        batch_size: Batch size\n",
    "        training: Whether this is for training\n",
    "        \n",
    "    Returns:\n",
    "        Optimized DataLoader\n",
    "    \"\"\"\n",
    "    # Determine optimal number of workers\n",
    "    num_workers = min(4, os.cpu_count() // 2) if os.cpu_count() else 0\n",
    "    \n",
    "    # Create DataLoader with optimizations\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=training,  # Shuffle only for training\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),  # Pin memory for GPU\n",
    "        drop_last=training,  # Drop last incomplete batch for training\n",
    "        persistent_workers=num_workers > 0,  # Keep workers alive\n",
    "        prefetch_factor=2 if num_workers > 0 else 2,  # Prefetch batches\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "# Create optimized loaders\n",
    "print(\"Creating Optimized DataLoaders\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "train_loader_opt = create_optimized_dataloader(train_subset, batch_size=32, training=True)\n",
    "val_loader_opt = create_optimized_dataloader(val_subset, batch_size=64, training=False)\n",
    "\n",
    "print(\"Training DataLoader:\")\n",
    "print(f\"  Batch size: {train_loader_opt.batch_size}\")\n",
    "print(f\"  Shuffle: {train_loader_opt.shuffle}\")\n",
    "print(f\"  Workers: {train_loader_opt.num_workers}\")\n",
    "print(f\"  Pin memory: {train_loader_opt.pin_memory}\")\n",
    "print(f\"  Drop last: {train_loader_opt.drop_last}\")\n",
    "\n",
    "print(\"\\nValidation DataLoader:\")\n",
    "print(f\"  Batch size: {val_loader_opt.batch_size}\")\n",
    "print(f\"  Shuffle: {val_loader_opt.shuffle}\")\n",
    "print(f\"  Workers: {val_loader_opt.num_workers}\")\n",
    "print(f\"  Pin memory: {val_loader_opt.pin_memory}\")\n",
    "print(f\"  Drop last: {val_loader_opt.drop_last}\")\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\nDataLoader Best Practices:\")\n",
    "print(\"\"\"\n",
    "1. Use multiple workers for CPU-bound preprocessing\n",
    "2. Pin memory when using GPU for faster transfers\n",
    "3. Shuffle training data, but not validation/test data\n",
    "4. Drop last incomplete batch during training for consistent batch sizes\n",
    "5. Use persistent workers to avoid recreation overhead\n",
    "6. Prefetch batches to hide I/O latency\n",
    "7. Larger batch sizes for validation (no gradients needed)\n",
    "8. Consider gradient accumulation for effective larger batches\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Data Loading Tutorial Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary = {\n",
    "    \"Dataset\": {\n",
    "        \"Total samples\": len(train_dataset) if 'train_dataset' in locals() else 0,\n",
    "        \"Classes\": AG_NEWS_NUM_CLASSES,\n",
    "        \"Class names\": AG_NEWS_CLASSES\n",
    "    },\n",
    "    \"Data Splits\": {\n",
    "        \"Train samples\": len(train_subset) if 'train_subset' in locals() else 0,\n",
    "        \"Validation samples\": len(val_subset) if 'val_subset' in locals() else 0\n",
    "    },\n",
    "    \"DataLoader Settings\": {\n",
    "        \"Optimal batch size\": best_config['batch_size'] if 'best_config' in locals() else 32,\n",
    "        \"Optimal workers\": best_config['num_workers'] if 'best_config' in locals() else 2,\n",
    "        \"GPU available\": torch.cuda.is_available()\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, items in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"\"\"\n",
    "1. AG News has 4 balanced classes suitable for classification\n",
    "2. PyTorch DataLoader provides efficient batch loading\n",
    "3. Multiple workers can significantly speed up data loading\n",
    "4. Pin memory improves GPU transfer speed\n",
    "5. Lazy loading enables handling of large datasets\n",
    "6. Proper configuration can improve training efficiency\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"\"\"\n",
    "1. Explore data preprocessing techniques (02_preprocessing_tutorial.ipynb)\n",
    "2. Learn about model training (03_model_training_basics.ipynb)\n",
    "3. Understand evaluation metrics (04_evaluation_tutorial.ipynb)\n",
    "4. Experiment with different batch sizes and configurations\n",
    "5. Implement custom data augmentation in the dataset\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
