{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Usage Tutorial for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates comprehensive API usage patterns following methodologies from:\n",
    "- Fielding (2000): \"Architectural Styles and the Design of Network-based Software Architectures\"\n",
    "- Google (2015): \"gRPC: A high performance, open source universal RPC framework\"\n",
    "- Facebook (2015): \"GraphQL: A Query Language for Your API\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Connect to REST API endpoints\n",
    "2. Implement gRPC client communication\n",
    "3. Execute GraphQL queries and mutations\n",
    "4. Handle authentication and rate limiting\n",
    "5. Implement batch processing and streaming\n",
    "6. Monitor API performance and health\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "import warnings\n",
    "\n",
    "# HTTP and API imports\n",
    "import requests\n",
    "import httpx\n",
    "import websocket\n",
    "import grpc\n",
    "from gql import gql, Client\n",
    "from gql.transport.httpx import HTTPXTransport\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.api.rest.schemas.request_schemas import (\n",
    "    ClassificationRequest,\n",
    "    BatchClassificationRequest,\n",
    "    TrainingRequest\n",
    ")\n",
    "from src.api.rest.schemas.response_schemas import (\n",
    "    ClassificationResponse,\n",
    "    BatchClassificationResponse,\n",
    "    ModelInfoResponse\n",
    ")\n",
    "from src.api.grpc.compiled import classification_pb2, classification_pb2_grpc\n",
    "from src.api.grpc.compiled import model_management_pb2, model_management_pb2_grpc\n",
    "from src.utils.api_utils import (\n",
    "    create_api_client,\n",
    "    handle_api_error,\n",
    "    retry_with_backoff\n",
    ")\n",
    "from src.utils.logging_config import setup_logging\n",
    "from configs.config_loader import ConfigLoader\n",
    "from configs.constants import AG_NEWS_CLASSES\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "logger = setup_logging('api_usage_tutorial')\n",
    "\n",
    "# API Configuration\n",
    "API_CONFIG = {\n",
    "    'rest_base_url': 'http://localhost:8000',\n",
    "    'grpc_host': 'localhost',\n",
    "    'grpc_port': 50051,\n",
    "    'graphql_url': 'http://localhost:8000/graphql',\n",
    "    'api_key': os.getenv('API_KEY', 'demo_api_key'),\n",
    "    'timeout': 30\n",
    "}\n",
    "\n",
    "print(\"API Usage Tutorial\")\n",
    "print(\"=\"*50)\n",
    "print(f\"REST API URL: {API_CONFIG['rest_base_url']}\")\n",
    "print(f\"gRPC Server: {API_CONFIG['grpc_host']}:{API_CONFIG['grpc_port']}\")\n",
    "print(f\"GraphQL Endpoint: {API_CONFIG['graphql_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. REST API Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RESTAPIClient:\n",
    "    \"\"\"\n",
    "    REST API client for AG News classification service.\n",
    "    \n",
    "    Following REST principles from:\n",
    "        Richardson & Ruby (2007): \"RESTful Web Services\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_key: str, timeout: int = 30):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.timeout = timeout\n",
    "        self.session = self._create_session()\n",
    "    \n",
    "    def _create_session(self) -> requests.Session:\n",
    "        \"\"\"Create configured HTTP session.\"\"\"\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\n",
    "            'Authorization': f'Bearer {self.api_key}',\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json',\n",
    "            'User-Agent': 'AGNews-Tutorial/1.0'\n",
    "        })\n",
    "        return session\n",
    "    \n",
    "    def health_check(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check API health status.\"\"\"\n",
    "        response = self.session.get(\n",
    "            f\"{self.base_url}/health\",\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def classify_text(self, text: str, model_id: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Classify single text.\"\"\"\n",
    "        payload = {\n",
    "            'text': text,\n",
    "            'model_id': model_id or 'default'\n",
    "        }\n",
    "        \n",
    "        response = self.session.post(\n",
    "            f\"{self.base_url}/api/v1/classify\",\n",
    "            json=payload,\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def batch_classify(self, texts: List[str], model_id: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Classify multiple texts in batch.\"\"\"\n",
    "        payload = {\n",
    "            'texts': texts,\n",
    "            'model_id': model_id or 'default',\n",
    "            'return_probabilities': True\n",
    "        }\n",
    "        \n",
    "        response = self.session.post(\n",
    "            f\"{self.base_url}/api/v1/classify/batch\",\n",
    "            json=payload,\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def get_models(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get available models.\"\"\"\n",
    "        response = self.session.get(\n",
    "            f\"{self.base_url}/api/v1/models\",\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()['models']\n",
    "    \n",
    "    def get_metrics(self, model_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get model metrics.\"\"\"\n",
    "        response = self.session.get(\n",
    "            f\"{self.base_url}/api/v1/metrics/{model_id}\",\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "# Initialize REST client\n",
    "rest_client = RESTAPIClient(\n",
    "    base_url=API_CONFIG['rest_base_url'],\n",
    "    api_key=API_CONFIG['api_key'],\n",
    "    timeout=API_CONFIG['timeout']\n",
    ")\n",
    "\n",
    "# Test health check\n",
    "print(\"Testing REST API Connection:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    health_status = rest_client.health_check()\n",
    "    print(f\"API Status: {health_status.get('status', 'unknown')}\")\n",
    "    print(f\"Version: {health_status.get('version', 'unknown')}\")\n",
    "    print(f\"Uptime: {health_status.get('uptime', 'unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to REST API: {e}\")\n",
    "    print(\"Please ensure the API server is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. REST API Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts for testing\n",
    "sample_texts = [\n",
    "    \"Apple reported record quarterly revenue driven by strong iPhone sales.\",\n",
    "    \"Scientists discover new exoplanet using advanced telescope technology.\",\n",
    "    \"The Lakers defeated the Celtics in overtime thriller last night.\",\n",
    "    \"UN Security Council meets to discuss ongoing international crisis.\"\n",
    "]\n",
    "\n",
    "print(\"REST API Classification Examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Single text classification\n",
    "print(\"\\n1. Single Text Classification:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "try:\n",
    "    result = rest_client.classify_text(sample_texts[0])\n",
    "    print(f\"Text: {sample_texts[0][:50]}...\")\n",
    "    print(f\"Predicted Category: {result['category']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"Processing Time: {result.get('processing_time_ms', 'N/A')} ms\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Batch classification\n",
    "print(\"\\n2. Batch Classification:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "try:\n",
    "    batch_results = rest_client.batch_classify(sample_texts)\n",
    "    \n",
    "    for i, result in enumerate(batch_results['predictions']):\n",
    "        print(f\"\\nText {i+1}: {sample_texts[i][:50]}...\")\n",
    "        print(f\"  Category: {result['category']}\")\n",
    "        print(f\"  Confidence: {result['confidence']:.3f}\")\n",
    "        if 'probabilities' in result:\n",
    "            print(f\"  Probabilities:\")\n",
    "            for cls, prob in result['probabilities'].items():\n",
    "                print(f\"    {cls}: {prob:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Get available models\n",
    "print(\"\\n3. Available Models:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "try:\n",
    "    models = rest_client.get_models()\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\nModel ID: {model['id']}\")\n",
    "        print(f\"  Name: {model['name']}\")\n",
    "        print(f\"  Type: {model['type']}\")\n",
    "        print(f\"  Status: {model['status']}\")\n",
    "        print(f\"  Accuracy: {model.get('accuracy', 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. gRPC API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPCClient:\n",
    "    \"\"\"\n",
    "    gRPC client for AG News classification service.\n",
    "    \n",
    "    Following gRPC best practices from:\n",
    "        Google Cloud (2023): \"gRPC Best Practices\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host: str, port: int, api_key: str):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.api_key = api_key\n",
    "        self.channel = None\n",
    "        self.classification_stub = None\n",
    "        self.model_stub = None\n",
    "        self._connect()\n",
    "    \n",
    "    def _connect(self):\n",
    "        \"\"\"Establish gRPC connection.\"\"\"\n",
    "        # Create channel with interceptors\n",
    "        self.channel = grpc.insecure_channel(f'{self.host}:{self.port}')\n",
    "        \n",
    "        # Add metadata for authentication\n",
    "        metadata = [('authorization', f'Bearer {self.api_key}')]\n",
    "        \n",
    "        # Create stubs\n",
    "        self.classification_stub = classification_pb2_grpc.ClassificationServiceStub(self.channel)\n",
    "        self.model_stub = model_management_pb2_grpc.ModelManagementServiceStub(self.channel)\n",
    "    \n",
    "    def classify(self, text: str, model_id: str = 'default') -> classification_pb2.ClassificationResponse:\n",
    "        \"\"\"Classify text using gRPC.\"\"\"\n",
    "        request = classification_pb2.ClassificationRequest(\n",
    "            text=text,\n",
    "            model_id=model_id\n",
    "        )\n",
    "        \n",
    "        metadata = [('authorization', f'Bearer {self.api_key}')]\n",
    "        response = self.classification_stub.Classify(request, metadata=metadata)\n",
    "        return response\n",
    "    \n",
    "    def stream_classify(self, texts: List[str]) -> Any:\n",
    "        \"\"\"Stream classification requests.\"\"\"\n",
    "        def request_generator():\n",
    "            for text in texts:\n",
    "                yield classification_pb2.ClassificationRequest(\n",
    "                    text=text,\n",
    "                    model_id='default'\n",
    "                )\n",
    "        \n",
    "        metadata = [('authorization', f'Bearer {self.api_key}')]\n",
    "        responses = self.classification_stub.StreamClassify(\n",
    "            request_generator(),\n",
    "            metadata=metadata\n",
    "        )\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def get_model_info(self, model_id: str) -> model_management_pb2.ModelInfo:\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        request = model_management_pb2.GetModelRequest(model_id=model_id)\n",
    "        metadata = [('authorization', f'Bearer {self.api_key}')]\n",
    "        response = self.model_stub.GetModel(request, metadata=metadata)\n",
    "        return response\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close gRPC channel.\"\"\"\n",
    "        if self.channel:\n",
    "            self.channel.close()\n",
    "\n",
    "\n",
    "# Initialize gRPC client\n",
    "print(\"gRPC API Setup:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    grpc_client = GRPCClient(\n",
    "        host=API_CONFIG['grpc_host'],\n",
    "        port=API_CONFIG['grpc_port'],\n",
    "        api_key=API_CONFIG['api_key']\n",
    "    )\n",
    "    print(f\"Connected to gRPC server: {API_CONFIG['grpc_host']}:{API_CONFIG['grpc_port']}\")\n",
    "    \n",
    "    # Test classification\n",
    "    print(\"\\nTesting gRPC Classification:\")\n",
    "    response = grpc_client.classify(sample_texts[0])\n",
    "    print(f\"Text: {sample_texts[0][:50]}...\")\n",
    "    print(f\"Category: {response.category}\")\n",
    "    print(f\"Confidence: {response.confidence:.3f}\")\n",
    "    \n",
    "    # Test streaming\n",
    "    print(\"\\nTesting Streaming Classification:\")\n",
    "    stream_responses = grpc_client.stream_classify(sample_texts[:2])\n",
    "    \n",
    "    for i, response in enumerate(stream_responses):\n",
    "        print(f\"  Result {i+1}: {response.category} (confidence: {response.confidence:.3f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to gRPC server: {e}\")\n",
    "    print(\"Please ensure the gRPC server is running.\")\n",
    "    grpc_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GraphQL API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphQLClient:\n",
    "    \"\"\"\n",
    "    GraphQL client for AG News classification service.\n",
    "    \n",
    "    Following GraphQL best practices from:\n",
    "        Porcello & Banks (2018): \"Learning GraphQL\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, url: str, api_key: str):\n",
    "        self.url = url\n",
    "        self.api_key = api_key\n",
    "        self.client = self._create_client()\n",
    "    \n",
    "    def _create_client(self) -> Client:\n",
    "        \"\"\"Create GraphQL client.\"\"\"\n",
    "        transport = HTTPXTransport(\n",
    "            url=self.url,\n",
    "            headers={'Authorization': f'Bearer {self.api_key}'}\n",
    "        )\n",
    "        return Client(transport=transport, fetch_schema_from_transport=True)\n",
    "    \n",
    "    def classify(self, text: str, model_id: str = 'default') -> Dict[str, Any]:\n",
    "        \"\"\"Classify text using GraphQL.\"\"\"\n",
    "        query = gql(\"\"\"\n",
    "            mutation ClassifyText($text: String!, $modelId: String) {\n",
    "                classify(text: $text, modelId: $modelId) {\n",
    "                    category\n",
    "                    confidence\n",
    "                    probabilities {\n",
    "                        category\n",
    "                        probability\n",
    "                    }\n",
    "                    processingTime\n",
    "                }\n",
    "            }\n",
    "        \"\"\")\n",
    "        \n",
    "        variables = {'text': text, 'modelId': model_id}\n",
    "        result = self.client.execute(query, variable_values=variables)\n",
    "        return result['classify']\n",
    "    \n",
    "    def batch_classify(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Batch classify texts.\"\"\"\n",
    "        query = gql(\"\"\"\n",
    "            mutation BatchClassify($texts: [String!]!) {\n",
    "                batchClassify(texts: $texts) {\n",
    "                    predictions {\n",
    "                        category\n",
    "                        confidence\n",
    "                        index\n",
    "                    }\n",
    "                    totalProcessingTime\n",
    "                }\n",
    "            }\n",
    "        \"\"\")\n",
    "        \n",
    "        variables = {'texts': texts}\n",
    "        result = self.client.execute(query, variable_values=variables)\n",
    "        return result['batchClassify']['predictions']\n",
    "    \n",
    "    def get_models(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get available models.\"\"\"\n",
    "        query = gql(\"\"\"\n",
    "            query GetModels {\n",
    "                models {\n",
    "                    id\n",
    "                    name\n",
    "                    type\n",
    "                    status\n",
    "                    accuracy\n",
    "                    lastUpdated\n",
    "                }\n",
    "            }\n",
    "        \"\"\")\n",
    "        \n",
    "        result = self.client.execute(query)\n",
    "        return result['models']\n",
    "    \n",
    "    def subscribe_to_predictions(self) -> Any:\n",
    "        \"\"\"Subscribe to real-time predictions.\"\"\"\n",
    "        subscription = gql(\"\"\"\n",
    "            subscription OnPrediction {\n",
    "                predictionMade {\n",
    "                    text\n",
    "                    category\n",
    "                    confidence\n",
    "                    timestamp\n",
    "                }\n",
    "            }\n",
    "        \"\"\")\n",
    "        \n",
    "        # This would return an async iterator in production\n",
    "        return subscription\n",
    "\n",
    "\n",
    "# Initialize GraphQL client\n",
    "print(\"GraphQL API Setup:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    graphql_client = GraphQLClient(\n",
    "        url=API_CONFIG['graphql_url'],\n",
    "        api_key=API_CONFIG['api_key']\n",
    "    )\n",
    "    print(f\"Connected to GraphQL endpoint: {API_CONFIG['graphql_url']}\")\n",
    "    \n",
    "    # Test classification\n",
    "    print(\"\\nTesting GraphQL Classification:\")\n",
    "    result = graphql_client.classify(sample_texts[1])\n",
    "    print(f\"Text: {sample_texts[1][:50]}...\")\n",
    "    print(f\"Category: {result['category']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    \n",
    "    if 'probabilities' in result:\n",
    "        print(\"Probabilities:\")\n",
    "        for prob in result['probabilities']:\n",
    "            print(f\"  {prob['category']}: {prob['probability']:.3f}\")\n",
    "    \n",
    "    # Test batch classification\n",
    "    print(\"\\nTesting Batch Classification:\")\n",
    "    batch_results = graphql_client.batch_classify(sample_texts[:2])\n",
    "    \n",
    "    for result in batch_results:\n",
    "        print(f\"  Index {result['index']}: {result['category']} ({result['confidence']:.3f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to GraphQL API: {e}\")\n",
    "    print(\"Please ensure the GraphQL server is running.\")\n",
    "    graphql_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. WebSocket Real-time Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSocketClient:\n",
    "    \"\"\"\n",
    "    WebSocket client for real-time classification.\n",
    "    \n",
    "    Following WebSocket protocol from:\n",
    "        RFC 6455: \"The WebSocket Protocol\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, url: str, api_key: str):\n",
    "        self.url = url\n",
    "        self.api_key = api_key\n",
    "        self.ws = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish WebSocket connection.\"\"\"\n",
    "        self.ws = websocket.WebSocketApp(\n",
    "            self.url,\n",
    "            header={'Authorization': f'Bearer {self.api_key}'},\n",
    "            on_open=self.on_open,\n",
    "            on_message=self.on_message,\n",
    "            on_error=self.on_error,\n",
    "            on_close=self.on_close\n",
    "        )\n",
    "    \n",
    "    def on_open(self, ws):\n",
    "        \"\"\"Handle connection open.\"\"\"\n",
    "        print(\"WebSocket connection established\")\n",
    "    \n",
    "    def on_message(self, ws, message):\n",
    "        \"\"\"Handle incoming message.\"\"\"\n",
    "        data = json.loads(message)\n",
    "        print(f\"Received: {data}\")\n",
    "    \n",
    "    def on_error(self, ws, error):\n",
    "        \"\"\"Handle error.\"\"\"\n",
    "        print(f\"WebSocket error: {error}\")\n",
    "    \n",
    "    def on_close(self, ws, close_status_code, close_msg):\n",
    "        \"\"\"Handle connection close.\"\"\"\n",
    "        print(\"WebSocket connection closed\")\n",
    "    \n",
    "    def send_for_classification(self, text: str):\n",
    "        \"\"\"Send text for classification.\"\"\"\n",
    "        if self.ws:\n",
    "            message = json.dumps({\n",
    "                'action': 'classify',\n",
    "                'text': text,\n",
    "                'timestamp': time.time()\n",
    "            })\n",
    "            self.ws.send(message)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close WebSocket connection.\"\"\"\n",
    "        if self.ws:\n",
    "            self.ws.close()\n",
    "\n",
    "\n",
    "# Demonstrate WebSocket usage\n",
    "print(\"WebSocket Real-time Communication:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "ws_url = f\"ws://localhost:8000/ws/classify\"\n",
    "\n",
    "try:\n",
    "    ws_client = WebSocketClient(ws_url, API_CONFIG['api_key'])\n",
    "    print(f\"Connecting to WebSocket: {ws_url}\")\n",
    "    \n",
    "    # Note: In production, this would run asynchronously\n",
    "    print(\"WebSocket client configured (would connect in production)\")\n",
    "    print(\"\\nWebSocket operations:\")\n",
    "    print(\"  - Real-time classification\")\n",
    "    print(\"  - Streaming predictions\")\n",
    "    print(\"  - Live model updates\")\n",
    "    print(\"  - Performance monitoring\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error setting up WebSocket: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable, Any\n",
    "import random\n",
    "\n",
    "\n",
    "class APIErrorHandler:\n",
    "    \"\"\"\n",
    "    Comprehensive error handling for API calls.\n",
    "    \n",
    "    Following error handling patterns from:\n",
    "        Nygard (2007): \"Release It! Design and Deploy Production-Ready Software\"\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def exponential_backoff(\n",
    "        func: Callable,\n",
    "        max_retries: int = 3,\n",
    "        base_delay: float = 1.0,\n",
    "        max_delay: float = 60.0\n",
    "    ) -> Any:\n",
    "        \"\"\"Retry with exponential backoff.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return func()\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                \n",
    "                # Calculate delay with jitter\n",
    "                delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                print(f\"Retrying in {delay:.2f} seconds...\")\n",
    "                time.sleep(delay)\n",
    "    \n",
    "    @staticmethod\n",
    "    def circuit_breaker(\n",
    "        func: Callable,\n",
    "        failure_threshold: int = 5,\n",
    "        recovery_timeout: float = 60.0\n",
    "    ) -> Any:\n",
    "        \"\"\"Implement circuit breaker pattern.\"\"\"\n",
    "        # Simplified circuit breaker implementation\n",
    "        failures = 0\n",
    "        last_failure_time = None\n",
    "        \n",
    "        if failures >= failure_threshold:\n",
    "            if time.time() - last_failure_time < recovery_timeout:\n",
    "                raise Exception(\"Circuit breaker is open\")\n",
    "            else:\n",
    "                failures = 0  # Reset after recovery timeout\n",
    "        \n",
    "        try:\n",
    "            result = func()\n",
    "            failures = 0  # Reset on success\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            failures += 1\n",
    "            last_failure_time = time.time()\n",
    "            raise\n",
    "\n",
    "\n",
    "# Test error handling\n",
    "print(\"Error Handling Examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simulate API call with potential failures\n",
    "def unreliable_api_call():\n",
    "    \"\"\"Simulate unreliable API call.\"\"\"\n",
    "    if random.random() < 0.7:  # 70% chance of failure\n",
    "        raise ConnectionError(\"API temporarily unavailable\")\n",
    "    return {'status': 'success', 'data': 'Classification result'}\n",
    "\n",
    "# Test exponential backoff\n",
    "print(\"\\n1. Testing Exponential Backoff:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "error_handler = APIErrorHandler()\n",
    "\n",
    "try:\n",
    "    result = error_handler.exponential_backoff(\n",
    "        unreliable_api_call,\n",
    "        max_retries=3,\n",
    "        base_delay=1.0\n",
    "    )\n",
    "    print(f\"Success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed after retries: {e}\")\n",
    "\n",
    "# Rate limiting handler\n",
    "class RateLimiter:\n",
    "    \"\"\"\n",
    "    Handle API rate limiting.\n",
    "    \n",
    "    Following rate limiting strategies from:\n",
    "        Cloud API Best Practices\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_requests: int = 100, window_seconds: int = 60):\n",
    "        self.max_requests = max_requests\n",
    "        self.window_seconds = window_seconds\n",
    "        self.requests = []\n",
    "    \n",
    "    def can_make_request(self) -> bool:\n",
    "        \"\"\"Check if request can be made.\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        # Remove old requests outside window\n",
    "        self.requests = [\n",
    "            req_time for req_time in self.requests\n",
    "            if now - req_time < self.window_seconds\n",
    "        ]\n",
    "        \n",
    "        if len(self.requests) < self.max_requests:\n",
    "            self.requests.append(now)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def wait_time(self) -> float:\n",
    "        \"\"\"Calculate wait time until next request.\"\"\"\n",
    "        if len(self.requests) < self.max_requests:\n",
    "            return 0\n",
    "        \n",
    "        oldest_request = min(self.requests)\n",
    "        wait = self.window_seconds - (time.time() - oldest_request)\n",
    "        return max(0, wait)\n",
    "\n",
    "\n",
    "# Test rate limiting\n",
    "print(\"\\n2. Testing Rate Limiting:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "rate_limiter = RateLimiter(max_requests=5, window_seconds=10)\n",
    "\n",
    "for i in range(8):\n",
    "    if rate_limiter.can_make_request():\n",
    "        print(f\"Request {i+1}: Allowed\")\n",
    "    else:\n",
    "        wait_time = rate_limiter.wait_time()\n",
    "        print(f\"Request {i+1}: Rate limited (wait {wait_time:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIPerformanceTester:\n",
    "    \"\"\"\n",
    "    Performance testing for API endpoints.\n",
    "    \n",
    "    Following performance testing practices from:\n",
    "        Molyneaux (2009): \"The Art of Application Performance Testing\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client: RESTAPIClient):\n",
    "        self.client = client\n",
    "        self.results = []\n",
    "    \n",
    "    def latency_test(\n",
    "        self,\n",
    "        text: str,\n",
    "        iterations: int = 10\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Test API latency.\"\"\"\n",
    "        latencies = []\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            start_time = time.perf_counter()\n",
    "            try:\n",
    "                _ = self.client.classify_text(text)\n",
    "                latency = (time.perf_counter() - start_time) * 1000  # Convert to ms\n",
    "                latencies.append(latency)\n",
    "            except Exception as e:\n",
    "                print(f\"Request failed: {e}\")\n",
    "        \n",
    "        if latencies:\n",
    "            return {\n",
    "                'min': min(latencies),\n",
    "                'max': max(latencies),\n",
    "                'mean': np.mean(latencies),\n",
    "                'median': np.median(latencies),\n",
    "                'p95': np.percentile(latencies, 95),\n",
    "                'p99': np.percentile(latencies, 99)\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def throughput_test(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        duration_seconds: int = 10\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Test API throughput.\"\"\"\n",
    "        start_time = time.time()\n",
    "        requests_completed = 0\n",
    "        errors = 0\n",
    "        \n",
    "        while time.time() - start_time < duration_seconds:\n",
    "            try:\n",
    "                text = random.choice(texts)\n",
    "                _ = self.client.classify_text(text)\n",
    "                requests_completed += 1\n",
    "            except Exception:\n",
    "                errors += 1\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'total_requests': requests_completed + errors,\n",
    "            'successful_requests': requests_completed,\n",
    "            'failed_requests': errors,\n",
    "            'requests_per_second': requests_completed / elapsed,\n",
    "            'success_rate': requests_completed / (requests_completed + errors) if (requests_completed + errors) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def concurrent_test(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        concurrent_requests: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Test concurrent request handling.\"\"\"\n",
    "        import concurrent.futures\n",
    "        \n",
    "        def make_request(text):\n",
    "            start = time.perf_counter()\n",
    "            try:\n",
    "                result = self.client.classify_text(text)\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'latency': (time.perf_counter() - start) * 1000,\n",
    "                    'result': result\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'latency': (time.perf_counter() - start) * 1000,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_requests) as executor:\n",
    "            futures = [executor.submit(make_request, text) for text in texts[:concurrent_requests]]\n",
    "            results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "        \n",
    "        successful = [r for r in results if r['success']]\n",
    "        failed = [r for r in results if not r['success']]\n",
    "        \n",
    "        return {\n",
    "            'total_requests': len(results),\n",
    "            'successful': len(successful),\n",
    "            'failed': len(failed),\n",
    "            'avg_latency': np.mean([r['latency'] for r in results]),\n",
    "            'max_latency': max([r['latency'] for r in results])\n",
    "        }\n",
    "\n",
    "\n",
    "# Run performance tests\n",
    "print(\"API Performance Testing:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if rest_client:\n",
    "    perf_tester = APIPerformanceTester(rest_client)\n",
    "    \n",
    "    # Latency test\n",
    "    print(\"\\n1. Latency Test (10 iterations):\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    latency_results = perf_tester.latency_test(sample_texts[0], iterations=10)\n",
    "    \n",
    "    if latency_results:\n",
    "        for metric, value in latency_results.items():\n",
    "            print(f\"  {metric:6}: {value:.2f} ms\")\n",
    "    \n",
    "    # Throughput test (simulated)\n",
    "    print(\"\\n2. Throughput Test (simulated):\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Simulated results for demonstration\n",
    "    throughput_results = {\n",
    "        'total_requests': 150,\n",
    "        'successful_requests': 145,\n",
    "        'failed_requests': 5,\n",
    "        'requests_per_second': 14.5,\n",
    "        'success_rate': 0.967\n",
    "    }\n",
    "    \n",
    "    for metric, value in throughput_results.items():\n",
    "        if 'rate' in metric:\n",
    "            print(f\"  {metric}: {value:.3f}\")\n",
    "        elif 'per_second' in metric:\n",
    "            print(f\"  {metric}: {value:.1f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    \n",
    "    # Concurrent test (simulated)\n",
    "    print(\"\\n3. Concurrent Request Test (simulated):\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    concurrent_results = {\n",
    "        'total_requests': 5,\n",
    "        'successful': 5,\n",
    "        'failed': 0,\n",
    "        'avg_latency': 125.4,\n",
    "        'max_latency': 187.2\n",
    "    }\n",
    "    \n",
    "    for metric, value in concurrent_results.items():\n",
    "        if 'latency' in metric:\n",
    "            print(f\"  {metric}: {value:.2f} ms\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "else:\n",
    "    print(\"REST client not available for performance testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. API Monitoring and Health Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIMonitor:\n",
    "    \"\"\"\n",
    "    Monitor API health and performance.\n",
    "    \n",
    "    Following monitoring practices from:\n",
    "        Turnbull (2018): \"The Art of Monitoring\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, clients: Dict[str, Any]):\n",
    "        self.clients = clients\n",
    "        self.metrics = []\n",
    "    \n",
    "    def health_check_all(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Check health of all API endpoints.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # REST API health check\n",
    "        if 'rest' in self.clients and self.clients['rest']:\n",
    "            try:\n",
    "                health = self.clients['rest'].health_check()\n",
    "                results['REST'] = {\n",
    "                    'status': 'healthy',\n",
    "                    'details': health\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results['REST'] = {\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        # gRPC health check\n",
    "        if 'grpc' in self.clients and self.clients['grpc']:\n",
    "            try:\n",
    "                # Simulate gRPC health check\n",
    "                results['gRPC'] = {\n",
    "                    'status': 'healthy',\n",
    "                    'details': {'serving': True}\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results['gRPC'] = {\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        # GraphQL health check\n",
    "        if 'graphql' in self.clients and self.clients['graphql']:\n",
    "            try:\n",
    "                # Simulate GraphQL health check\n",
    "                results['GraphQL'] = {\n",
    "                    'status': 'healthy',\n",
    "                    'details': {'schema_loaded': True}\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results['GraphQL'] = {\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def collect_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Collect API metrics.\"\"\"\n",
    "        metrics = {\n",
    "            'timestamp': time.time(),\n",
    "            'endpoints': {},\n",
    "            'aggregated': {}\n",
    "        }\n",
    "        \n",
    "        # Simulate metric collection\n",
    "        metrics['endpoints'] = {\n",
    "            'REST': {\n",
    "                'requests_total': 1523,\n",
    "                'requests_per_minute': 25.4,\n",
    "                'avg_latency_ms': 112.3,\n",
    "                'error_rate': 0.02\n",
    "            },\n",
    "            'gRPC': {\n",
    "                'requests_total': 892,\n",
    "                'requests_per_minute': 14.9,\n",
    "                'avg_latency_ms': 87.6,\n",
    "                'error_rate': 0.01\n",
    "            },\n",
    "            'GraphQL': {\n",
    "                'queries_total': 456,\n",
    "                'mutations_total': 234,\n",
    "                'avg_latency_ms': 145.2,\n",
    "                'error_rate': 0.03\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Calculate aggregated metrics\n",
    "        total_requests = sum(\n",
    "            ep.get('requests_total', 0) + ep.get('queries_total', 0) + ep.get('mutations_total', 0)\n",
    "            for ep in metrics['endpoints'].values()\n",
    "        )\n",
    "        \n",
    "        metrics['aggregated'] = {\n",
    "            'total_requests': total_requests,\n",
    "            'avg_error_rate': np.mean([ep.get('error_rate', 0) for ep in metrics['endpoints'].values()]),\n",
    "            'system_health': 'healthy' if all(\n",
    "                ep.get('error_rate', 1) < 0.05 for ep in metrics['endpoints'].values()\n",
    "            ) else 'degraded'\n",
    "        }\n",
    "        \n",
    "        self.metrics.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate monitoring report.\"\"\"\n",
    "        latest_metrics = self.collect_metrics()\n",
    "        health_status = self.health_check_all()\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"API Monitoring Report\")\n",
    "        report.append(\"=\" * 50)\n",
    "        \n",
    "        # Health status\n",
    "        report.append(\"\\nHealth Status:\")\n",
    "        for api, status in health_status.items():\n",
    "            report.append(f\"  {api}: {status['status'].upper()}\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        report.append(\"\\nPerformance Metrics:\")\n",
    "        for api, metrics in latest_metrics['endpoints'].items():\n",
    "            report.append(f\"\\n  {api}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                if isinstance(value, float):\n",
    "                    report.append(f\"    {metric}: {value:.2f}\")\n",
    "                else:\n",
    "                    report.append(f\"    {metric}: {value}\")\n",
    "        \n",
    "        # System summary\n",
    "        report.append(\"\\nSystem Summary:\")\n",
    "        for metric, value in latest_metrics['aggregated'].items():\n",
    "            if isinstance(value, float):\n",
    "                report.append(f\"  {metric}: {value:.2f}\")\n",
    "            else:\n",
    "                report.append(f\"  {metric}: {value}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "\n",
    "# Create monitor\n",
    "api_clients = {\n",
    "    'rest': rest_client if 'rest_client' in locals() else None,\n",
    "    'grpc': grpc_client if 'grpc_client' in locals() else None,\n",
    "    'graphql': graphql_client if 'graphql_client' in locals() else None\n",
    "}\n",
    "\n",
    "monitor = APIMonitor(api_clients)\n",
    "\n",
    "# Generate monitoring report\n",
    "print(monitor.generate_report())\n",
    "\n",
    "# Visualize metrics\n",
    "metrics = monitor.collect_metrics()\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# API request distribution\n",
    "api_names = list(metrics['endpoints'].keys())\n",
    "request_counts = [\n",
    "    metrics['endpoints'][api].get('requests_total', 0) +\n",
    "    metrics['endpoints'][api].get('queries_total', 0) +\n",
    "    metrics['endpoints'][api].get('mutations_total', 0)\n",
    "    for api in api_names\n",
    "]\n",
    "\n",
    "axes[0].bar(api_names, request_counts)\n",
    "axes[0].set_xlabel('API Type')\n",
    "axes[0].set_ylabel('Total Requests')\n",
    "axes[0].set_title('Request Distribution by API')\n",
    "\n",
    "# Latency comparison\n",
    "latencies = [metrics['endpoints'][api].get('avg_latency_ms', 0) for api in api_names]\n",
    "\n",
    "axes[1].bar(api_names, latencies, color='orange')\n",
    "axes[1].set_xlabel('API Type')\n",
    "axes[1].set_ylabel('Average Latency (ms)')\n",
    "axes[1].set_title('Average Latency by API')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Usage Best Practices Summary\n",
    "best_practices = {\n",
    "    'Authentication': [\n",
    "        'Always use secure API keys',\n",
    "        'Implement token refresh mechanism',\n",
    "        'Use OAuth 2.0 for production',\n",
    "        'Never expose credentials in code'\n",
    "    ],\n",
    "    'Error Handling': [\n",
    "        'Implement exponential backoff',\n",
    "        'Use circuit breaker pattern',\n",
    "        'Log all errors for debugging',\n",
    "        'Provide meaningful error messages'\n",
    "    ],\n",
    "    'Performance': [\n",
    "        'Batch requests when possible',\n",
    "        'Implement request caching',\n",
    "        'Use connection pooling',\n",
    "        'Monitor latency and throughput'\n",
    "    ],\n",
    "    'Rate Limiting': [\n",
    "        'Respect API rate limits',\n",
    "        'Implement client-side throttling',\n",
    "        'Use request queuing',\n",
    "        'Monitor quota usage'\n",
    "    ],\n",
    "    'Protocol Selection': [\n",
    "        'REST: Simple CRUD operations',\n",
    "        'gRPC: High-performance, streaming',\n",
    "        'GraphQL: Flexible queries, reduce over-fetching',\n",
    "        'WebSocket: Real-time bidirectional communication'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"API Usage Best Practices:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  - {practice}\")\n",
    "\n",
    "# Performance comparison table\n",
    "comparison_data = {\n",
    "    'Protocol': ['REST', 'gRPC', 'GraphQL', 'WebSocket'],\n",
    "    'Latency': ['Medium', 'Low', 'Medium-High', 'Very Low'],\n",
    "    'Throughput': ['Medium', 'High', 'Medium', 'High'],\n",
    "    'Complexity': ['Low', 'Medium', 'Medium-High', 'Medium'],\n",
    "    'Use Case': ['CRUD', 'Microservices', 'Flexible Queries', 'Real-time']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nProtocol Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions and Next Steps\n",
    "\n",
    "### API Usage Summary\n",
    "\n",
    "This tutorial demonstrated comprehensive API usage patterns:\n",
    "\n",
    "1. **REST API**: Simple HTTP-based communication with JSON\n",
    "2. **gRPC**: High-performance RPC with Protocol Buffers\n",
    "3. **GraphQL**: Flexible query language for precise data fetching\n",
    "4. **WebSocket**: Real-time bidirectional communication\n",
    "5. **Error Handling**: Exponential backoff, circuit breakers\n",
    "6. **Rate Limiting**: Client-side throttling and quota management\n",
    "7. **Performance Testing**: Latency, throughput, concurrent requests\n",
    "8. **Monitoring**: Health checks, metrics collection, reporting\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Protocol Selection**: Choose based on use case requirements\n",
    "2. **Error Resilience**: Always implement retry logic and fallbacks\n",
    "3. **Performance Optimization**: Batch requests and use caching\n",
    "4. **Security First**: Secure authentication and data transmission\n",
    "5. **Monitoring Essential**: Track health and performance metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Advanced Integration**:\n",
    "   - Implement API gateway patterns\n",
    "   - Use service mesh for microservices\n",
    "   - Apply API versioning strategies\n",
    "\n",
    "2. **Performance Optimization**:\n",
    "   - Implement response caching\n",
    "   - Use CDN for static content\n",
    "   - Apply request deduplication\n",
    "\n",
    "3. **Security Enhancement**:\n",
    "   - Implement mTLS for gRPC\n",
    "   - Use API key rotation\n",
    "   - Apply request signing\n",
    "\n",
    "4. **Production Deployment**:\n",
    "   - Set up load balancing\n",
    "   - Implement blue-green deployment\n",
    "   - Configure auto-scaling\n",
    "\n",
    "### References\n",
    "\n",
    "For deeper understanding, consult:\n",
    "- Service integration: `notebooks/tutorials/08_service_integration.ipynb`\n",
    "- API documentation: `docs/api_reference/`\n",
    "- Deployment guide: `docs/user_guide/deployment.md`\n",
    "- Performance tuning: `docs/operations/runbooks/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
