{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Basics for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates fundamental model training concepts following methodologies from:\n",
    "- Devlin et al. (2019): \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
    "- Liu et al. (2019): \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"\n",
    "- He et al. (2021): \"DeBERTa: Decoding-enhanced BERT with Disentangled Attention\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Load and prepare AG News dataset\n",
    "2. Configure transformer models\n",
    "3. Implement training loops\n",
    "4. Apply optimization strategies\n",
    "5. Monitor training progress\n",
    "6. Save and load checkpoints\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "# Data and ML imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.data.loaders.dataloader import create_dataloader\n",
    "from src.models.transformers.roberta.roberta_enhanced import RoBERTaEnhanced\n",
    "from src.training.trainers.base_trainer import BaseTrainer\n",
    "from src.training.callbacks.early_stopping import EarlyStopping\n",
    "from src.training.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.memory_utils import get_memory_usage, clear_memory\n",
    "from configs.constants import (\n",
    "    AG_NEWS_CLASSES,\n",
    "    AG_NEWS_NUM_CLASSES,\n",
    "    DATA_DIR,\n",
    "    MODEL_DIR\n",
    ")\n",
    "from configs.config_loader import ConfigLoader\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training configuration\n",
    "config_loader = ConfigLoader()\n",
    "training_config = config_loader.load_config('training/standard/base_training.yaml')\n",
    "\n",
    "@dataclass\n",
    "class TrainingArgs:\n",
    "    \"\"\"Training arguments following Transformers library conventions.\"\"\"\n",
    "    model_name: str = \"roberta-base\"\n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 2e-5\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_length: int = 256\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    fp16: bool = torch.cuda.is_available()\n",
    "    evaluation_steps: int = 500\n",
    "    save_steps: int = 1000\n",
    "    logging_steps: int = 100\n",
    "    \n",
    "# Override with loaded config\n",
    "args = TrainingArgs(\n",
    "    model_name=training_config.get('model_name', 'roberta-base'),\n",
    "    num_epochs=training_config.get('num_epochs', 3),\n",
    "    batch_size=training_config.get('batch_size', 16),\n",
    "    learning_rate=training_config.get('learning_rate', 2e-5)\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in args.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading AG News dataset...\")\n",
    "data_config = AGNewsConfig(\n",
    "    data_dir=DATA_DIR / \"processed\",\n",
    "    max_length=args.max_length\n",
    ")\n",
    "\n",
    "train_dataset = AGNewsDataset(data_config, split=\"train\")\n",
    "val_dataset = AGNewsDataset(data_config, split=\"validation\")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "\n",
    "# Sample data for quick training (optional for tutorial)\n",
    "SAMPLE_SIZE = 5000  # Use smaller subset for tutorial\n",
    "if len(train_dataset) > SAMPLE_SIZE:\n",
    "    print(f\"\\nUsing {SAMPLE_SIZE} samples for tutorial (full dataset: {len(train_dataset)})\")\n",
    "    train_indices = np.random.choice(len(train_dataset), SAMPLE_SIZE, replace=False)\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(f\"\\nLoading tokenizer: {args.model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text examples.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=args.max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(f\"Initializing model: {args.model_name}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model_name,\n",
    "    num_labels=AG_NEWS_NUM_CLASSES,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1e9:.2f} GB (fp32)\")\n",
    "\n",
    "# Enable mixed precision if available\n",
    "scaler = torch.cuda.amp.GradScaler() if args.fp16 else None\n",
    "if scaler:\n",
    "    print(\"  Mixed precision training: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total training steps\n",
    "total_steps = len(train_loader) * args.num_epochs\n",
    "warmup_steps = int(total_steps * args.warmup_ratio)\n",
    "\n",
    "print(\"Optimization Configuration:\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Learning rate: {args.learning_rate}\")\n",
    "print(f\"  Weight decay: {args.weight_decay}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args.learning_rate,\n",
    "    weight_decay=args.weight_decay,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nOptimizer and scheduler initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, criterion, device, scaler=None):\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "    \n",
    "    Following training practices from:\n",
    "    - Loshchilov & Hutter (2019): \"Decoupled Weight Decay Regularization\"\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        texts, labels = batch\n",
    "        inputs = tokenizer(texts, \n",
    "                          padding=True, \n",
    "                          truncation=True, \n",
    "                          max_length=args.max_length, \n",
    "                          return_tensors='pt')\n",
    "        \n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, \n",
    "                              attention_mask=attention_mask,\n",
    "                              labels=labels)\n",
    "                loss = outputs.loss\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': correct / total,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=\"Evaluating\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        texts, labels = batch\n",
    "        inputs = tokenizer(texts,\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=args.max_length,\n",
    "                          return_tensors='pt')\n",
    "        \n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_predictions, all_labels\n",
    "\n",
    "print(\"Training functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_f1': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "# Initialize callbacks\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "checkpoint_dir = MODEL_DIR / \"checkpoints\" / f\"{args.model_name.replace('/', '-')}\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{args.num_epochs}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, criterion, device, scaler\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Validation phase\n",
    "    start_time = time.time()\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    val_time = time.time() - start_time\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    print(f\"Time: Train {train_time:.1f}s | Val {val_time:.1f}s\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        checkpoint_path = checkpoint_dir / \"best_model.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1,\n",
    "            'args': args\n",
    "        }, checkpoint_path)\n",
    "        print(f\"✓ Saved best model (Val Acc: {val_acc:.4f})\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss):\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "        break\n",
    "    \n",
    "    # Memory management\n",
    "    if device.type == 'cuda':\n",
    "        clear_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f} (Epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Loss plot\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training and Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "ax.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Training and Validation Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score plot\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['val_f1'], label='Val F1', marker='D', color='green')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('Validation F1 Score')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history['learning_rate'], label='Learning Rate', marker='^', color='orange')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Progress Overview', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save training history\n",
    "history_path = checkpoint_dir / \"training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"\\nTraining history saved to: {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load and Test Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "checkpoint_path = checkpoint_dir / \"best_model.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Best validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "print(f\"Best validation F1: {checkpoint['val_f1']:.4f}\")\n",
    "\n",
    "# Test on sample texts\n",
    "test_texts = [\n",
    "    \"The stock market reached record highs today as investors celebrated positive earnings reports.\",\n",
    "    \"Scientists discover new exoplanet that could potentially harbor life.\",\n",
    "    \"The Lakers defeated the Celtics in overtime with a final score of 115-112.\",\n",
    "    \"Apple announces new iPhone with revolutionary camera technology.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting on sample texts:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in test_texts:\n",
    "        inputs = tokenizer(text, \n",
    "                          return_tensors='pt',\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=args.max_length)\n",
    "        \n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        print(f\"\\nText: {text[:80]}...\")\n",
    "        print(f\"Predicted: {AG_NEWS_CLASSES[predicted_class]} (confidence: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Summary and Next Steps\n",
    "\n",
    "### Training Summary\n",
    "\n",
    "This tutorial demonstrated fundamental concepts in training transformer models:\n",
    "\n",
    "1. **Data Preparation**: \n",
    "   - Loaded AG News dataset\n",
    "   - Created efficient data loaders\n",
    "   - Implemented proper tokenization\n",
    "\n",
    "2. **Model Configuration**:\n",
    "   - Initialized pre-trained transformer model\n",
    "   - Configured for sequence classification\n",
    "   - Enabled mixed precision training\n",
    "\n",
    "3. **Training Process**:\n",
    "   - Implemented training and evaluation loops\n",
    "   - Applied gradient accumulation\n",
    "   - Used learning rate scheduling\n",
    "   - Monitored multiple metrics\n",
    "\n",
    "4. **Best Practices Applied**:\n",
    "   - Early stopping to prevent overfitting\n",
    "   - Model checkpointing for best weights\n",
    "   - Memory management for efficiency\n",
    "   - Comprehensive logging and visualization\n",
    "\n",
    "### Advanced Training Techniques\n",
    "\n",
    "To further improve performance, consider:\n",
    "\n",
    "1. **Advanced Models**:\n",
    "   - DeBERTa-v3 for better performance\n",
    "   - Ensemble methods for robustness\n",
    "   - Domain-adapted models\n",
    "\n",
    "2. **Training Strategies**:\n",
    "   - Adversarial training for robustness\n",
    "   - Curriculum learning for efficiency\n",
    "   - Knowledge distillation for compression\n",
    "\n",
    "3. **Optimization Techniques**:\n",
    "   - SAM optimizer for better generalization\n",
    "   - Lookahead optimizer for stability\n",
    "   - Gradient clipping for training stability\n",
    "\n",
    "4. **Data Augmentation**:\n",
    "   - Back-translation for more data\n",
    "   - Paraphrasing for diversity\n",
    "   - Mixup for regularization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Evaluation**: Proceed to `04_evaluation_tutorial.ipynb` for comprehensive model evaluation\n",
    "2. **Advanced Training**: Explore prompt-based methods in `05_prompt_engineering.ipynb`\n",
    "3. **Production**: Learn API deployment in `07_api_usage.ipynb`\n",
    "4. **Optimization**: Implement efficient training with LoRA and PEFT methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
