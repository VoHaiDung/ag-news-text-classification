{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Basics for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates fundamental model training techniques following methodologies from:\n",
    "- Devlin et al. (2019): \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
    "- Liu et al. (2019): \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"\n",
    "- He et al. (2021): \"DeBERTa: Decoding-enhanced BERT with Disentangled Attention\"\n",
    "\n",
    "### Tutorial Objectives\n",
    "1. Load and prepare training data\n",
    "2. Initialize transformer models\n",
    "3. Configure training parameters\n",
    "4. Execute training loop\n",
    "5. Monitor training progress\n",
    "6. Save and evaluate models\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "\n",
    "# Data and ML imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.data.loaders.dataloader import create_dataloaders\n",
    "from src.models.transformers.deberta.deberta_v3 import DeBERTaV3Classifier\n",
    "from src.models.transformers.roberta.roberta_enhanced import RoBERTaEnhancedClassifier\n",
    "from src.training.trainers.base_trainer import BaseTrainer\n",
    "from src.training.trainers.standard_trainer import StandardTrainer\n",
    "from src.training.callbacks.early_stopping import EarlyStopping\n",
    "from src.training.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from src.utils.reproducibility import set_seed, get_reproducible_config\n",
    "from src.utils.logging_config import setup_logging\n",
    "from configs.config_loader import ConfigLoader\n",
    "from configs.constants import AG_NEWS_NUM_CLASSES, MODEL_DIR, OUTPUT_DIR\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "set_seed(42)\n",
    "logger = setup_logging(\"model_training_tutorial\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training configuration\n",
    "config_loader = ConfigLoader()\n",
    "\n",
    "# Load base training config\n",
    "training_config = config_loader.load_config('training/standard/base_training.yaml')\n",
    "\n",
    "# Load model config (using DeBERTa as example)\n",
    "model_config = config_loader.load_config('models/single/deberta_v3_xlarge.yaml')\n",
    "\n",
    "# Override for tutorial (smaller settings for demonstration)\n",
    "tutorial_overrides = {\n",
    "    'batch_size': 8,  # Smaller batch for memory\n",
    "    'num_epochs': 3,  # Fewer epochs for speed\n",
    "    'max_samples': 1000,  # Limit samples for tutorial\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'eval_steps': 50,\n",
    "    'save_steps': 100,\n",
    "    'logging_steps': 10,\n",
    "    'warmup_steps': 100\n",
    "}\n",
    "\n",
    "# Apply overrides\n",
    "for key, value in tutorial_overrides.items():\n",
    "    if key in training_config:\n",
    "        training_config[key] = value\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in training_config.items():\n",
    "    if not isinstance(value, dict):\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "model_name = model_config.get('model_name', 'microsoft/deberta-v3-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Tokenizer: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Load datasets\n",
    "data_config = AGNewsConfig(\n",
    "    max_samples=tutorial_overrides['max_samples'],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=model_config.get('max_length', 256)\n",
    ")\n",
    "\n",
    "print(\"\\nLoading datasets...\")\n",
    "train_dataset = AGNewsDataset(data_config, split='train')\n",
    "val_dataset = AGNewsDataset(data_config, split='validation')\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader, val_dataloader = create_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_dataloader)}\")\n",
    "print(f\"  Validation batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "def initialize_model(model_type: str = 'deberta') -> nn.Module:\n",
    "    \"\"\"\n",
    "    Initialize a transformer model for classification.\n",
    "    \n",
    "    Following initialization strategies from:\n",
    "        Glorot & Bengio (2010): \"Understanding the difficulty of training deep feedforward neural networks\"\n",
    "    \"\"\"\n",
    "    if model_type == 'deberta':\n",
    "        model = DeBERTaV3Classifier(\n",
    "            model_name=model_name,\n",
    "            num_labels=AG_NEWS_NUM_CLASSES,\n",
    "            dropout_rate=model_config.get('dropout_rate', 0.1),\n",
    "            use_pooler=model_config.get('use_pooler', False)\n",
    "        )\n",
    "    elif model_type == 'roberta':\n",
    "        model = RoBERTaEnhancedClassifier(\n",
    "            model_name='roberta-base',\n",
    "            num_labels=AG_NEWS_NUM_CLASSES,\n",
    "            dropout_rate=model_config.get('dropout_rate', 0.1)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = initialize_model('deberta')\n",
    "model = model.to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Information:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model type: DeBERTa-v3\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB (fp32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer\n",
    "from src.training.optimization.optimizers.adamw_custom import create_adamw_optimizer\n",
    "\n",
    "optimizer = create_adamw_optimizer(\n",
    "    model=model,\n",
    "    learning_rate=training_config.get('learning_rate', 2e-5),\n",
    "    weight_decay=training_config.get('weight_decay', 0.01),\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "from src.training.optimization.schedulers.cosine_warmup import CosineWarmupScheduler\n",
    "\n",
    "num_training_steps = len(train_dataloader) * training_config['num_epochs']\n",
    "scheduler = CosineWarmupScheduler(\n",
    "    optimizer=optimizer,\n",
    "    warmup_steps=training_config['warmup_steps'],\n",
    "    total_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Setup loss function\n",
    "from src.training.objectives.losses.label_smoothing import LabelSmoothingCrossEntropy\n",
    "\n",
    "criterion = LabelSmoothingCrossEntropy(\n",
    "    num_classes=AG_NEWS_NUM_CLASSES,\n",
    "    smoothing=training_config.get('label_smoothing', 0.1)\n",
    ")\n",
    "\n",
    "print(\"Training Setup:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"Learning rate: {training_config.get('learning_rate', 2e-5)}\")\n",
    "print(f\"Scheduler: Cosine with warmup\")\n",
    "print(f\"Warmup steps: {training_config['warmup_steps']}\")\n",
    "print(f\"Total training steps: {num_training_steps}\")\n",
    "print(f\"Loss function: Label Smoothing Cross-Entropy\")\n",
    "print(f\"Label smoothing: {training_config.get('label_smoothing', 0.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = []\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=training_config.get('early_stopping_patience', 3),\n",
    "    min_delta=training_config.get('early_stopping_delta', 0.001),\n",
    "    mode='max',  # For accuracy\n",
    "    verbose=True\n",
    ")\n",
    "callbacks.append(early_stopping)\n",
    "\n",
    "# Model checkpoint\n",
    "checkpoint_dir = OUTPUT_DIR / \"checkpoints\" / \"tutorial\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    directory=checkpoint_dir,\n",
    "    filename_prefix=\"deberta_v3\",\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=True\n",
    ")\n",
    "callbacks.append(model_checkpoint)\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"[CONFIGURED] Early Stopping (patience={training_config.get('early_stopping_patience', 3)})\")\n",
    "print(f\"[CONFIGURED] Model Checkpoint (saving to {checkpoint_dir})\")\n",
    "print(f\"[CONFIGURED] Learning Rate Monitor\")\n",
    "print(f\"[CONFIGURED] Gradient Norm Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = StandardTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    callbacks=callbacks,\n",
    "    gradient_clip_val=training_config.get('gradient_clip_val', 1.0),\n",
    "    gradient_accumulation_steps=training_config.get('gradient_accumulation_steps', 1),\n",
    "    mixed_precision=training_config.get('mixed_precision', False)\n",
    ")\n",
    "\n",
    "# Training history storage\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(training_config['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{training_config['num_epochs']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    train_metrics = trainer.train_epoch(\n",
    "        train_dataloader=train_dataloader,\n",
    "        epoch=epoch\n",
    "    )\n",
    "    \n",
    "    # Validation phase\n",
    "    val_metrics = trainer.validate(\n",
    "        val_dataloader=val_dataloader\n",
    "    )\n",
    "    \n",
    "    # Store metrics\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "    history['learning_rate'].append(scheduler.get_last_lr()[0])\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_metrics['loss']:.4f} | Train Acc: {train_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    # Check early stopping\n",
    "    if early_stopping.check_stop(val_metrics['accuracy']):\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['train_accuracy'], label='Train Accuracy', marker='o')\n",
    "axes[1].plot(history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot\n",
    "axes[2].plot(history['learning_rate'], label='Learning Rate', marker='d', color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Progress', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Validation Accuracy: {max(history['val_accuracy']):.4f}\")\n",
    "print(f\"Final Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation on validation set\n",
    "from src.evaluation.metrics.classification_metrics import ClassificationMetrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_calculator = ClassificationMetrics(num_classes=AG_NEWS_NUM_CLASSES)\n",
    "metrics = metrics_calculator.compute_metrics(\n",
    "    predictions=np.array(all_predictions),\n",
    "    labels=np.array(all_labels),\n",
    "    probabilities=np.array(all_probs)\n",
    ")\n",
    "\n",
    "print(\"Detailed Evaluation Metrics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "print(f\"Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
    "print(f\"Macro Precision: {metrics['macro_precision']:.4f}\")\n",
    "print(f\"Macro Recall: {metrics['macro_recall']:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "from configs.constants import ID_TO_LABEL\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(\n",
    "    all_labels, \n",
    "    all_predictions, \n",
    "    target_names=[ID_TO_LABEL[i] for i in range(AG_NEWS_NUM_CLASSES)]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = MODEL_DIR / \"tutorial\" / \"deberta_v3_trained\"\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "torch.save({\n",
    "    'epoch': training_config['num_epochs'],\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'best_accuracy': max(history['val_accuracy']),\n",
    "    'history': history,\n",
    "    'config': training_config\n",
    "}, save_path / \"checkpoint.pt\")\n",
    "\n",
    "print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_path / \"tokenizer\")\n",
    "print(f\"Tokenizer saved to: {save_path / 'tokenizer'}\")\n",
    "\n",
    "# Demonstrate loading\n",
    "print(\"\\nLoading saved model...\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(save_path / \"checkpoint.pt\", map_location=device)\n",
    "\n",
    "# Create new model instance\n",
    "loaded_model = initialize_model('deberta')\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Best accuracy from checkpoint: {checkpoint['best_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions and Next Steps\n",
    "\n",
    "### Training Summary\n",
    "\n",
    "This tutorial demonstrated fundamental model training concepts:\n",
    "\n",
    "1. **Data Preparation**: Loaded and preprocessed AG News dataset\n",
    "2. **Model Initialization**: Set up DeBERTa-v3 classifier\n",
    "3. **Training Configuration**: Configured optimizer, scheduler, and loss\n",
    "4. **Training Loop**: Executed training with validation\n",
    "5. **Monitoring**: Tracked metrics and visualized progress\n",
    "6. **Evaluation**: Computed comprehensive metrics\n",
    "7. **Persistence**: Saved and loaded trained models\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Hyperparameters Matter**: Learning rate, batch size, and warmup significantly impact performance\n",
    "2. **Early Stopping**: Prevents overfitting and saves computation\n",
    "3. **Label Smoothing**: Improves generalization for classification\n",
    "4. **Gradient Accumulation**: Enables larger effective batch sizes\n",
    "5. **Mixed Precision**: Speeds up training with minimal accuracy loss\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Advanced Training**:\n",
    "   - Try different models (RoBERTa, XLNet)\n",
    "   - Implement ensemble training\n",
    "   - Use adversarial training\n",
    "\n",
    "2. **Optimization**:\n",
    "   - Hyperparameter tuning with Optuna\n",
    "   - Learning rate scheduling experiments\n",
    "   - Different loss functions\n",
    "\n",
    "3. **Efficiency**:\n",
    "   - LoRA fine-tuning\n",
    "   - Quantization-aware training\n",
    "   - Knowledge distillation\n",
    "\n",
    "4. **Production**:\n",
    "   - Model optimization for inference\n",
    "   - API deployment\n",
    "   - Monitoring and maintenance\n",
    "\n",
    "### References\n",
    "\n",
    "For deeper understanding, consult:\n",
    "- Training documentation: `docs/user_guide/model_training.md`\n",
    "- Advanced techniques: `notebooks/tutorials/06_instruction_tuning.ipynb`\n",
    "- API deployment: `notebooks/tutorials/07_api_usage.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
