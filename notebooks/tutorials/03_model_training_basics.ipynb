{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Basics for AG News Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates fundamental model training concepts following methodologies from:\n",
    "- Goodfellow et al. (2016): \"Deep Learning\"\n",
    "- Smith (2018): \"A Disciplined Approach to Neural Network Hyper-Parameters\"\n",
    "- Howard & Ruder (2018): \"Universal Language Model Fine-tuning for Text Classification\"\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand the training loop architecture\n",
    "2. Implement basic model training with transformers\n",
    "3. Apply optimization strategies and learning rate scheduling\n",
    "4. Monitor training progress and prevent overfitting\n",
    "5. Save and load model checkpoints\n",
    "\n",
    "Author: Võ Hải Dũng  \n",
    "Email: vohaidung.work@gmail.com  \n",
    "Date: 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project setup\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Project imports\n",
    "from src.data.datasets.ag_news import AGNewsDataset, AGNewsConfig\n",
    "from src.models.base.base_model import BaseModel\n",
    "from src.training.trainers.base_trainer import BaseTrainer\n",
    "from src.evaluation.metrics.classification_metrics import ClassificationMetrics\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.logging_config import setup_logging\n",
    "from configs.constants import (\n",
    "    AG_NEWS_NUM_CLASSES,\n",
    "    DATA_DIR,\n",
    "    MODEL_DIR\n",
    ")\n",
    "\n",
    "# Setup\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_config = AGNewsConfig(\n",
    "    data_dir=DATA_DIR / \"processed\",\n",
    "    max_samples=1000,  # Use subset for tutorial\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_dataset = AGNewsDataset(dataset_config, split=\"train\")\n",
    "    val_dataset = AGNewsDataset(dataset_config, split=\"validation\")\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "except:\n",
    "    print(\"Using synthetic data for demonstration\")\n",
    "    # Create synthetic dataset\n",
    "    class SyntheticDataset(Dataset):\n",
    "        def __init__(self, size=1000):\n",
    "            self.texts = [f\"Sample text {i}\" for i in range(size)]\n",
    "            self.labels = torch.randint(0, AG_NEWS_NUM_CLASSES, (size,))\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                'text': self.texts[idx],\n",
    "                'label': self.labels[idx]\n",
    "            }\n",
    "    \n",
    "    train_dataset = SyntheticDataset(800)\n",
    "    val_dataset = SyntheticDataset(200)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple neural network classifier for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize classifier.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input dimension\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            num_classes: Number of output classes\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Logits tensor [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "simple_model = SimpleClassifier(\n",
    "    input_dim=768,  # BERT embedding dimension\n",
    "    hidden_dim=256,\n",
    "    num_classes=AG_NEWS_NUM_CLASSES,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "simple_model = simple_model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in simple_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in simple_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration.\"\"\"\n",
    "    num_epochs: int = 3\n",
    "    learning_rate: float = 2e-5\n",
    "    warmup_steps: int = 100\n",
    "    weight_decay: float = 0.01\n",
    "    gradient_clip_norm: float = 1.0\n",
    "    log_interval: int = 10\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, config):\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        dataloader: Training data loader\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        device: Device to use\n",
    "        config: Training configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Prepare batch (simplified for demo)\n",
    "        # In real scenario, tokenize texts here\n",
    "        inputs = torch.randn(len(batch['label']), 768).to(device)  # Dummy embeddings\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip_norm)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if batch_idx % config.log_interval == 0:\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * correct / total:.2f}%',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': 100 * correct / total\n",
    "    }\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Validate model.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to validate\n",
    "        dataloader: Validation data loader\n",
    "        device: Device to use\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            # Prepare batch (simplified)\n",
    "            inputs = torch.randn(len(batch['label']), 768).to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': 100 * correct / total\n",
    "    }\n",
    "\n",
    "# Setup training\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(\n",
    "    simple_model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Initialize scheduler\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Warmup steps: {config.warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Best model tracking\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = train_epoch(\n",
    "        simple_model, train_loader, optimizer, scheduler, device, config\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = validate(simple_model, val_loader, device)\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['train_acc'].append(train_metrics['accuracy'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['accuracy'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['accuracy']\n",
    "        best_model_state = simple_model.state_dict().copy()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['accuracy']:.2f}%\")\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze training behavior\n",
    "print(\"Training Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for overfitting\n",
    "final_train_acc = history['train_acc'][-1]\n",
    "final_val_acc = history['val_acc'][-1]\n",
    "gap = final_train_acc - final_val_acc\n",
    "\n",
    "if gap > 10:\n",
    "    print(f\"Warning: Potential overfitting detected (gap: {gap:.2f}%)\")\n",
    "    print(\"Recommendations:\")\n",
    "    print(\"  - Increase dropout\")\n",
    "    print(\"  - Add regularization\")\n",
    "    print(\"  - Use data augmentation\")\n",
    "    print(\"  - Reduce model complexity\")\n",
    "elif gap < 2:\n",
    "    print(f\"Model is well-generalized (gap: {gap:.2f}%)\")\n",
    "    print(\"Could potentially increase model capacity\")\n",
    "else:\n",
    "    print(f\"Reasonable generalization (gap: {gap:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transformer Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained transformer model\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Use a small model for demonstration\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transformer_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=AG_NEWS_NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "transformer_model = transformer_model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in transformer_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTransformer model loaded:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Training configuration for transformer\n",
    "@dataclass\n",
    "class TransformerTrainingConfig:\n",
    "    num_epochs: int = 2\n",
    "    learning_rate: float = 5e-5\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_length: int = 128\n",
    "    batch_size: int = 8\n",
    "\n",
    "transformer_config = TransformerTrainingConfig()\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Epochs: {transformer_config.num_epochs}\")\n",
    "print(f\"  Learning rate: {transformer_config.learning_rate}\")\n",
    "print(f\"  Batch size: {transformer_config.batch_size}\")\n",
    "print(f\"  Max sequence length: {transformer_config.max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCheckpoint:\n",
    "    \"\"\"\n",
    "    Model checkpointing utility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, save_dir: Path, model_name: str):\n",
    "        \"\"\"\n",
    "        Initialize checkpoint manager.\n",
    "        \n",
    "        Args:\n",
    "            save_dir: Directory to save checkpoints\n",
    "            model_name: Name of the model\n",
    "        \"\"\"\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.model_name = model_name\n",
    "        self.best_metric = float('-inf')\n",
    "    \n",
    "    def save(self, model, optimizer, scheduler, epoch, metrics, is_best=False):\n",
    "        \"\"\"\n",
    "        Save model checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to save\n",
    "            optimizer: Optimizer state\n",
    "            scheduler: Scheduler state\n",
    "            epoch: Current epoch\n",
    "            metrics: Current metrics\n",
    "            is_best: Whether this is the best model\n",
    "        \"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        latest_path = self.save_dir / f\"{self.model_name}_latest.pt\"\n",
    "        torch.save(checkpoint, latest_path)\n",
    "        \n",
    "        # Save best checkpoint if applicable\n",
    "        if is_best:\n",
    "            best_path = self.save_dir / f\"{self.model_name}_best.pt\"\n",
    "            torch.save(checkpoint, best_path)\n",
    "            self.best_metric = metrics.get('val_accuracy', metrics.get('val_loss', 0))\n",
    "            print(f\"Saved best model with metric: {self.best_metric:.4f}\")\n",
    "    \n",
    "    def load(self, model, optimizer=None, scheduler=None, checkpoint_type='best'):\n",
    "        \"\"\"\n",
    "        Load model checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to load weights into\n",
    "            optimizer: Optimizer to load state\n",
    "            scheduler: Scheduler to load state\n",
    "            checkpoint_type: 'best' or 'latest'\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with checkpoint information\n",
    "        \"\"\"\n",
    "        checkpoint_path = self.save_dir / f\"{self.model_name}_{checkpoint_type}.pt\"\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            print(f\"No checkpoint found at {checkpoint_path}\")\n",
    "            return None\n",
    "        \n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "        print(f\"Metrics: {checkpoint['metrics']}\")\n",
    "        \n",
    "        return checkpoint\n",
    "\n",
    "# Create checkpoint manager\n",
    "checkpoint_manager = ModelCheckpoint(\n",
    "    save_dir=MODEL_DIR / \"checkpoints\",\n",
    "    model_name=\"simple_classifier\"\n",
    ")\n",
    "\n",
    "# Save the best model from training\n",
    "if best_model_state:\n",
    "    simple_model.load_state_dict(best_model_state)\n",
    "    checkpoint_manager.save(\n",
    "        model=simple_model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epoch=config.num_epochs,\n",
    "        metrics={'val_accuracy': best_val_acc},\n",
    "        is_best=True\n",
    "    )\n",
    "    print(f\"\\nBest model saved to {checkpoint_manager.save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training best practices summary\n",
    "best_practices = {\n",
    "    \"Data Preparation\": [\n",
    "        \"Use stratified splits to maintain class balance\",\n",
    "        \"Apply appropriate data augmentation\",\n",
    "        \"Normalize/standardize inputs consistently\",\n",
    "        \"Use proper train/val/test splits\"\n",
    "    ],\n",
    "    \"Model Selection\": [\n",
    "        \"Start with pre-trained models when available\",\n",
    "        \"Choose architecture based on task requirements\",\n",
    "        \"Consider model size vs performance trade-offs\",\n",
    "        \"Use ensemble methods for better performance\"\n",
    "    ],\n",
    "    \"Training Configuration\": [\n",
    "        \"Use learning rate scheduling (warmup + decay)\",\n",
    "        \"Apply gradient clipping for stability\",\n",
    "        \"Use mixed precision training for efficiency\",\n",
    "        \"Monitor multiple metrics during training\"\n",
    "    ],\n",
    "    \"Optimization\": [\n",
    "        \"Use AdamW optimizer for transformers\",\n",
    "        \"Apply weight decay for regularization\",\n",
    "        \"Tune learning rate as primary hyperparameter\",\n",
    "        \"Use gradient accumulation for larger effective batch sizes\"\n",
    "    ],\n",
    "    \"Monitoring\": [\n",
    "        \"Track training and validation metrics\",\n",
    "        \"Use early stopping to prevent overfitting\",\n",
    "        \"Save checkpoints regularly\",\n",
    "        \"Log experiments for reproducibility\"\n",
    "    ],\n",
    "    \"Debugging\": [\n",
    "        \"Start with small data subset\",\n",
    "        \"Verify model can overfit single batch\",\n",
    "        \"Check gradient flow and magnitudes\",\n",
    "        \"Visualize predictions on samples\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Training Best Practices\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for i, practice in enumerate(practices, 1):\n",
    "        print(f\"  {i}. {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"Training Tutorial Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nWhat we covered:\")\n",
    "print(\"1. Basic model architecture implementation\")\n",
    "print(\"2. Training loop with optimization\")\n",
    "print(\"3. Learning rate scheduling strategies\")\n",
    "print(\"4. Model checkpointing and saving\")\n",
    "print(\"5. Training monitoring and visualization\")\n",
    "print(\"6. Best practices for model training\")\n",
    "\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"- Start simple and gradually increase complexity\")\n",
    "print(\"- Monitor training closely to detect issues early\")\n",
    "print(\"- Use pre-trained models when possible\")\n",
    "print(\"- Apply regularization to prevent overfitting\")\n",
    "print(\"- Save checkpoints for model recovery\")\n",
    "print(\"- Document experiments for reproducibility\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Train full transformer model on complete dataset\")\n",
    "print(\"2. Implement advanced training strategies (adversarial, curriculum)\")\n",
    "print(\"3. Explore model ensembling techniques\")\n",
    "print(\"4. Optimize hyperparameters systematically\")\n",
    "print(\"5. Deploy trained model for inference\")\n",
    "\n",
    "# Save training report\n",
    "training_report = {\n",
    "    'model_type': 'simple_classifier',\n",
    "    'config': config.__dict__,\n",
    "    'history': history,\n",
    "    'best_val_accuracy': best_val_acc,\n",
    "    'device': str(device),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "report_path = PROJECT_ROOT / \"outputs\" / \"training\" / \"tutorial_report.json\"\n",
    "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(training_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining report saved to: {report_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
