{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Implement and Test a PyTorch-Based Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and parameters\n",
    "dataset_path = './images_dataSAT/'\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Question: Why is random initialization useful for the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Random initialization is useful for the model for several important reasons:\n",
    "\n",
    "1. **Symmetry Breaking:** If all weights were initialized to the same value (e.g., zero), all neurons in a layer would compute the same output and receive the same gradient updates during backpropagation. This means they would never differentiate from each other. Random initialization breaks this symmetry, allowing each neuron to learn different features.\n",
    "\n",
    "2. **Diverse Feature Learning:** Random initialization ensures that different neurons start with different weight values, enabling them to capture diverse patterns and features from the input data.\n",
    "\n",
    "3. **Efficient Gradient Flow:** Proper random initialization (e.g., Xavier/Glorot or He initialization) helps maintain appropriate gradient magnitudes during training, preventing vanishing or exploding gradient problems.\n",
    "\n",
    "4. **Exploration of Loss Landscape:** Random starting points allow the optimizer to explore different regions of the loss landscape, increasing the chance of finding a good local minimum.\n",
    "\n",
    "5. **Reproducibility with Seeds:** By setting a random seed, experiments can be reproduced while still benefiting from random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create the training transformation pipeline train_transform using transforms.Compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Create training transformation pipeline\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(degrees=45),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Training transformation pipeline created:\")\n",
    "print(train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create the validation transformation pipeline val_transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Create validation transformation pipeline\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Validation transformation pipeline created:\")\n",
    "print(val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset and split into train/val\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path)\n",
    "\n",
    "# Calculate split sizes\n",
    "total_size = len(full_dataset)\n",
    "val_size = int(0.2 * total_size)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Apply transforms\n",
    "train_dataset.dataset.transform = train_transform\n",
    "\n",
    "# Create train_loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {total_size}\")\n",
    "print(f\"Training set size: {train_size}\")\n",
    "print(f\"Validation set size: {val_size}\")\n",
    "print(f\"Classes: {full_dataset.classes}\")\n",
    "print(f\"Class to idx: {full_dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create val_loader for the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Create val_loader\n",
    "# Apply val_transform to the validation subset\n",
    "# We need to create separate datasets with proper transforms\n",
    "train_dataset_proper = datasets.ImageFolder(root=dataset_path, transform=train_transform)\n",
    "val_dataset_proper = datasets.ImageFolder(root=dataset_path, transform=val_transform)\n",
    "\n",
    "# Use same split indices\n",
    "train_indices, val_indices = torch.utils.data.random_split(\n",
    "    range(total_size), [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset_proper, train_indices.indices)\n",
    "val_subset = torch.utils.data.Subset(val_dataset_proper, val_indices.indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"val_loader created successfully!\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Training samples: {len(train_subset)}\")\n",
    "print(f\"Validation samples: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Conv Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNNClassifier(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Question: What is tqdm used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "`tqdm` is a Python library used to display **progress bars** for loops and iterative processes. Its name comes from the Arabic word \"taqaddum\" (تقدّم) meaning \"progress.\"\n",
    "\n",
    "In the context of deep learning model training, `tqdm` is used for:\n",
    "\n",
    "1. **Visual Progress Tracking:** It wraps around iterable objects (like data loaders) and displays a dynamic progress bar showing how many batches have been processed out of the total.\n",
    "\n",
    "2. **Time Estimation:** It provides an estimated time of arrival (ETA) for the completion of the loop, helping users gauge how long training will take.\n",
    "\n",
    "3. **Real-time Metrics Display:** When combined with `set_postfix()` or `set_description()`, it can display real-time training metrics such as loss and accuracy alongside the progress bar.\n",
    "\n",
    "4. **Iteration Speed:** It shows the iterations per second (it/s), which helps monitor training throughput.\n",
    "\n",
    "Example usage: `for batch in tqdm(train_loader, desc='Training'):` displays a progress bar like:\n",
    "```\n",
    "Training: 75%|███████▌  | 30/40 [00:15<00:05, 2.00it/s, loss=0.342, acc=0.891]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Question: Why are the train_loss, train_correct, and train_total set to 0 in every epoch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The variables `train_loss`, `train_correct`, and `train_total` are reset to 0 at the beginning of every epoch for the following reasons:\n",
    "\n",
    "1. **Per-Epoch Metric Calculation:** These variables serve as accumulators to compute the average loss and accuracy **for each individual epoch**. If they were not reset, the metrics would accumulate across epochs and give incorrect (cumulative) values rather than per-epoch performance.\n",
    "\n",
    "2. **Accurate Monitoring:** Resetting ensures that:\n",
    "   - `train_loss` accumulates only the current epoch's total loss, which is then divided by the number of batches to get the average loss for that epoch.\n",
    "   - `train_correct` counts only the correctly classified samples in the current epoch.\n",
    "   - `train_total` counts only the total samples processed in the current epoch.\n",
    "   - The accuracy is calculated as `train_correct / train_total` for each epoch independently.\n",
    "\n",
    "3. **Fair Comparison:** This allows for a fair comparison of model performance across epochs, making it easy to observe whether the model is improving, overfitting, or plateauing.\n",
    "\n",
    "4. **Clean State:** Starting fresh each epoch ensures no residual data from previous epochs contaminates the current epoch's metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with history tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ========== Training Phase ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Train]')\n",
    "    for images, labels in train_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_bar.set_postfix(loss=loss.item(), acc=train_correct/train_total)\n",
    "    \n",
    "    epoch_train_loss = train_loss / len(train_loader)\n",
    "    epoch_train_acc = train_correct / train_total\n",
    "    \n",
    "    # ========== Validation Phase ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Val]')\n",
    "        for images, labels in val_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_bar.set_postfix(loss=loss.item(), acc=val_correct/val_total)\n",
    "    \n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    epoch_val_acc = val_correct / val_total\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['train_acc'].append(epoch_train_acc)\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['val_acc'].append(epoch_val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if epoch_val_acc > best_val_acc:\n",
    "        best_val_acc = epoch_val_acc\n",
    "        torch.save(model.state_dict(), 'best_pytorch_model.pth')\n",
    "        print(f\"  >> Best model saved with val_acc: {best_val_acc:.4f}\")\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} - '\n",
    "          f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} - '\n",
    "          f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
    "    print()\n",
    "\n",
    "print(f\"\\nTraining completed! Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Question: Why do you need to use torch.no_grad() in the validation loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "`torch.no_grad()` is used in the validation loop for several critical reasons:\n",
    "\n",
    "1. **Disable Gradient Computation:** During validation, we only need to evaluate the model's performance — we do NOT want to update the model weights. `torch.no_grad()` disables gradient tracking, which means PyTorch will not build the computational graph needed for backpropagation.\n",
    "\n",
    "2. **Memory Efficiency:** Gradient computation requires storing intermediate activations and building a computational graph, which consumes significant GPU/CPU memory. By disabling gradients, memory usage is substantially reduced, allowing larger batch sizes or preventing out-of-memory errors.\n",
    "\n",
    "3. **Faster Computation:** Without the overhead of tracking gradients and building the computational graph, forward passes execute faster, making the validation phase more efficient.\n",
    "\n",
    "4. **Preventing Accidental Updates:** Using `torch.no_grad()` provides a safety mechanism that ensures no gradient is accidentally computed and no weight update occurs during validation, which would corrupt the model's learned parameters.\n",
    "\n",
    "5. **Correct Evaluation:** It ensures the model is evaluated purely on its current state without any modifications, providing an unbiased assessment of performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Question: What are two different metrics on which the model can be evaluated for best performance during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Two different metrics on which the model can be evaluated for best performance during training are:\n",
    "\n",
    "### 1. Validation Accuracy (`val_accuracy`)\n",
    "- **What it measures:** The proportion of correctly classified samples out of all samples in the validation set.\n",
    "- **Formula:** `Accuracy = Number of Correct Predictions / Total Number of Predictions`\n",
    "- **When to use:** Best suited for **balanced datasets** where classes have roughly equal representation.\n",
    "- **Monitoring:** Save the model checkpoint when `val_accuracy` reaches its **maximum** value (`mode='max'`).\n",
    "\n",
    "### 2. Validation Loss (`val_loss`)\n",
    "- **What it measures:** The average loss (e.g., cross-entropy loss) computed on the validation set, indicating how well the model's predicted probability distribution matches the true labels.\n",
    "- **Why it's useful:** Loss provides a more nuanced and continuous measure of model performance compared to accuracy. A model can have the same accuracy but different loss values — lower loss indicates higher confidence in correct predictions.\n",
    "- **When to use:** Useful for both balanced and **imbalanced datasets**, and when you want to monitor the model's confidence.\n",
    "- **Monitoring:** Save the model checkpoint when `val_loss` reaches its **minimum** value (`mode='min'`).\n",
    "\n",
    "**Other possible metrics** include F1-score, Precision, Recall, and AUC-ROC, but `val_accuracy` and `val_loss` are the two most commonly used during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Plot the Model Loss from the training history of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9: Plot Model Loss\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss vs Validation Loss\n",
    "axes[0].plot(range(1, EPOCHS+1), history['train_loss'], label='Training Loss', color='blue', linewidth=2, marker='o', markersize=4)\n",
    "axes[0].plot(range(1, EPOCHS+1), history['val_loss'], label='Validation Loss', color='red', linewidth=2, marker='s', markersize=4)\n",
    "axes[0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Accuracy vs Validation Accuracy\n",
    "axes[1].plot(range(1, EPOCHS+1), history['train_acc'], label='Training Accuracy', color='blue', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].plot(range(1, EPOCHS+1), history['val_acc'], label='Validation Accuracy', color='red', linewidth=2, marker='s', markersize=4)\n",
    "axes[1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('PyTorch Model Training History', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_acc']):.4f} (Epoch {np.argmax(history['val_acc'])+1})\")\n",
    "print(f\"Lowest Validation Loss: {min(history['val_loss']):.4f} (Epoch {np.argmin(history['val_loss'])+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Code for the images from val_loader, get a list of: All predictions all_preds and The ground truth labels all_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10: Get all predictions and ground truth labels from val_loader\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_pytorch_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc='Getting predictions'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Append to lists\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(f\"Total predictions: {len(all_preds)}\")\n",
    "print(f\"Total ground truth labels: {len(all_labels)}\")\n",
    "print(f\"\\nFirst 20 predictions:  {all_preds[:20]}\")\n",
    "print(f\"First 20 true labels:  {all_labels[:20]}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(all_preds == all_labels)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "class_names = full_dataset.classes\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## All 10 tasks completed successfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
