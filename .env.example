# Project Core Configuration
PROJECT_NAME=ag-news-text-classification
PROJECT_VERSION=1.0.0
PROJECT_STAGE=research  # Options: research, development, staging, production
PROJECT_DESCRIPTION="State-of-the-art AG News text classification framework"

# Environment settings
ENVIRONMENT=development  # Options: development, testing, staging, production
DEBUG=true  # Enable debug mode
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
VERBOSE=false  # Enable verbose output
SEED=42  # Random seed for reproducibility

# Python configuration
PYTHON_VERSION=3.10
PYTHONPATH="${PYTHONPATH}:./src:./scripts"
PYTHONHASHSEED=42
PYTHONDONTWRITEBYTECODE=1
PYTHONUNBUFFERED=1

# Paths and Directories
# Base paths
ROOT_DIR=/path/to/ag-news-text-classification
SRC_DIR=${ROOT_DIR}/src
DATA_DIR=${ROOT_DIR}/data
CONFIGS_DIR=${ROOT_DIR}/configs
OUTPUTS_DIR=${ROOT_DIR}/outputs
SCRIPTS_DIR=${ROOT_DIR}/scripts
NOTEBOOKS_DIR=${ROOT_DIR}/notebooks
CACHE_DIR=${ROOT_DIR}/.cache

# Data paths
RAW_DATA_DIR=${DATA_DIR}/raw
PROCESSED_DATA_DIR=${DATA_DIR}/processed
AUGMENTED_DATA_DIR=${DATA_DIR}/augmented
EXTERNAL_DATA_DIR=${DATA_DIR}/external
PSEUDO_LABELED_DATA_DIR=${DATA_DIR}/pseudo_labeled
SELECTED_SUBSETS_DIR=${DATA_DIR}/selected_subsets

# Model paths
MODELS_DIR=${OUTPUTS_DIR}/models
CHECKPOINTS_DIR=${MODELS_DIR}/checkpoints
PRETRAINED_DIR=${MODELS_DIR}/pretrained
FINE_TUNED_DIR=${MODELS_DIR}/fine_tuned
ENSEMBLE_DIR=${MODELS_DIR}/ensembles
OPTIMIZED_DIR=${MODELS_DIR}/optimized
EXPORTED_DIR=${MODELS_DIR}/exported
PROMPTED_DIR=${MODELS_DIR}/prompted
DISTILLED_DIR=${MODELS_DIR}/distilled

# Output paths
RESULTS_DIR=${OUTPUTS_DIR}/results
ANALYSIS_DIR=${OUTPUTS_DIR}/analysis
LOGS_DIR=${OUTPUTS_DIR}/logs
ARTIFACTS_DIR=${OUTPUTS_DIR}/artifacts
BENCHMARKS_DIR=${OUTPUTS_DIR}/benchmarks

# Hardware and Compute Configuration
# GPU settings
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  # Available GPU devices
CUDA_VERSION=11.8
CUDA_LAUNCH_BLOCKING=0  # Set to 1 for debugging
CUBLAS_WORKSPACE_CONFIG=:4096:8  # For deterministic operations
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
USE_GPU=true
GPU_MEMORY_FRACTION=0.9  # Fraction of GPU memory to use
MIXED_PRECISION=true  # Enable automatic mixed precision
GRADIENT_CHECKPOINTING=false  # Enable for large models

# CPU settings
NUM_WORKERS=8  # Number of data loading workers
OMP_NUM_THREADS=8
MKL_NUM_THREADS=8
OPENBLAS_NUM_THREADS=8

# Memory management
MAX_MEMORY_GB=32
MEMORY_EFFICIENT_MODE=false
CLEAR_CACHE_FREQUENCY=100  # Clear cache every N batches

# Distributed training
DISTRIBUTED_BACKEND=nccl  # Options: nccl, gloo, mpi
MASTER_ADDR=localhost
MASTER_PORT=29500
WORLD_SIZE=1
RANK=0
LOCAL_RANK=0
NCCL_DEBUG=INFO
TORCH_DISTRIBUTED_DEBUG=OFF

# Data Configuration
# Dataset settings
DATASET_NAME=ag_news
DATASET_VERSION=latest
DATA_FORMAT=csv  # Options: csv, json, parquet, arrow
TEXT_COLUMN=text
LABEL_COLUMN=label
NUM_CLASSES=4

# Data splits
TRAIN_SPLIT_RATIO=0.8
VAL_SPLIT_RATIO=0.1
TEST_SPLIT_RATIO=0.1
STRATIFIED_SPLIT=true
K_FOLD_SPLITS=5

# Data processing
MAX_LENGTH=512
TRUNCATION_STRATEGY=longest_first  # Options: longest_first, only_first, only_second
PADDING_STRATEGY=max_length  # Options: max_length, longest, do_not_pad
NORMALIZE_TEXT=true
LOWERCASE=false
REMOVE_SPECIAL_CHARS=false
REMOVE_STOPWORDS=false
STEMMING=false
LEMMATIZATION=false

# Data augmentation
ENABLE_AUGMENTATION=true
AUGMENTATION_FACTOR=2  # Number of augmented samples per original
BACKTRANSLATION_LANGS=de,fr,es,zh  # Languages for back-translation
PARAPHRASE_MODEL=tuner007/pegasus_paraphrase
SYNONYM_REPLACEMENT_PROB=0.1
RANDOM_INSERTION_PROB=0.1
RANDOM_SWAP_PROB=0.1
RANDOM_DELETION_PROB=0.1
MIXUP_ALPHA=0.2
CUTMIX_ALPHA=1.0

# Contrast sets
ENABLE_CONTRAST_SETS=true
CONTRAST_SET_SIZE=1000
CONTRAST_GENERATION_METHOD=manual  # Options: manual, automatic, gpt4

# External data
USE_EXTERNAL_DATA=true
NEWS_CORPUS_SOURCES=cnn_dailymail,reuters,bbc_news,reddit_news
WIKIPEDIA_DUMP_DATE=20240101
PRETRAIN_CORPUS_SIZE=1000000

# Model Configuration
# Model selection
MODEL_TYPE=transformer  # Options: transformer, ensemble, prompt_based, efficient
MODEL_NAME=microsoft/deberta-v3-xlarge
MODEL_VARIANT=base  # Options: base, large, xlarge
PRETRAINED_MODEL_PATH=${PRETRAINED_DIR}/deberta-v3-xlarge
USE_PRETRAINED=true
FREEZE_EMBEDDINGS=false
FREEZE_ENCODER_LAYERS=0  # Number of encoder layers to freeze

# Model architecture
HIDDEN_SIZE=1024
NUM_HIDDEN_LAYERS=24
NUM_ATTENTION_HEADS=16
INTERMEDIATE_SIZE=4096
HIDDEN_DROPOUT_PROB=0.1
ATTENTION_PROBS_DROPOUT_PROB=0.1
CLASSIFIER_DROPOUT_PROB=0.2
LAYER_NORM_EPS=1e-7
POSITION_EMBEDDING_TYPE=relative_key_query
MAX_POSITION_EMBEDDINGS=512

# Pooling strategy
POOLING_STRATEGY=mean  # Options: cls, mean, max, attention
USE_HIDDEN_STATES=false  # Use hidden states from multiple layers
HIDDEN_STATES_LAYERS=-4,-3,-2,-1  # Layers to use (negative indexing)

# Training Configuration
# Training parameters
NUM_EPOCHS=10
BATCH_SIZE=32
GRADIENT_ACCUMULATION_STEPS=1
EFFECTIVE_BATCH_SIZE=32  # batch_size * gradient_accumulation_steps * world_size
MAX_STEPS=-1  # Override num_epochs if set
WARMUP_STEPS=500
WARMUP_RATIO=0.1
LOGGING_STEPS=50
EVAL_STEPS=500
SAVE_STEPS=1000
SAVE_TOTAL_LIMIT=3
SAVE_BEST_MODEL=true
METRIC_FOR_BEST_MODEL=f1_macro
GREATER_IS_BETTER=true

# Optimization
OPTIMIZER=adamw  # Options: adamw, adam, sgd, lamb, adafactor
LEARNING_RATE=2e-5
WEIGHT_DECAY=0.01
ADAM_BETA1=0.9
ADAM_BETA2=0.999
ADAM_EPSILON=1e-8
MAX_GRAD_NORM=1.0
GRADIENT_CLIPPING_VALUE=1.0
USE_SAM_OPTIMIZER=false  # Sharpness Aware Minimization
SAM_RHO=0.05

# Learning rate schedule
LR_SCHEDULER_TYPE=cosine  # Options: linear, cosine, polynomial, constant_with_warmup
LR_END=1e-7
POWER=1.0  # For polynomial decay
CYCLE_LENGTH=1000  # For cyclic schedulers

# Advanced training strategies
USE_CURRICULUM_LEARNING=false
CURRICULUM_STRATEGY=competence  # Options: competence, self_paced, data_driven
USE_ADVERSARIAL_TRAINING=false
ADVERSARIAL_METHOD=fgm  # Options: fgm, pgd, freelb
ADVERSARIAL_EPSILON=1.0
USE_MIXOUT=false
MIXOUT_PROB=0.1
USE_R_DROP=false
R_DROP_ALPHA=0.5

# Multi-task learning
USE_MULTITASK_LEARNING=false
AUXILIARY_TASKS=sentiment,topic  # Comma-separated auxiliary tasks
TASK_WEIGHTS=1.0,0.5,0.5  # Main task weight, auxiliary weights

# Ensemble Configuration
# Ensemble settings
USE_ENSEMBLE=true
ENSEMBLE_METHOD=voting  # Options: voting, stacking, blending, bayesian
ENSEMBLE_MODELS=deberta,roberta,xlnet,electra,longformer
NUM_ENSEMBLE_MODELS=5
ENSEMBLE_WEIGHTS=0.3,0.25,0.2,0.15,0.1  # Model weights for weighted voting

# Voting ensemble
VOTING_TYPE=soft  # Options: hard, soft
USE_WEIGHTED_VOTING=true
OPTIMIZE_WEIGHTS=true

# Stacking ensemble
META_LEARNER=xgboost  # Options: logistic_regression, xgboost, catboost, neural_network
USE_CROSS_VALIDATION_STACKING=true
STACKING_CV_FOLDS=5

# Blending ensemble
BLENDING_SPLIT_RATIO=0.2
USE_DYNAMIC_BLENDING=true

# Prompt Engineering and Instruction Tuning
# Prompt settings
USE_PROMPT_TUNING=false
PROMPT_TEMPLATE="Classify the following news article into one of four categories: World, Sports, Business, or Sci/Tech.\n\nArticle: {text}\n\nCategory:"
NUM_PROMPT_TOKENS=20
PROMPT_INITIALIZATION=random  # Options: random, vocab, text
SOFT_PROMPT_TUNING=false

# Instruction tuning
USE_INSTRUCTION_TUNING=false
INSTRUCTION_TEMPLATE="You are a news classifier. Classify the given text into the appropriate category."
FEW_SHOT_EXAMPLES=5
USE_CHAIN_OF_THOUGHT=false
COT_TEMPLATE="Let's think step by step..."

# Prefix tuning
USE_PREFIX_TUNING=false
PREFIX_LENGTH=10
PREFIX_PROJECTION_DIM=512

# Efficient Training and Optimization
# LoRA/PEFT
USE_LORA=false
LORA_R=8  # Rank
LORA_ALPHA=16  # Scaling parameter
LORA_DROPOUT=0.1
LORA_TARGET_MODULES=q_proj,v_proj,k_proj,o_proj
USE_QLORA=false  # Quantized LoRA
QLORA_BITS=4

# Adapter tuning
USE_ADAPTERS=false
ADAPTER_SIZE=64
ADAPTER_DROPOUT=0.1
USE_ADAPTER_FUSION=false

# Quantization
USE_QUANTIZATION=false
QUANTIZATION_BITS=8  # Options: 4, 8
QUANTIZATION_METHOD=dynamic  # Options: dynamic, static, qat
USE_INT8_TRAINING=false

# Pruning
USE_PRUNING=false
PRUNING_METHOD=magnitude  # Options: magnitude, structured, lottery_ticket
PRUNING_SPARSITY=0.5

# Model compression
USE_KNOWLEDGE_DISTILLATION=false
TEACHER_MODEL=gpt-4
DISTILLATION_TEMPERATURE=3.0
DISTILLATION_ALPHA=0.7

# Domain Adaptation and Transfer Learning
# Domain adaptive pretraining (DAPT)
USE_DAPT=true
DAPT_CORPUS=news_corpus
DAPT_STEPS=100000
DAPT_MLM_PROBABILITY=0.15
DAPT_LEARNING_RATE=5e-5

# Task adaptive pretraining (TAPT)
USE_TAPT=false
TAPT_STEPS=10000

# Pseudo labeling
USE_PSEUDO_LABELING=false
PSEUDO_LABEL_THRESHOLD=0.95
PSEUDO_LABEL_ITERATIONS=3

# Self-training
USE_SELF_TRAINING=false
SELF_TRAINING_ITERATIONS=5
CONFIDENCE_THRESHOLD=0.9

# Evaluation and Metrics
# Evaluation settings
EVAL_STRATEGY=steps  # Options: no, steps, epoch
EVAL_BATCH_SIZE=64
EVAL_ACCUMULATION_STEPS=1
USE_CACHED_DATASETS=true

# Metrics
METRICS=accuracy,precision,recall,f1,f1_macro,f1_micro,f1_weighted,matthews_corrcoef,cohen_kappa
PRIMARY_METRIC=f1_macro
COMPUTE_CONFUSION_MATRIX=true
COMPUTE_CLASS_WISE_METRICS=true
COMPUTE_CONFIDENCE_INTERVALS=true
CI_CONFIDENCE_LEVEL=0.95
BOOTSTRAP_ITERATIONS=1000

# Robustness evaluation
EVALUATE_ROBUSTNESS=true
ADVERSARIAL_ATTACK_METHODS=textfooler,bert-attack,deepwordbug
EVALUATE_OOD_DETECTION=true
EVALUATE_CALIBRATION=true

# Interpretability
COMPUTE_ATTENTION_WEIGHTS=true
COMPUTE_SALIENCY_MAPS=true
USE_SHAP=true
USE_LIME=true
USE_INTEGRATED_GRADIENTS=true

# Experiment Tracking and Logging
# Weights & Biases
USE_WANDB=true
WANDB_API_KEY=your_wandb_api_key_here
WANDB_PROJECT=ag-news-classification
WANDB_ENTITY=your_wandb_entity
WANDB_RUN_NAME=${MODEL_NAME}_${CURRENT_DATE}
WANDB_TAGS=transformer,ensemble,sota
WANDB_NOTES="Experiment with advanced training strategies"
WANDB_MODE=online  # Options: online, offline, disabled

# MLflow
USE_MLFLOW=false
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=ag-news-experiments
MLFLOW_RUN_NAME=${MODEL_NAME}_${CURRENT_DATE}

# TensorBoard
USE_TENSORBOARD=true
TENSORBOARD_LOG_DIR=${LOGS_DIR}/tensorboard

# Neptune
USE_NEPTUNE=false
NEPTUNE_API_TOKEN=your_neptune_api_token
NEPTUNE_PROJECT=username/ag-news

# Comet ML
USE_COMET=false
COMET_API_KEY=your_comet_api_key
COMET_PROJECT_NAME=ag-news
COMET_WORKSPACE=your_workspace

# API and External Services
# OpenAI API (for GPT-4 distillation)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=2048
OPENAI_TIMEOUT=60
OPENAI_MAX_RETRIES=3
OPENAI_RATE_LIMIT=60  # Requests per minute

# Hugging Face
HF_TOKEN=your_huggingface_token_here
HF_CACHE_DIR=${CACHE_DIR}/huggingface
HF_DATASETS_CACHE=${HF_CACHE_DIR}/datasets
HF_MODULES_CACHE=${HF_CACHE_DIR}/modules
HF_METRICS_CACHE=${HF_CACHE_DIR}/metrics
HF_HUB_OFFLINE=false

# Google Cloud (for translation)
GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
GCP_PROJECT_ID=your-gcp-project-id
GCS_BUCKET=ag-news-data

# AWS
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_DEFAULT_REGION=us-west-2
S3_BUCKET=ag-news-models
SAGEMAKER_ROLE=arn:aws:iam::account-id:role/service-role/SageMakerRole

# Azure
AZURE_SUBSCRIPTION_ID=your_azure_subscription_id
AZURE_RESOURCE_GROUP=ag-news-rg
AZURE_STORAGE_ACCOUNT=agnewsstorage
AZURE_STORAGE_KEY=your_storage_key
AZURE_ML_WORKSPACE=ag-news-workspace

# Database Configuration
# PostgreSQL
DB_TYPE=postgresql
DB_HOST=localhost
DB_PORT=5432
DB_NAME=ag_news_db
DB_USER=ag_news_user
DB_PASSWORD=your_secure_password_here
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_ECHO=false

# Redis (for caching)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password
REDIS_DB=0
REDIS_DECODE_RESPONSES=true
CACHE_TTL=3600  # Cache time-to-live in seconds

# MongoDB (for document storage)
MONGO_URI=mongodb://localhost:27017/
MONGO_DB_NAME=ag_news
MONGO_COLLECTION=experiments

# Elasticsearch (for search)
ELASTICSEARCH_HOST=localhost
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_INDEX=ag_news_index

# API Server Configuration
# FastAPI settings
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_RELOAD=false
API_DEBUG=false
API_CORS_ORIGINS=["http://localhost:3000", "http://localhost:8080"]
API_RATE_LIMIT=100  # Requests per minute
API_TIMEOUT=30  # Request timeout in seconds

# Authentication
API_SECRET_KEY=your_very_secure_secret_key_here_minimum_32_characters
API_ALGORITHM=HS256
API_ACCESS_TOKEN_EXPIRE_MINUTES=30
API_REFRESH_TOKEN_EXPIRE_DAYS=7
ENABLE_API_AUTH=true

# gRPC settings
GRPC_HOST=0.0.0.0
GRPC_PORT=50051
GRPC_MAX_WORKERS=10

# GraphQL settings
GRAPHQL_ENABLED=false
GRAPHQL_PATH=/graphql
GRAPHQL_PLAYGROUND=true

# Deployment and Production
# Docker settings
DOCKER_REGISTRY=agnews-research
DOCKER_IMAGE_NAME=ag-news-classifier
DOCKER_TAG=latest
DOCKER_BUILD_ARGS="--build-arg PYTHON_VERSION=3.10"

# Kubernetes
K8S_NAMESPACE=ag-news
K8S_DEPLOYMENT_NAME=ag-news-deployment
K8S_SERVICE_NAME=ag-news-service
K8S_REPLICAS=3
K8S_CPU_REQUEST=2
K8S_CPU_LIMIT=4
K8S_MEMORY_REQUEST=4Gi
K8S_MEMORY_LIMIT=8Gi

# Model serving
MODEL_SERVER=torchserve  # Options: torchserve, triton, bentoml, seldon
MODEL_SERVER_PORT=8080
MODEL_SERVER_WORKERS=4
ENABLE_BATCHING=true
MAX_BATCH_SIZE=32
BATCH_TIMEOUT_MS=100

# Load balancing
LOAD_BALANCER=nginx  # Options: nginx, haproxy, traefik
LB_STRATEGY=round_robin  # Options: round_robin, least_conn, ip_hash

# Monitoring and Alerting
# Prometheus
PROMETHEUS_PORT=9090
ENABLE_METRICS=true
METRICS_PATH=/metrics

# Grafana
GRAFANA_HOST=localhost
GRAFANA_PORT=3000
GRAFANA_USER=admin
GRAFANA_PASSWORD=admin

# Alerting
ENABLE_ALERTS=true
ALERT_EMAIL=alerts@agnews-research.ai
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
PAGERDUTY_API_KEY=your_pagerduty_key

# Logging
LOG_FORMAT=json  # Options: json, text
LOG_FILE_MAX_SIZE=100MB
LOG_FILE_BACKUP_COUNT=10
LOG_TO_FILE=true
LOG_TO_CONSOLE=true
SENTRY_DSN=https://your_sentry_dsn@sentry.io/project_id

# Security Configuration
# Security settings
ENABLE_HTTPS=true
SSL_CERT_FILE=/path/to/cert.pem
SSL_KEY_FILE=/path/to/key.pem
ENABLE_RATE_LIMITING=true
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_PERIOD=60  # seconds

# Data privacy
ENABLE_PII_DETECTION=true
PII_MASKING=true
DATA_ENCRYPTION=true
ENCRYPTION_KEY=your_encryption_key_here

# Model security
ENABLE_MODEL_ENCRYPTION=true
ENABLE_ADVERSARIAL_DEFENSE=true
MODEL_WATERMARKING=false

# Testing and Quality Assurance
# Testing configuration
RUN_TESTS_BEFORE_COMMIT=true
TEST_COVERAGE_THRESHOLD=80
PYTEST_ARGS=-v --cov=src --cov-report=html
ENABLE_INTEGRATION_TESTS=true
ENABLE_PERFORMANCE_TESTS=true
ENABLE_SECURITY_TESTS=true

# Code quality
ENABLE_LINTING=true
LINTING_TOOLS=black,isort,flake8,mypy,pylint
MAX_LINE_LENGTH=100
MAX_COMPLEXITY=10

# Research and Experimentation
# Research mode
RESEARCH_MODE=true
EXPERIMENT_NAME=sota_ensemble_v1
EXPERIMENT_TAGS=week6,ensemble,distillation
HYPOTHESIS="Ensemble of fine-tuned transformers with GPT-4 distillation will achieve SOTA"

# Week-based research schedule
CURRENT_WEEK=6  # Current week in 10-week schedule
WEEK1_COMPLETED=true  # Classical baselines
WEEK2_3_COMPLETED=true  # Deep learning & transformers
WEEK4_5_COMPLETED=true  # Advanced training
WEEK6_7_IN_PROGRESS=true  # SOTA & LLMs
WEEK8_9_PLANNED=false  # Optimization
WEEK10_PLANNED=false  # Deployment

# Ablation studies
RUN_ABLATION_STUDIES=true
ABLATION_COMPONENTS=model_size,data_augmentation,ensemble_method
ABLATION_METRICS=accuracy,f1_macro,inference_time

# Hyperparameter search
ENABLE_HYPEROPT=true
HYPEROPT_METHOD=optuna  # Options: optuna, ray_tune, hyperband
HYPEROPT_TRIALS=100
HYPEROPT_TIMEOUT=86400  # 24 hours

# Reproducibility
ENSURE_REPRODUCIBILITY=true
DETERMINISTIC_ALGORITHMS=true
BENCHMARK_MODE=false  # Disable for reproducibility

# Performance Optimization
# Caching
ENABLE_CACHING=true
CACHE_STRATEGY=lru  # Options: lru, lfu, fifo
CACHE_SIZE=1000
CACHE_EMBEDDINGS=true
CACHE_PREDICTIONS=true

# Optimization flags
COMPILE_MODEL=false  # torch.compile() for PyTorch 2.0+
USE_FLASH_ATTENTION=false
USE_XFORMERS=false
USE_DEEPSPEED=false
DEEPSPEED_CONFIG=configs/deepspeed/zero2.json

# Profiling
ENABLE_PROFILING=false
PROFILE_MEMORY=true
PROFILE_TIME=true
PROFILE_OUTPUT_DIR=${OUTPUTS_DIR}/profiles

# UI and Visualization
# Streamlit app
STREAMLIT_PORT=8501
STREAMLIT_SERVER_ADDRESS=0.0.0.0
STREAMLIT_THEME=dark
STREAMLIT_WIDE_MODE=true

# Gradio interface
GRADIO_PORT=7860
GRADIO_SHARE=false
GRADIO_AUTH_USERNAME=admin
GRADIO_AUTH_PASSWORD=admin

# Jupyter settings
JUPYTER_PORT=8888
JUPYTER_TOKEN=your_jupyter_token
JUPYTER_ENABLE_LAB=true

# Custom and Advanced Settings
# Custom model registry
MODEL_REGISTRY_URL=https://models.agnews-research.ai
MODEL_REGISTRY_API_KEY=your_registry_key

# Feature flags
ENABLE_FEATURE_FLAGS=true
FEATURE_NEW_AUGMENTATION=true
FEATURE_EXPERIMENTAL_MODELS=false
FEATURE_BETA_API=false

# A/B testing
ENABLE_AB_TESTING=false
AB_TEST_ID=ensemble_vs_single
AB_TEST_SPLIT=0.5

# Time zone and locale
TIMEZONE=UTC
LOCALE=en_US.UTF-8

# Notification settings
ENABLE_NOTIFICATIONS=true
NOTIFICATION_CHANNELS=email,slack
NOTIFY_ON_TRAINING_COMPLETE=true
NOTIFY_ON_ERROR=true
NOTIFY_ON_PERFORMANCE_DROP=true

# Backup and recovery
ENABLE_AUTO_BACKUP=true
BACKUP_FREQUENCY=daily  # Options: hourly, daily, weekly
BACKUP_RETENTION_DAYS=30
BACKUP_LOCATION=s3://ag-news-backups

# Cost tracking
TRACK_COMPUTE_COSTS=true
MAX_BUDGET_USD=1000
ALERT_ON_BUDGET_EXCEED=true

# Environmental impact
TRACK_CARBON_FOOTPRINT=true
CARBON_TRACKER_PROJECT=ag-news
OPTIMIZE_FOR_GREEN_COMPUTING=false

# Development Tools
# VSCode remote development
VSCODE_REMOTE_PORT=8443
ENABLE_CODE_SERVER=false

# Debug settings
BREAKPOINT_ON_ERROR=false
ENABLE_PDB=false
IPDB_CONTEXT_SIZE=10

# Git hooks
ENABLE_PRE_COMMIT=true
ENABLE_COMMIT_MSG_CHECK=true
ENFORCE_CONVENTIONAL_COMMITS=true

# CI/CD
CI_PIPELINE=github_actions  # Options: github_actions, gitlab_ci, jenkins
CD_STRATEGY=blue_green  # Options: rolling, blue_green, canary
AUTO_DEPLOY_ON_TAG=false

# Documentation
AUTO_GENERATE_DOCS=true
DOCS_FORMAT=sphinx  # Options: sphinx, mkdocs
DOCS_THEME=readthedocs

# Note: Remember to keep this file updated with any new environment variables
# Last updated: 2024-01-20
# Maintainer: AG News Research Team
