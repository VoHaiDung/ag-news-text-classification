{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Comparative Analysis of Keras and PyTorch Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and parameters\n",
    "dataset_path = './images_dataSAT/'\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the print_metrics function\n",
    "def print_metrics(y_true, y_pred, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Print performance metrics for a given model.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted')\n",
    "    rec = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Performance Metrics for {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Non-Agricultural', 'Agricultural']))\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== KERAS MODEL SETUP ==========\n",
    "# Create validation data generator for Keras\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "keras_val_generator = val_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Keras validation samples: {keras_val_generator.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Keras model\n",
    "try:\n",
    "    keras_model = load_model('best_model.keras')\n",
    "    print(\"Keras model loaded successfully!\")\n",
    "except:\n",
    "    print(\"Keras model file not found. Using a new model for demonstration.\")\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "    \n",
    "    keras_model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)),\n",
    "        BatchNormalization(), MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(), MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(), MaxPooling2D((2, 2)),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(), MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'), Dropout(0.5),\n",
    "        Dense(256, activation='relu'), Dropout(0.4),\n",
    "        Dense(128, activation='relu'), Dropout(0.3),\n",
    "        Dense(64, activation='relu'), Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Keras predictions\n",
    "keras_val_generator.reset()\n",
    "preds = keras_model.predict(keras_val_generator, verbose=1)\n",
    "preds = (preds > 0.5).astype(int).flatten()\n",
    "keras_true_labels = keras_val_generator.classes\n",
    "\n",
    "print(f\"Keras predictions shape: {preds.shape}\")\n",
    "print(f\"Keras true labels shape: {keras_true_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Question: What does the code preds > 0.5 in line preds = (preds > 0.5).astype(int).flatten() do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The code `preds > 0.5` in the line `preds = (preds > 0.5).astype(int).flatten()` performs **thresholding** to convert continuous probability outputs into discrete binary class predictions. Here's the step-by-step breakdown:\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. **`preds > 0.5` (Boolean Thresholding):**\n",
    "   - The Keras model's output layer uses a **sigmoid activation function**, which outputs a probability value between 0 and 1.\n",
    "   - This comparison creates a **boolean array** where each element is `True` if the predicted probability is greater than 0.5, and `False` otherwise.\n",
    "   - Values > 0.5 are interpreted as **class 1** (Agricultural), and values ≤ 0.5 as **class 0** (Non-Agricultural).\n",
    "   - Example: `[0.8, 0.3, 0.6, 0.1]` → `[True, False, True, False]`\n",
    "\n",
    "2. **`.astype(int)` (Type Conversion):**\n",
    "   - Converts the boolean array to integers: `True` → `1`, `False` → `0`.\n",
    "   - Example: `[True, False, True, False]` → `[1, 0, 1, 0]`\n",
    "\n",
    "3. **`.flatten()` (Reshape):**\n",
    "   - Flattens the array to a 1D array, ensuring the predictions are in a simple flat format for comparison with true labels.\n",
    "   - Example: `[[1], [0], [1], [0]]` → `[1, 0, 1, 0]`\n",
    "\n",
    "### Why 0.5?\n",
    "The threshold of 0.5 is the standard **decision boundary** for binary classification: if the model is more than 50% confident that the sample belongs to class 1, it is classified as class 1; otherwise, class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Print the performance metrics for the Keras model using the print_metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Print Keras model performance metrics\n",
    "keras_metrics = print_metrics(keras_true_labels, preds, model_name='Keras CNN Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Question: What is the significance of the F1 score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The **F1 score** is a highly significant metric in machine learning classification tasks for the following reasons:\n",
    "\n",
    "### Definition\n",
    "The F1 score is the **harmonic mean** of Precision and Recall:\n",
    "\n",
    "$$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$$\n",
    "\n",
    "### Significance:\n",
    "\n",
    "1. **Balances Precision and Recall:** The F1 score provides a single metric that balances both precision (how many of the predicted positives are actually positive) and recall (how many of the actual positives were correctly identified). This is crucial when both false positives and false negatives carry significant costs.\n",
    "\n",
    "2. **Handles Class Imbalance:** Unlike accuracy, which can be misleading with imbalanced datasets (e.g., 95% accuracy by always predicting the majority class), the F1 score accounts for both types of errors. A model with high accuracy but low recall will have a low F1 score, revealing its poor performance on the minority class.\n",
    "\n",
    "3. **Harmonic Mean Property:** The harmonic mean penalizes extreme values. If either precision or recall is very low, the F1 score will also be low, even if the other metric is high. This ensures the model performs well on both metrics.\n",
    "\n",
    "4. **Practical Decision Making:** In applications like medical diagnosis, fraud detection, or **agricultural land classification**, both false positives and false negatives have consequences. The F1 score helps select models that minimize both types of errors.\n",
    "\n",
    "5. **Model Comparison:** F1 score provides a reliable single number for comparing different models, especially when precision-recall trade-offs differ between them.\n",
    "\n",
    "### Range\n",
    "- **F1 = 1.0:** Perfect precision and recall\n",
    "- **F1 = 0.0:** Worst possible performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PYTORCH MODEL SETUP ==========\n",
    "# Define PyTorch CNN model (same architecture as Question 5)\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create validation dataset and loader for PyTorch\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=val_transform)\n",
    "total_size = len(full_dataset)\n",
    "val_size = int(0.2 * total_size)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "_, val_subset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "pytorch_val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load PyTorch model\n",
    "pytorch_model = CNNClassifier(num_classes=2).to(device)\n",
    "try:\n",
    "    pytorch_model.load_state_dict(torch.load('best_pytorch_model.pth', map_location=device))\n",
    "    print(\"PyTorch model loaded successfully!\")\n",
    "except:\n",
    "    print(\"PyTorch model file not found. Using randomly initialized model for demonstration.\")\n",
    "\n",
    "pytorch_model.eval()\n",
    "\n",
    "# Get PyTorch predictions\n",
    "pytorch_all_preds = []\n",
    "pytorch_all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in pytorch_val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = pytorch_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pytorch_all_preds.extend(predicted.cpu().numpy())\n",
    "        pytorch_all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "pytorch_all_preds = np.array(pytorch_all_preds)\n",
    "pytorch_all_labels = np.array(pytorch_all_labels)\n",
    "\n",
    "print(f\"PyTorch predictions: {len(pytorch_all_preds)}\")\n",
    "print(f\"PyTorch true labels: {len(pytorch_all_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Print the performance metrics for the PyTorch model using print_metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Print PyTorch model performance metrics\n",
    "pytorch_metrics = print_metrics(pytorch_all_labels, pytorch_all_preds, model_name='PyTorch CNN Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Question: What is the total number of false negatives in the confusion matrix for the PyTorch model evaluated above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Calculate and display false negatives from PyTorch confusion matrix\n",
    "cm_pytorch = pytorch_metrics['confusion_matrix']\n",
    "\n",
    "print(\"PyTorch Model Confusion Matrix:\")\n",
    "print(cm_pytorch)\n",
    "print()\n",
    "print(\"Confusion Matrix Layout:\")\n",
    "print(\"                  Predicted\")\n",
    "print(\"                  Neg(0)  Pos(1)\")\n",
    "print(f\"Actual Neg(0):   TN={cm_pytorch[0][0]:5d}  FP={cm_pytorch[0][1]:5d}\")\n",
    "print(f\"Actual Pos(1):   FN={cm_pytorch[1][0]:5d}  TP={cm_pytorch[1][1]:5d}\")\n",
    "print()\n",
    "\n",
    "# False Negatives: Actual Positive predicted as Negative\n",
    "false_negatives = cm_pytorch[1][0]\n",
    "print(f\"Total number of False Negatives (FN): {false_negatives}\")\n",
    "print()\n",
    "print(\"A False Negative means the image was actually Agricultural land (class 1)\")\n",
    "print(\"but the model incorrectly predicted it as Non-Agricultural (class 0).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== COMPARATIVE ANALYSIS ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARATIVE ANALYSIS: Keras vs PyTorch\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Keras': [keras_metrics['accuracy'], keras_metrics['precision'], \n",
    "              keras_metrics['recall'], keras_metrics['f1']],\n",
    "    'PyTorch': [pytorch_metrics['accuracy'], pytorch_metrics['precision'],\n",
    "                pytorch_metrics['recall'], pytorch_metrics['f1']]\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Metric':<15} {'Keras':>10} {'PyTorch':>10} {'Difference':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for i, metric in enumerate(comparison_data['Metric']):\n",
    "    k_val = comparison_data['Keras'][i]\n",
    "    p_val = comparison_data['PyTorch'][i]\n",
    "    diff = k_val - p_val\n",
    "    print(f\"{metric:<15} {k_val:>10.4f} {p_val:>10.4f} {diff:>+12.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison_data['Metric']))\n",
    "width = 0.35\n",
    "bars1 = ax.bar(x - width/2, comparison_data['Keras'], width, label='Keras', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, comparison_data['PyTorch'], width, label='PyTorch', color='coral')\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Keras vs PyTorch Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_data['Metric'], fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## All 5 tasks completed successfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
