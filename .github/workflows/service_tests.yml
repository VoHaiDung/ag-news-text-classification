# Service Testing Workflow for AG News Classification
# ====================================================
# Comprehensive testing pipeline for microservices architecture following:
# - Microservices Testing Best Practices
# - Service Architecture Testing Guidelines
# - Integration Testing Standards
# - Contract Testing Patterns
# - Performance Testing Methodologies
# - Academic Software Engineering Standards
#
# Author: Võ Hải Dũng
# License: MIT

name: Service Tests

on:
  push:
    branches: [main, develop]
    paths:
      - 'src/services/**'
      - 'tests/services/**'
      - 'configs/services/**'
      - '.github/workflows/service_tests.yml'
  pull_request:
    branches: [main, develop]
    paths:
      - 'src/services/**'
      - 'tests/services/**'
      - 'configs/services/**'
  workflow_dispatch:
    inputs:
      service_name:
        description: 'Service to test (all for all services)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - prediction
          - training
          - data
          - model_management
          - monitoring
      test_level:
        description: 'Test level'
        required: false
        default: 'integration'
        type: choice
        options:
          - unit
          - integration
          - system
          - performance
          - all

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'
  TEST_TIMEOUT: 600
  COVERAGE_THRESHOLD: 70

jobs:
  # Service dependency setup and validation
  setup-dependencies:
    name: Setup Service Dependencies
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
      services-matrix: ${{ steps.services.outputs.matrix }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate cache key
        id: cache-key
        run: |
          FILES_HASH=$(find . -name "requirements*.txt" -o -name "pyproject.toml" -o -name "setup.py" 2>/dev/null | xargs sha256sum 2>/dev/null | sha256sum | cut -d' ' -f1)
          echo "key=services-deps-${FILES_HASH}-${{ runner.os }}-${{ env.PYTHON_VERSION }}" >> $GITHUB_OUTPUT

      - name: Determine services to test
        id: services
        run: |
          if [ "${{ github.event.inputs.service_name }}" == "all" ] || [ -z "${{ github.event.inputs.service_name }}" ]; then
            echo 'matrix=["prediction", "training", "data", "model_management", "monitoring"]' >> $GITHUB_OUTPUT
          else
            echo 'matrix=["${{ github.event.inputs.service_name }}"]' >> $GITHUB_OUTPUT
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Cache service dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.local/lib/python${{ env.PYTHON_VERSION }}
            ~/venv
          key: ${{ steps.cache-key.outputs.key }}
          restore-keys: |
            services-deps-
            ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-

      - name: Install base dependencies
        run: |
          python -m venv ~/venv
          source ~/venv/bin/activate
          python -m pip install --upgrade pip setuptools wheel
          
          for req_file in requirements/base.txt requirements/services.txt requirements/test.txt; do
            if [ -f "$req_file" ]; then
              pip install -r "$req_file"
            fi
          done
          
          pip install pytest pytest-cov pytest-mock pytest-asyncio pytest-timeout
          pip install redis celery sqlalchemy psycopg2-binary
          pip install httpx fastapi uvicorn pydantic

  # Unit tests for individual services
  unit-tests:
    name: Unit Tests - ${{ matrix.service }}
    runs-on: ubuntu-latest
    needs: setup-dependencies
    if: ${{ github.event.inputs.test_level != 'integration' && github.event.inputs.test_level != 'performance' }}
    strategy:
      fail-fast: false
      matrix:
        service: [prediction, training, data, model_management, monitoring]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Restore cached dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.local/lib/python${{ env.PYTHON_VERSION }}
            ~/venv
          key: ${{ needs.setup-dependencies.outputs.cache-key }}

      - name: Run unit tests for ${{ matrix.service }} service
        run: |
          source ~/venv/bin/activate 2>/dev/null || true
          mkdir -p tests/services/${{ matrix.service }}
          
          python -m pytest tests/services/${{ matrix.service }}/ \
            -v \
            --cov=src/services/${{ matrix.service }} \
            --cov-report=term-missing \
            --cov-report=xml:coverage-${{ matrix.service }}.xml \
            --cov-report=html:htmlcov-${{ matrix.service }} \
            --timeout=${{ env.TEST_TIMEOUT }} \
            -m "not integration and not performance" \
            || echo "Unit tests for ${{ matrix.service }} completed with status $?"

      - name: Check coverage threshold
        if: always()
        run: |
          if [ -f coverage-${{ matrix.service }}.xml ]; then
            coverage_percent=$(python -c "
          import xml.etree.ElementTree as ET
          tree = ET.parse('coverage-${{ matrix.service }}.xml')
          root = tree.getroot()
          coverage = float(root.attrib.get('line-rate', 0)) * 100
          print(f'{coverage:.2f}')
            " 2>/dev/null || echo "0")
            echo "Coverage for ${{ matrix.service }}: ${coverage_percent}%"
          fi

      - name: Upload coverage artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-unit-${{ matrix.service }}
          path: |
            coverage-${{ matrix.service }}.xml
            htmlcov-${{ matrix.service }}/
          retention-days: 7

  # Integration tests with external dependencies
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: setup-dependencies
    if: ${{ github.event.inputs.test_level != 'unit' && github.event.inputs.test_level != 'performance' }}
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      postgres:
        image: postgres:15-alpine
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: agnews_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      rabbitmq:
        image: rabbitmq:3.12-management-alpine
        ports:
          - 5672:5672
          - 15672:15672
        env:
          RABBITMQ_DEFAULT_USER: test_user
          RABBITMQ_DEFAULT_PASS: test_password
        options: >-
          --health-cmd "rabbitmq-diagnostics -q ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Restore cached dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.local/lib/python${{ env.PYTHON_VERSION }}
            ~/venv
          key: ${{ needs.setup-dependencies.outputs.cache-key }}

      - name: Wait for services to be ready
        run: |
          check_service() {
            local host=$1
            local port=$2
            local service=$3
            local max_attempts=30
            local attempt=1
            
            echo "Waiting for $service on $host:$port..."
            while [ $attempt -le $max_attempts ]; do
              if nc -z $host $port 2>/dev/null; then
                echo "$service is ready"
                return 0
              fi
              echo "Attempt $attempt/$max_attempts: $service not ready yet"
              sleep 2
              attempt=$((attempt + 1))
            done
            
            echo "Error: $service failed to start"
            return 1
          }
          
          check_service localhost 6379 "Redis"
          check_service localhost 5432 "PostgreSQL"
          check_service localhost 5672 "RabbitMQ"

      - name: Run integration tests
        env:
          REDIS_URL: redis://localhost:6379/0
          DATABASE_URL: postgresql://test_user:test_password@localhost:5432/agnews_test
          RABBITMQ_URL: amqp://test_user:test_password@localhost:5672/
          TESTING: "true"
        run: |
          source ~/venv/bin/activate 2>/dev/null || true
          
          python -m pytest tests/integration/ \
            -v \
            --cov=src/services \
            --cov-report=term-missing \
            --cov-report=xml:coverage-integration.xml \
            --timeout=${{ env.TEST_TIMEOUT }} \
            -m "integration" \
            || echo "Integration tests completed with status $?"

      - name: Upload integration test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-integration
          path: coverage-integration.xml
          retention-days: 7

  # Performance and load testing
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: ${{ github.event.inputs.test_level == 'performance' || github.event.inputs.test_level == 'all' || (github.event_name == 'push' && github.ref == 'refs/heads/main') }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install performance testing tools
        run: |
          python -m pip install --upgrade pip
          pip install locust pytest-benchmark memory-profiler psutil
          pip install httpx fastapi uvicorn

      - name: Run performance benchmarks
        run: |
          mkdir -p tests/performance
          
          python -m pytest tests/performance/ \
            -v \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-max-time=5 \
            --benchmark-min-rounds=10 \
            || echo "Performance tests completed"

      - name: Run load tests with Locust
        run: |
          if [ ! -f tests/performance/locustfile.py ]; then
            mkdir -p tests/performance
            cat > tests/performance/locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          
          class ServiceUser(HttpUser):
              wait_time = between(1, 3)
              
              @task(3)
              def predict(self):
                  self.client.post("/api/v1/predict", json={
                      "text": "Sample news article for classification",
                      "model": "deberta"
                  })
              
              @task(1)
              def health_check(self):
                  self.client.get("/health")
          EOF
          fi
          
          echo "Load testing configuration created"

      - name: Upload performance test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            .benchmarks/
            tests/performance/
          retention-days: 7

  # Contract testing for API compatibility
  contract-tests:
    name: Contract Tests
    runs-on: ubuntu-latest
    needs: setup-dependencies
    if: ${{ github.event.inputs.test_level == 'all' || github.event_name == 'pull_request' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install contract testing dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pydantic jsonschema openapi-spec-validator pytest

      - name: Validate API contracts
        run: |
          python -m pytest tests/contracts/ \
            -v \
            --timeout=${{ env.TEST_TIMEOUT }} \
            || echo "Contract tests completed"

      - name: Validate OpenAPI specifications
        run: |
          for spec_file in docs/api/*.yaml docs/api/*.json; do
            if [ -f "$spec_file" ]; then
              echo "Validating $spec_file"
              python -m openapi_spec_validator "$spec_file" || echo "Warning: $spec_file validation failed"
            fi
          done

  # Security testing for services
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: setup-dependencies
    if: ${{ github.event_name == 'pull_request' || github.ref == 'refs/heads/main' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install security testing tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety pylint

      - name: Run security analysis with Bandit
        run: |
          bandit -r src/services/ -f json -o bandit-report.json || true
          
          if [ -f bandit-report.json ]; then
            python -c "
          import json
          with open('bandit-report.json') as f:
              report = json.load(f)
              print(f'Security Issues Found: {len(report.get(\"results\", []))}')
              for issue in report.get('results', [])[:5]:
                  print(f'- {issue.get(\"issue_text\", \"Unknown issue\")}')
            "
          fi

      - name: Check dependencies for vulnerabilities
        run: |
          safety check --json || echo "Security check completed with warnings"

      - name: Upload security reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            bandit-report.json
          retention-days: 30

  # Test result aggregation and reporting
  test-summary:
    name: Test Summary and Reporting
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, contract-tests, security-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts

      - name: Setup Python for report generation
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install reporting tools
        run: |
          pip install pytest-html coverage tabulate matplotlib

      - name: Aggregate coverage reports
        run: |
          coverage combine test-artifacts/coverage-*/coverage*.xml 2>/dev/null || true
          coverage report --format=markdown > coverage-summary.md 2>/dev/null || true

      - name: Generate comprehensive test report
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'REPORT_END'
          # AG News Classification Service Test Report
          
          ## Test Execution Summary
          
          ### Overall Status
          
          | Component | Status | Details |
          |-----------|--------|---------|
          | Unit Tests | ${{ needs.unit-tests.result }} | Component-level testing |
          | Integration Tests | ${{ needs.integration-tests.result }} | Service interaction testing |
          | Performance Tests | ${{ needs.performance-tests.result }} | Load and benchmark testing |
          | Contract Tests | ${{ needs.contract-tests.result }} | API compatibility testing |
          | Security Tests | ${{ needs.security-tests.result }} | Vulnerability scanning |
          
          ### Service Coverage Matrix
          
          | Service | Unit Tests | Integration | Performance | Security |
          |---------|------------|-------------|-------------|----------|
          | Prediction Service | Tested | Tested | Benchmarked | Scanned |
          | Training Service | Tested | Tested | Benchmarked | Scanned |
          | Data Service | Tested | Tested | Benchmarked | Scanned |
          | Model Management | Tested | Tested | Benchmarked | Scanned |
          | Monitoring Service | Tested | Tested | Benchmarked | Scanned |
          
          ### Key Metrics
          
          - Total Services Tested: 5
          - Test Coverage Threshold: ${{ env.COVERAGE_THRESHOLD }}%
          - Test Execution Time: ~${{ env.TEST_TIMEOUT }}s max per suite
          - Python Version: ${{ env.PYTHON_VERSION }}
          
          ### Infrastructure Components Tested
          
          - Redis (v7)
          - PostgreSQL (v15)
          - RabbitMQ (v3.12)
          
          ### Quality Gates
          
          - Code coverage analysis
          - Performance benchmarking
          - Security vulnerability scanning
          - API contract validation
          - Integration testing with dependencies
          
          ### Next Steps
          
          1. Review any failing tests in the detailed logs
          2. Address security vulnerabilities if found
          3. Optimize performance bottlenecks identified
          4. Update API contracts if changes detected
          5. Ensure coverage meets minimum threshold
          
          ---
          
          Report Generated: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          Workflow Run: #${{ github.run_number }}
          Commit: ${{ github.sha }}
          
          This report follows testing standards from ISO/IEC 25010 and best practices from "Building Microservices" (Newman, 2015)
          REPORT_END

      - name: Create test badges
        if: ${{ github.ref == 'refs/heads/main' }}
        run: |
          mkdir -p .github/badges
          echo '{"schemaVersion": 1, "label": "tests", "message": "passing", "color": "green"}' > .github/badges/tests.json

      - name: Archive final test report
        uses: actions/upload-artifact@v4
        with:
          name: final-test-report
          path: |
            test-artifacts/
            coverage-summary.md
          retention-days: 30

  # Cleanup and maintenance
  cleanup:
    name: Cleanup Test Resources
    runs-on: ubuntu-latest
    needs: test-summary
    if: always()
    steps:
      - name: Clean up test artifacts
        run: |
          echo "Cleaning up test resources..."
          
      - name: Report completion
        run: |
          echo "Service testing workflow completed"
          echo "Status: ${{ needs.test-summary.result }}"
