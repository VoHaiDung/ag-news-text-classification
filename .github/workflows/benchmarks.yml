# Performance Benchmarking Workflow
# ==================================
# Automated performance testing following:
# - MLPerf Benchmarking Standards
# - IEEE Performance Evaluation Guidelines
# - Academic Research Benchmarking Practices
#
# Author: Võ Hải Dũng
# License: MIT
# Version: 1.0.0

name: Benchmarks

on:
  push:
    branches: [main]
    paths:
      - 'src/**/*.py'
      - 'benchmarks/**'
      - '.github/workflows/benchmarks.yml'
  schedule:
    - cron: '0 3 * * 1'  # Weekly on Monday at 3 AM UTC
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - inference
          - training
          - memory
          - data

env:
  PYTHON_VERSION: '3.10'
  BENCHMARK_TIMEOUT: 600

jobs:
  # Inference performance benchmarks
  inference-benchmarks:
    name: Inference Benchmarks
    runs-on: ubuntu-latest
    strategy:
      matrix:
        model_variant: [baseline, optimized]
    steps:
      # Step 1: Repository checkout
      - name: Checkout repository
        uses: actions/checkout@v4
        
      # Step 2: Python environment setup
      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      # Step 3: Cache management
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/huggingface
          key: ${{ runner.os }}-benchmark-${{ hashFiles('requirements/*.txt') }}
          restore-keys: |
            ${{ runner.os }}-benchmark-
          
      # Step 4: Install benchmarking dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy scipy pandas
          pip install pytest-benchmark memory_profiler psutil
          pip install matplotlib seaborn  # For visualization
          
      # Step 5: Create benchmark directory
      - name: Prepare benchmark environment
        run: |
          mkdir -p benchmarks/{results,scripts,reports}
          
      # Step 6: Run inference benchmarks
      - name: Execute inference benchmarks
        run: |
          echo "Running inference benchmarks for model: ${{ matrix.model_variant }}"
          
          # Create benchmark script
          cat > benchmarks/scripts/benchmark_inference.py << 'PYTHON'
          """
          Inference benchmark for AG News Classification.
          Measures model inference performance metrics.
          """
          import time
          import json
          import numpy as np
          from datetime import datetime
          import os
          import sys
          
          class InferenceBenchmark:
              """Benchmark class for inference performance measurement."""
              
              def __init__(self, model_variant="baseline"):
                  self.model_variant = model_variant
                  self.results = {
                      "model_variant": model_variant,
                      "timestamp": datetime.now().isoformat(),
                      "system_info": self._get_system_info()
                  }
              
              def _get_system_info(self):
                  """Collect system information for reproducibility."""
                  import platform
                  return {
                      "python_version": platform.python_version(),
                      "platform": platform.platform(),
                      "processor": platform.processor(),
                      "machine": platform.machine()
                  }
              
              def benchmark_forward_pass(self, batch_size=32, sequence_length=128, num_iterations=100):
                  """Benchmark forward pass performance."""
                  print(f"Benchmarking forward pass: batch_size={batch_size}, seq_len={sequence_length}")
                  
                  # Simulate model inference
                  latencies = []
                  
                  for i in range(num_iterations):
                      # Simulate input tensor
                      input_data = np.random.randn(batch_size, sequence_length, 768)
                      
                      start_time = time.perf_counter()
                      
                      # Simulate forward pass operations
                      # Layer normalization
                      normalized = (input_data - np.mean(input_data)) / (np.std(input_data) + 1e-8)
                      
                      # Attention mechanism simulation
                      attention_scores = np.matmul(normalized, normalized.transpose(0, 2, 1))
                      attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)
                      
                      # Output projection
                      output = np.mean(attention_weights, axis=1)
                      
                      # Classification head
                      logits = np.random.randn(batch_size, 4)  # 4 classes for AG News
                      probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)
                      
                      end_time = time.perf_counter()
                      latencies.append(end_time - start_time)
                  
                  # Calculate statistics
                  latencies = np.array(latencies)
                  self.results["forward_pass"] = {
                      "batch_size": batch_size,
                      "sequence_length": sequence_length,
                      "num_iterations": num_iterations,
                      "mean_latency_ms": float(np.mean(latencies) * 1000),
                      "std_latency_ms": float(np.std(latencies) * 1000),
                      "min_latency_ms": float(np.min(latencies) * 1000),
                      "max_latency_ms": float(np.max(latencies) * 1000),
                      "p50_latency_ms": float(np.percentile(latencies, 50) * 1000),
                      "p95_latency_ms": float(np.percentile(latencies, 95) * 1000),
                      "p99_latency_ms": float(np.percentile(latencies, 99) * 1000),
                      "throughput_samples_per_second": float(batch_size * num_iterations / np.sum(latencies))
                  }
                  
                  return self.results["forward_pass"]
              
              def benchmark_batch_processing(self):
                  """Benchmark different batch sizes."""
                  batch_sizes = [1, 8, 16, 32, 64]
                  batch_results = []
                  
                  for batch_size in batch_sizes:
                      result = self.benchmark_forward_pass(
                          batch_size=batch_size,
                          sequence_length=128,
                          num_iterations=50
                      )
                      batch_results.append({
                          "batch_size": batch_size,
                          "mean_latency_ms": result["mean_latency_ms"],
                          "throughput": result["throughput_samples_per_second"]
                      })
                  
                  self.results["batch_processing"] = batch_results
                  return batch_results
              
              def save_results(self, output_dir="benchmarks/results"):
                  """Save benchmark results to JSON file."""
                  os.makedirs(output_dir, exist_ok=True)
                  
                  filename = f"inference_{self.model_variant}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                  filepath = os.path.join(output_dir, filename)
                  
                  with open(filepath, 'w') as f:
                      json.dump(self.results, f, indent=2)
                  
                  print(f"Results saved to {filepath}")
                  return filepath
          
          def main():
              """Main benchmark execution."""
              model_variant = sys.argv[1] if len(sys.argv) > 1 else "baseline"
              
              print(f"Starting inference benchmark for {model_variant} model")
              
              benchmark = InferenceBenchmark(model_variant)
              
              # Run benchmarks
              benchmark.benchmark_forward_pass()
              benchmark.benchmark_batch_processing()
              
              # Save results
              benchmark.save_results()
              
              # Print summary
              print("\nBenchmark Summary:")
              print(f"Model Variant: {model_variant}")
              print(f"Mean Latency: {benchmark.results['forward_pass']['mean_latency_ms']:.2f} ms")
              print(f"Throughput: {benchmark.results['forward_pass']['throughput_samples_per_second']:.2f} samples/sec")
          
          if __name__ == "__main__":
              main()
          PYTHON
          
          # Execute benchmark
          python benchmarks/scripts/benchmark_inference.py ${{ matrix.model_variant }}
          
      # Step 7: Upload benchmark results
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: inference-benchmarks-${{ matrix.model_variant }}
          path: benchmarks/results/
          retention-days: 30
          if-no-files-found: warn

  # Memory usage benchmarks
  memory-benchmarks:
    name: Memory Benchmarks
    runs-on: ubuntu-latest
    steps:
      # Step 1: Repository checkout
      - name: Checkout repository
        uses: actions/checkout@v4
        
      # Step 2: Python environment setup
      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install memory_profiler psutil numpy pandas
          
      # Step 4: Run memory benchmarks
      - name: Execute memory benchmarks
        run: |
          echo "Running memory usage benchmarks..."
          
          mkdir -p benchmarks/{results,scripts}
          
          # Create memory benchmark script
          cat > benchmarks/scripts/benchmark_memory.py << 'PYTHON'
          """
          Memory usage benchmark for AG News Classification.
          Profiles memory consumption patterns.
          """
          import psutil
          import json
          import numpy as np
          import gc
          from datetime import datetime
          import os
          
          class MemoryBenchmark:
              """Memory profiling for model operations."""
              
              def __init__(self):
                  self.process = psutil.Process()
                  self.results = {
                      "timestamp": datetime.now().isoformat(),
                      "measurements": []
                  }
              
              def get_memory_info(self):
                  """Get current memory usage statistics."""
                  mem_info = self.process.memory_info()
                  return {
                      "rss_mb": mem_info.rss / 1024 / 1024,  # Resident Set Size
                      "vms_mb": mem_info.vms / 1024 / 1024,  # Virtual Memory Size
                      "percent": self.process.memory_percent()
                  }
              
              def benchmark_model_loading(self):
                  """Benchmark memory usage during model loading simulation."""
                  print("Benchmarking model loading memory usage...")
                  
                  # Initial memory
                  gc.collect()
                  initial_memory = self.get_memory_info()
                  
                  # Simulate model parameters
                  model_sizes = {
                      "embeddings": (30000, 768),  # Vocabulary x Hidden size
                      "encoder_layers": [(768, 768) for _ in range(12)],  # 12 transformer layers
                      "classifier": (768, 4)  # Hidden size x Num classes
                  }
                  
                  allocated_arrays = []
                  
                  # Allocate embeddings
                  embeddings = np.random.randn(*model_sizes["embeddings"]).astype(np.float32)
                  allocated_arrays.append(embeddings)
                  embeddings_memory = self.get_memory_info()
                  
                  # Allocate encoder layers
                  for layer_shape in model_sizes["encoder_layers"]:
                      layer = np.random.randn(*layer_shape).astype(np.float32)
                      allocated_arrays.append(layer)
                  
                  encoder_memory = self.get_memory_info()
                  
                  # Allocate classifier
                  classifier = np.random.randn(*model_sizes["classifier"]).astype(np.float32)
                  allocated_arrays.append(classifier)
                  
                  peak_memory = self.get_memory_info()
                  
                  # Cleanup
                  del allocated_arrays
                  gc.collect()
                  
                  final_memory = self.get_memory_info()
                  
                  # Store results
                  self.results["model_loading"] = {
                      "initial_memory_mb": initial_memory["rss_mb"],
                      "embeddings_memory_mb": embeddings_memory["rss_mb"],
                      "encoder_memory_mb": encoder_memory["rss_mb"],
                      "peak_memory_mb": peak_memory["rss_mb"],
                      "final_memory_mb": final_memory["rss_mb"],
                      "memory_increase_mb": peak_memory["rss_mb"] - initial_memory["rss_mb"],
                      "memory_released_mb": peak_memory["rss_mb"] - final_memory["rss_mb"]
                  }
                  
                  return self.results["model_loading"]
              
              def benchmark_batch_processing_memory(self):
                  """Benchmark memory usage for different batch sizes."""
                  print("Benchmarking batch processing memory usage...")
                  
                  batch_sizes = [1, 8, 16, 32, 64]
                  batch_results = []
                  
                  for batch_size in batch_sizes:
                      gc.collect()
                      initial = self.get_memory_info()
                      
                      # Simulate batch processing
                      batch_data = np.random.randn(batch_size, 128, 768).astype(np.float32)
                      intermediate = np.matmul(batch_data, batch_data.transpose(0, 2, 1))
                      output = np.mean(intermediate, axis=1)
                      
                      peak = self.get_memory_info()
                      
                      del batch_data, intermediate, output
                      gc.collect()
                      
                      final = self.get_memory_info()
                      
                      batch_results.append({
                          "batch_size": batch_size,
                          "peak_memory_mb": peak["rss_mb"],
                          "memory_per_sample_mb": (peak["rss_mb"] - initial["rss_mb"]) / batch_size
                      })
                  
                  self.results["batch_processing"] = batch_results
                  return batch_results
              
              def save_results(self, output_dir="benchmarks/results"):
                  """Save benchmark results."""
                  os.makedirs(output_dir, exist_ok=True)
                  
                  filename = f"memory_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                  filepath = os.path.join(output_dir, filename)
                  
                  with open(filepath, 'w') as f:
                      json.dump(self.results, f, indent=2)
                  
                  print(f"Results saved to {filepath}")
                  return filepath
          
          def main():
              """Main execution."""
              benchmark = MemoryBenchmark()
              
              # Run benchmarks
              benchmark.benchmark_model_loading()
              benchmark.benchmark_batch_processing_memory()
              
              # Save results
              benchmark.save_results()
              
              # Print summary
              print("\nMemory Benchmark Summary:")
              print(f"Peak Memory: {benchmark.results['model_loading']['peak_memory_mb']:.2f} MB")
              print(f"Memory Increase: {benchmark.results['model_loading']['memory_increase_mb']:.2f} MB")
          
          if __name__ == "__main__":
              main()
          PYTHON
          
          # Execute benchmark
          python benchmarks/scripts/benchmark_memory.py
          
      # Step 5: Upload results
      - name: Upload memory benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: memory-benchmarks
          path: benchmarks/results/
          retention-days: 30
          if-no-files-found: warn

  # Benchmark report generation
  benchmark-report:
    name: Generate Benchmark Report
    runs-on: ubuntu-latest
    needs: [inference-benchmarks, memory-benchmarks]
    if: always()
    steps:
      # Step 1: Repository checkout
      - name: Checkout repository
        uses: actions/checkout@v4
        
      # Step 2: Download artifacts
      - name: Download benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          path: benchmarks/artifacts/
        continue-on-error: true
        
      # Step 3: Generate comprehensive report
      - name: Generate benchmark report
        run: |
          echo "Generating comprehensive benchmark report..."
          
          mkdir -p benchmarks
          
          # Create detailed report
          cat > benchmarks/REPORT.md << 'MARKDOWN'
          # Performance Benchmark Report
          
          ## Executive Summary
          
          This report presents comprehensive performance benchmarks for the AG News Classification system.
          
          ### Report Metadata
          
          - **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          - **Commit SHA:** ${{ github.sha }}
          - **Branch:** ${{ github.ref_name }}
          - **Run ID:** ${{ github.run_id }}
          - **Triggered by:** ${{ github.event_name }}
          
          ## Benchmark Results
          
          ### Inference Performance
          
          The inference benchmarks measure model forward pass latency and throughput across different configurations.
          
          | Metric | Status |
          |--------|--------|
          | Inference Benchmarks | ${{ needs.inference-benchmarks.result }} |
          | Memory Benchmarks | ${{ needs.memory-benchmarks.result }} |
          
          ### Key Performance Indicators
          
          1. **Latency**: Time required for single inference
          2. **Throughput**: Number of samples processed per second
          3. **Memory Usage**: RAM consumption during operations
          4. **Scalability**: Performance across different batch sizes
          
          ## Recommendations
          
          Based on the benchmark results:
          
          1. Monitor performance trends across commits
          2. Identify and optimize bottlenecks
          3. Compare against baseline metrics
          4. Ensure scalability requirements are met
          
          ## Detailed Results
          
          Detailed benchmark data is available in the artifacts section.
          
          ---
          *Generated automatically by the benchmarking workflow*
          MARKDOWN
          
          echo "Benchmark report generated successfully"
          
      # Step 4: Upload final report
      - name: Upload benchmark report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report
          path: benchmarks/REPORT.md
          retention-days: 90
          if-no-files-found: warn
