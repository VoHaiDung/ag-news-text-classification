# Performance Benchmarking Workflow
# ==================================
# Automated performance testing following:
# - MLPerf Benchmarking Standards
# - IEEE Performance Evaluation Guidelines
# - Academic Research Benchmarking Practices
#
# Author: AG News Research Team
# License: MIT

name: Benchmarks

on:
  push:
    branches: [main]
    paths:
      - 'src/**/*.py'
      - 'benchmarks/**'
      - '.github/workflows/benchmarks.yml'
  schedule:
    - cron: '0 3 * * 1'  # Weekly on Monday at 3 AM UTC
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - inference
          - training
          - memory
          - data

env:
  PYTHON_VERSION: '3.10'
  BENCHMARK_TIMEOUT: 600
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  PIP_NO_CACHE_DIR: 1
  PYTHONUNBUFFERED: 1

jobs:
  # Inference performance benchmarks
  inference-benchmarks:
    name: Inference Benchmarks (${{ matrix.model_variant }})
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        model_variant: [baseline, optimized]
    
    steps:
      # Repository checkout
      - name: Checkout repository
        uses: actions/checkout@v4
        
      # Python environment setup
      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      # Cache management
      - name: Cache dependencies
        uses: actions/cache@v4
        id: cache
        with:
          path: |
            ~/.cache/pip
            ~/.cache/huggingface
            ~/venv
          key: ${{ runner.os }}-benchmark-${{ hashFiles('requirements/*.txt') }}-v2
          restore-keys: |
            ${{ runner.os }}-benchmark-
          
      # Create virtual environment
      - name: Create virtual environment
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          python -m venv ~/venv
          echo "$HOME/venv/bin" >> $GITHUB_PATH
          
      # Install system dependencies
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            python3-dev \
            libfreetype6-dev \
            pkg-config
          
      # Upgrade pip and install wheel
      - name: Upgrade pip and setuptools
        run: |
          source ~/venv/bin/activate 2>/dev/null || true
          python -m pip install --upgrade pip setuptools wheel
          
      # Install core dependencies with retry
      - name: Install core dependencies
        run: |
          source ~/venv/bin/activate 2>/dev/null || true
          
          # Function to install with retry
          install_with_retry() {
            local package=$1
            local max_attempts=3
            local attempt=1
            
            while [ $attempt -le $max_attempts ]; do
              echo "Installing $package (attempt $attempt/$max_attempts)..."
              if pip install --timeout 60 --retries 5 "$package"; then
                echo "Successfully installed $package"
                return 0
              fi
              echo "Failed to install $package, retrying..."
              sleep 5
              attempt=$((attempt + 1))
            done
            
            echo "Failed to install $package after $max_attempts attempts"
            return 1
          }
          
          # Install packages one by one with retry
          install_with_retry "numpy>=1.21.0,<2.0.0"
          install_with_retry "scipy>=1.7.0"
          install_with_retry "pandas>=1.3.0"
          
      # Install benchmark dependencies
      - name: Install benchmark dependencies
        run: |
          source ~/venv/bin/activate 2>/dev/null || true
          
          # Install testing and profiling tools
          pip install --timeout 60 --retries 5 \
            pytest-benchmark \
            memory_profiler \
            psutil \
            tabulate
          
      # Install visualization dependencies (optional, with fallback)
      - name: Install visualization dependencies
        continue-on-error: true
        run: |
          source ~/venv/bin/activate 2>/dev/null || true
          
          # Try to install matplotlib with fallback
          pip install --timeout 60 --retries 5 matplotlib || \
          pip install --timeout 60 --index-url https://pypi.python.org/simple/ matplotlib || \
          echo "Warning: matplotlib installation failed, continuing without visualization"
          
          # Try to install seaborn (optional)
          pip install --timeout 60 --retries 5 seaborn || \
          echo "Warning: seaborn installation failed, continuing without it"
          
      # Create benchmark directory structure
      - name: Prepare benchmark environment
        run: |
          mkdir -p benchmarks/{results,scripts,reports,logs}
          mkdir -p benchmarks/results/${{ matrix.model_variant }}
          
      # Create and run inference benchmark script
      - name: Execute inference benchmarks
        run: |
          source ~/venv/bin/activate 2>/dev/null || true
          
          echo "Running inference benchmarks for model: ${{ matrix.model_variant }}"
          
          # Create benchmark script
          cat > benchmarks/scripts/benchmark_inference.py << 'PYTHON'
          """
          Inference Benchmark for AG News Classification
          
          Measures model inference performance metrics including:
          - Latency (mean, p50, p95, p99)
          - Throughput (samples/second)
          - Memory usage
          - CPU utilization
          """
          
          import time
          import json
          import numpy as np
          from datetime import datetime
          import os
          import sys
          import gc
          import warnings
          warnings.filterwarnings('ignore')
          
          # Optional imports with fallback
          try:
              import psutil
              HAS_PSUTIL = True
          except ImportError:
              HAS_PSUTIL = False
              print("Warning: psutil not available, memory metrics will be limited")
          
          try:
              from memory_profiler import memory_usage
              HAS_MEMORY_PROFILER = True
          except ImportError:
              HAS_MEMORY_PROFILER = False
              print("Warning: memory_profiler not available")
          
          
          class InferenceBenchmark:
              """Benchmark class for inference performance measurement."""
              
              def __init__(self, model_variant="baseline"):
                  self.model_variant = model_variant
                  self.results = {
                      "model_variant": model_variant,
                      "timestamp": datetime.now().isoformat(),
                      "environment": {
                          "python_version": sys.version,
                          "platform": sys.platform,
                      }
                  }
                  
                  if HAS_PSUTIL:
                      self.results["environment"]["cpu_count"] = psutil.cpu_count()
                      self.results["environment"]["memory_total_gb"] = psutil.virtual_memory().total / (1024**3)
              
              def benchmark_forward_pass(self, batch_sizes=[1, 8, 16, 32], sequence_length=128, num_iterations=100):
                  """Benchmark forward pass performance across different batch sizes."""
                  
                  all_results = []
                  
                  for batch_size in batch_sizes:
                      print(f"\nBenchmarking: batch_size={batch_size}, seq_len={sequence_length}")
                      
                      # Warmup
                      for _ in range(10):
                          input_data = np.random.randn(batch_size, sequence_length, 768).astype(np.float32)
                          _ = self._simulate_forward_pass(input_data)
                      
                      # Actual benchmark
                      latencies = []
                      memory_before = self._get_memory_usage()
                      
                      for i in range(num_iterations):
                          input_data = np.random.randn(batch_size, sequence_length, 768).astype(np.float32)
                          
                          start_time = time.perf_counter()
                          _ = self._simulate_forward_pass(input_data)
                          end_time = time.perf_counter()
                          
                          latencies.append(end_time - start_time)
                          
                          # Progress indicator
                          if (i + 1) % 20 == 0:
                              print(f"  Progress: {i + 1}/{num_iterations}")
                      
                      memory_after = self._get_memory_usage()
                      
                      # Calculate metrics
                      latencies = np.array(latencies)
                      batch_results = {
                          "batch_size": batch_size,
                          "sequence_length": sequence_length,
                          "num_iterations": num_iterations,
                          "latency_ms": {
                              "mean": float(np.mean(latencies) * 1000),
                              "std": float(np.std(latencies) * 1000),
                              "min": float(np.min(latencies) * 1000),
                              "max": float(np.max(latencies) * 1000),
                              "p50": float(np.percentile(latencies, 50) * 1000),
                              "p95": float(np.percentile(latencies, 95) * 1000),
                              "p99": float(np.percentile(latencies, 99) * 1000),
                          },
                          "throughput_samples_per_sec": float(batch_size / np.mean(latencies)),
                          "memory_delta_mb": float((memory_after - memory_before) / (1024 * 1024))
                      }
                      
                      all_results.append(batch_results)
                      
                      # Print summary
                      print(f"  Mean latency: {batch_results['latency_ms']['mean']:.2f} ms")
                      print(f"  Throughput: {batch_results['throughput_samples_per_sec']:.2f} samples/sec")
                      
                      # Cleanup
                      gc.collect()
                  
                  self.results["benchmarks"] = all_results
                  return self.results
              
              def _simulate_forward_pass(self, input_data):
                  """Simulate model forward pass with realistic operations."""
                  # Layer normalization
                  normalized = (input_data - np.mean(input_data, axis=-1, keepdims=True)) / \
                               (np.std(input_data, axis=-1, keepdims=True) + 1e-8)
                  
                  # Simulated attention mechanism
                  batch_size, seq_len, hidden_dim = input_data.shape
                  
                  # Query, Key, Value projections (simplified)
                  q = np.matmul(normalized, np.random.randn(hidden_dim, hidden_dim).astype(np.float32) * 0.02)
                  k = np.matmul(normalized, np.random.randn(hidden_dim, hidden_dim).astype(np.float32) * 0.02)
                  v = np.matmul(normalized, np.random.randn(hidden_dim, hidden_dim).astype(np.float32) * 0.02)
                  
                  # Attention scores
                  scores = np.matmul(q, k.transpose(0, 2, 1)) / np.sqrt(hidden_dim)
                  
                  # Softmax
                  exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
                  attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)
                  
                  # Apply attention
                  attention_output = np.matmul(attention_weights, v)
                  
                  # Feed-forward network
                  ff_output = np.matmul(attention_output, np.random.randn(hidden_dim, hidden_dim * 4).astype(np.float32) * 0.02)
                  ff_output = np.maximum(0, ff_output)  # ReLU
                  ff_output = np.matmul(ff_output, np.random.randn(hidden_dim * 4, hidden_dim).astype(np.float32) * 0.02)
                  
                  # Residual connection
                  output = normalized + ff_output
                  
                  # Classification head
                  pooled = np.mean(output, axis=1)
                  logits = np.matmul(pooled, np.random.randn(hidden_dim, 4).astype(np.float32) * 0.02)
                  
                  return logits
              
              def _get_memory_usage(self):
                  """Get current memory usage in bytes."""
                  if HAS_PSUTIL:
                      process = psutil.Process()
                      return process.memory_info().rss
                  else:
                      # Fallback: basic memory estimate
                      import resource
                      return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * 1024
              
              def save_results(self, output_dir="benchmarks/results"):
                  """Save benchmark results to JSON file."""
                  os.makedirs(output_dir, exist_ok=True)
                  
                  timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                  filename = f"inference_{self.model_variant}_{timestamp}.json"
                  filepath = os.path.join(output_dir, self.model_variant, filename)
                  
                  os.makedirs(os.path.dirname(filepath), exist_ok=True)
                  
                  with open(filepath, 'w') as f:
                      json.dump(self.results, f, indent=2)
                  
                  print(f"\nResults saved to {filepath}")
                  
                  # Also save a summary
                  self._save_summary(output_dir)
              
              def _save_summary(self, output_dir):
                  """Save a human-readable summary."""
                  summary_file = os.path.join(output_dir, self.model_variant, "summary.txt")
                  
                  with open(summary_file, 'w') as f:
                      f.write(f"Inference Benchmark Summary\n")
                      f.write(f"{'=' * 50}\n")
                      f.write(f"Model Variant: {self.model_variant}\n")
                      f.write(f"Timestamp: {self.results['timestamp']}\n\n")
                      
                      if 'benchmarks' in self.results:
                          for bench in self.results['benchmarks']:
                              f.write(f"Batch Size: {bench['batch_size']}\n")
                              f.write(f"  Mean Latency: {bench['latency_ms']['mean']:.2f} ms\n")
                              f.write(f"  P95 Latency: {bench['latency_ms']['p95']:.2f} ms\n")
                              f.write(f"  Throughput: {bench['throughput_samples_per_sec']:.2f} samples/sec\n")
                              f.write(f"  Memory Delta: {bench['memory_delta_mb']:.2f} MB\n\n")
          
          def main():
              """Main benchmark execution."""
              model_variant = sys.argv[1] if len(sys.argv) > 1 else "baseline"
              
              print(f"Starting inference benchmark for {model_variant} model")
              print("=" * 60)
              
              benchmark = InferenceBenchmark(model_variant)
              
              # Run benchmarks with different configurations
              if model_variant == "baseline":
                  batch_sizes = [1, 8, 16, 32]
              else:  # optimized
                  batch_sizes = [1, 16, 32, 64]
              
              benchmark.benchmark_forward_pass(
                  batch_sizes=batch_sizes,
                  sequence_length=128,
                  num_iterations=50
              )
              
              benchmark.save_results()
              
              print("\nBenchmark completed successfully!")
          
          if __name__ == "__main__":
              main()
          PYTHON
          
          # Execute benchmark
          python benchmarks/scripts/benchmark_inference.py ${{ matrix.model_variant }}
          
      # Generate benchmark report
      - name: Generate benchmark report
        if: success()
        run: |
          source ~/venv/bin/activate 2>/dev/null || true
          
          # Create simple report generator
          cat > benchmarks/scripts/generate_report.py << 'PYTHON'
          import json
          import os
          from pathlib import Path
          
          def generate_report(variant):
              results_dir = Path(f"benchmarks/results/{variant}")
              if not results_dir.exists():
                  print(f"No results found for {variant}")
                  return
              
              # Find the latest JSON file
              json_files = list(results_dir.glob("*.json"))
              if not json_files:
                  print("No JSON files found")
                  return
              
              latest_file = max(json_files, key=os.path.getctime)
              
              with open(latest_file, 'r') as f:
                  data = json.load(f)
              
              # Create markdown report
              report_path = results_dir / "report.md"
              with open(report_path, 'w') as f:
                  f.write(f"# Inference Benchmark Report: {variant}\n\n")
                  f.write(f"**Timestamp:** {data.get('timestamp', 'N/A')}\n\n")
                  
                  if 'benchmarks' in data:
                      f.write("## Performance Metrics\n\n")
                      f.write("| Batch Size | Mean Latency (ms) | P95 Latency (ms) | Throughput (samples/sec) |\n")
                      f.write("|------------|-------------------|------------------|-------------------------|\n")
                      
                      for bench in data['benchmarks']:
                          f.write(f"| {bench['batch_size']} | "
                                 f"{bench['latency_ms']['mean']:.2f} | "
                                 f"{bench['latency_ms']['p95']:.2f} | "
                                 f"{bench['throughput_samples_per_sec']:.2f} |\n")
              
              print(f"Report generated: {report_path}")
          
          if __name__ == "__main__":
              import sys
              variant = sys.argv[1] if len(sys.argv) > 1 else "baseline"
              generate_report(variant)
          PYTHON
          
          python benchmarks/scripts/generate_report.py ${{ matrix.model_variant }}
          
      # Upload benchmark results
      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: inference-benchmarks-${{ matrix.model_variant }}
          path: |
            benchmarks/results/${{ matrix.model_variant }}/
            benchmarks/logs/
          retention-days: 30
          if-no-files-found: warn
          
      # Comment on PR with results (if PR context)
      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request' && success()
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const variant = '${{ matrix.model_variant }}';
            const summaryPath = path.join('benchmarks', 'results', variant, 'summary.txt');
            
            if (fs.existsSync(summaryPath)) {
              const summary = fs.readFileSync(summaryPath, 'utf8');
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: `## Benchmark Results: ${variant}\n\n\`\`\`\n${summary}\n\`\`\``
              });
            }

  # Summary job to check all benchmarks passed
  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: [inference-benchmarks]
    if: always()
    
    steps:
      - name: Check benchmark status
        run: |
          if [ "${{ needs.inference-benchmarks.result }}" == "success" ]; then
            echo "All benchmarks completed successfully"
            exit 0
          else
            echo "Some benchmarks failed"
            exit 1
          fi
