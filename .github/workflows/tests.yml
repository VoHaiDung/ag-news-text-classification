# ============================================================================
# Comprehensive Testing Pipeline for AG News Text Classification
# ============================================================================
# Project: AG News Text Classification (ag-news-text-classification)
# Description: Multi-level testing pipeline implementing the testing pyramid
#              for ML systems with academic rigor and reproducibility
# Author: Võ Hải Dũng
# Email: vohaidung.work@gmail.com
# License: MIT
# ============================================================================
#
# Academic Rationale:
#   This testing pipeline follows principles from:
#   - "Testing Machine Learning Systems" (Breck et al., 2017)
#   - "The ML Test Score: A Rubric for ML Production Readiness" (Google, 2017)
#   - "Continuous Testing for ML" (Sculley et al., 2015)
#   - "Test-Driven Development for Data Science" (pytest documentation)
#   - "Software Quality Assurance" (Galin, 2018)
#
# Testing Philosophy:
#   1. Test Pyramid: More unit tests, fewer integration/e2e tests
#   2. Isolation: Tests should be independent and reproducible
#   3. Speed: Fast feedback loop for developers
#   4. Coverage: Comprehensive but meaningful coverage
#   5. Clarity: Clear test names and assertions
#   6. Maintainability: Easy to update and extend
#   7. Documentation: Tests as living documentation
#   8. Determinism: Same input always produces same output
#
# Testing Levels (6-Level Pyramid):
#   Level 1: Unit Tests (60% of tests)
#     - Individual functions and classes
#     - Fast execution (milliseconds)
#     - No external dependencies
#   
#   Level 2: Integration Tests (20% of tests)
#     - Component interactions
#     - Moderate execution time (seconds)
#     - May use test databases/files
#   
#   Level 3: Platform-Specific Tests (10% of tests)
#     - Environment-specific functionality
#     - Platform detection and optimization
#     - Colab, Kaggle, Local compatibility
#   
#   Level 4: Performance Tests (5% of tests)
#     - Speed benchmarks
#     - Memory usage
#     - Scalability validation
#   
#   Level 5: End-to-End Tests (3% of tests)
#     - Complete workflows
#     - User scenarios
#     - Full pipeline validation
#   
#   Level 6: Regression Tests (2% of tests)
#     - Historical bug prevention
#     - Accuracy baseline validation
#     - Model performance tracking
#
# Test Organization:
#   tests/
#   ├── unit/                 Level 1
#   │   ├── core/
#   │   ├── data/
#   │   ├── models/
#   │   ├── training/
#   │   └── utils/
#   ├── integration/          Level 2
#   ├── platform_specific/    Level 3
#   ├── performance/          Level 4
#   ├── e2e/                  Level 5
#   ├── regression/           Level 6
#   └── fixtures/
#
# Quality Metrics:
#   - Code coverage: Minimum 60%, Target 80%
#   - Test execution time: Under 15 minutes
#   - Test flakiness: 0% acceptable
#   - Test maintainability: High (DRY principle)
#
# References:
#   - pytest documentation: https://docs.pytest.org/
#   - ML Testing: https://madewithml.com/courses/mlops/testing/
#   - Google Testing Blog: https://testing.googleblog.com/
#
# ============================================================================

name: Comprehensive Testing

# ============================================================================
# Trigger Configuration
# ============================================================================
# Academic Justification:
#   Tests run on every push and pull request to ensure code quality
#   and prevent regressions. Schedule ensures ongoing compatibility.

on:
  push:
    branches:
      - main
      - develop
      - 'feature/**'
      - 'fix/**'
      - 'test/**'
      - 'experiment/**'
    paths:
      - 'src/**'
      - 'tests/**'
      - 'configs/**'
      - 'requirements/**'
      - 'setup.py'
      - 'pyproject.toml'
      - '.github/workflows/tests.yml'
  
  pull_request:
    branches:
      - main
      - develop
    types:
      - opened
      - synchronize
      - reopened
      - ready_for_review
  
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to run'
        required: true
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'unit'
          - 'integration'
          - 'platform'
          - 'performance'
          - 'e2e'
          - 'regression'
      
      coverage_threshold:
        description: 'Minimum coverage percentage'
        required: false
        default: '60'
        type: string
      
      python_version:
        description: 'Python version for testing'
        required: false
        default: '3.10'
        type: choice
        options:
          - '3.8'
          - '3.9'
          - '3.10'
          - '3.11'
      
      verbose_output:
        description: 'Enable verbose test output'
        required: false
        default: false
        type: boolean
      
      fail_fast:
        description: 'Stop on first test failure'
        required: false
        default: false
        type: boolean
      
      run_slow_tests:
        description: 'Run slow tests (may take longer)'
        required: false
        default: false
        type: boolean
  
  schedule:
    - cron: '0 3 * * *'

# ============================================================================
# Global Environment Variables
# ============================================================================
# Academic Justification:
#   Centralized test configuration ensures consistency and reproducibility

env:
  PYTHON_VERSION_DEFAULT: '3.10'
  
  PYTEST_TIMEOUT: 600
  PYTEST_WORKERS: 'auto'
  MIN_COVERAGE: 60
  TARGET_COVERAGE: 80
  
  PROJECT_NAME: 'AG News Text Classification'
  PROJECT_SLUG: 'ag-news-text-classification'
  PROJECT_AUTHOR: 'Võ Hải Dũng'
  PROJECT_EMAIL: 'vohaidung.work@gmail.com'
  PROJECT_LICENSE: 'MIT'
  
  FORCE_COLOR: '1'
  PYTHONUNBUFFERED: '1'
  PYTHONDONTWRITEBYTECODE: '1'
  PYTEST_DISABLE_PLUGIN_AUTOLOAD: '1'

# ============================================================================
# Concurrency Control
# ============================================================================
# Academic Justification:
#   Cancel redundant test runs to optimize CI resources

concurrency:
  group: tests-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# ============================================================================
# Jobs Definition
# ============================================================================

jobs:
  # ==========================================================================
  # Job 1: Unit Tests
  # ==========================================================================
  # Academic Justification:
  #   Unit tests form the foundation of the testing pyramid, providing fast
  #   feedback on individual component correctness. Target: 60% of test suite.

  unit-tests:
    name: Unit Tests (Python ${{ matrix.python-version }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    
    strategy:
      fail-fast: ${{ github.event.inputs.fail_fast == 'true' }}
      matrix:
        os: [ubuntu-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11']
        include:
          - os: ubuntu-latest
            python-version: '3.10'
            coverage: true
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-test-${{ matrix.python-version }}-${{ hashFiles('requirements/base.txt', 'requirements/dev.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-test-${{ matrix.python-version }}-
            ${{ runner.os }}-pip-test-
            ${{ runner.os }}-pip-
      
      - name: Install testing dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            pytest>=7.4.0 \
            pytest-cov>=4.1.0 \
            pytest-xdist>=3.5.0 \
            pytest-timeout>=2.2.0 \
            pytest-mock>=3.12.0 \
            pytest-benchmark>=4.0.0 \
            pytest-asyncio>=0.21.0 \
            coverage>=7.4.0 \
            pyyaml>=6.0 \
            numpy>=1.24.0 \
            pandas>=2.0.0 \
            scikit-learn>=1.3.0
          
          if [ -f "requirements/base.txt" ]; then
            pip install -r requirements/base.txt || echo "Base requirements partially installed"
          fi
      
      - name: Create comprehensive test structure
        run: |
          mkdir -p tests/{unit,integration,platform_specific,performance,e2e,regression,chaos,compatibility,fixtures}
          mkdir -p tests/unit/{core,data,models,training,evaluation,inference,api,services,utils,deployment}
          
          cat > pytest.ini << 'PYTEST_EOF'
          [pytest]
          minversion = 7.0
          testpaths = tests
          python_files = test_*.py
          python_classes = Test*
          python_functions = test_*
          
          addopts =
              -v
              --strict-markers
              --tb=short
              --disable-warnings
          
          markers =
              unit: Unit tests (fast, isolated)
              integration: Integration tests (component interactions)
              platform: Platform-specific tests (Colab, Kaggle, Local)
              performance: Performance benchmarks
              e2e: End-to-end workflow tests
              regression: Regression tests (bug prevention)
              slow: Slow-running tests
              gpu: Tests requiring GPU
              network: Tests requiring network
              overfitting: Overfitting prevention tests
          
          timeout = 300
          PYTEST_EOF
          
          cat > tests/conftest.py << 'CONFTEST_EOF'
          """
          Root conftest for AG News Text Classification test suite.
          
          This module provides shared fixtures and configuration for all tests,
          implementing best practices for reproducible testing in ML systems.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          import sys
          from pathlib import Path
          import pytest
          import random
          import numpy as np
          
          project_root = Path(__file__).parent.parent
          if str(project_root) not in sys.path:
              sys.path.insert(0, str(project_root))
          
          src_dir = project_root / 'src'
          if src_dir.exists() and str(src_dir) not in sys.path:
              sys.path.insert(0, str(src_dir))
          
          @pytest.fixture(scope="session", autouse=True)
          def set_random_seeds():
              """Set random seeds for reproducibility."""
              random.seed(42)
              np.random.seed(42)
          
          @pytest.fixture(scope="session")
          def project_root_path():
              """Provide project root path."""
              return Path(__file__).parent.parent
          
          @pytest.fixture(scope="session")
          def src_path(project_root_path):
              """Provide src directory path."""
              return project_root_path / 'src'
          
          @pytest.fixture(scope="session")
          def test_data_path(project_root_path):
              """Provide test data directory path."""
              data_path = project_root_path / 'data' / 'test_samples'
              data_path.mkdir(parents=True, exist_ok=True)
              return data_path
          
          @pytest.fixture(scope="session")
          def configs_path(project_root_path):
              """Provide configs directory path."""
              return project_root_path / 'configs'
          
          @pytest.fixture
          def sample_text():
              """Provide sample text for testing."""
              return "This is a sample news article for testing purposes."
          
          @pytest.fixture
          def sample_texts():
              """Provide multiple sample texts."""
              return [
                  "World news about international events.",
                  "Sports news covering latest matches.",
                  "Business news on market trends.",
                  "Technology news about innovations."
              ]
          
          @pytest.fixture
          def sample_labels():
              """Provide sample labels for AG News dataset."""
              return [0, 1, 2, 3]
          
          @pytest.fixture
          def sample_dataset():
              """Provide sample dataset."""
              return {
                  'texts': [
                      "World news about international events.",
                      "Sports news covering latest matches.",
                      "Business news on market trends.",
                      "Technology news about innovations."
                  ],
                  'labels': [0, 1, 2, 3]
              }
          
          def pytest_configure(config):
              """Configure pytest with custom markers."""
              config.addinivalue_line("markers", "unit: Unit tests")
              config.addinivalue_line("markers", "integration: Integration tests")
              config.addinivalue_line("markers", "platform: Platform-specific tests")
              config.addinivalue_line("markers", "performance: Performance tests")
              config.addinivalue_line("markers", "e2e: End-to-end tests")
              config.addinivalue_line("markers", "regression: Regression tests")
              config.addinivalue_line("markers", "slow: Slow-running tests")
              config.addinivalue_line("markers", "gpu: GPU-required tests")
              config.addinivalue_line("markers", "network: Network-required tests")
              config.addinivalue_line("markers", "overfitting: Overfitting prevention tests")
          CONFTEST_EOF
      
      - name: Create comprehensive unit test files
        run: |
          cat > tests/unit/test_imports.py << 'TEST_EOF'
          """
          Unit tests for dependency imports.
          
          These tests verify that all required dependencies are properly installed
          and importable, following best practices for dependency management.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          import sys
          import pytest
          
          @pytest.mark.unit
          class TestPythonEnvironment:
              """Test Python environment setup."""
              
              def test_python_version(self):
                  """Verify Python version meets requirements."""
                  assert sys.version_info >= (3, 8), "Python 3.8+ required"
                  assert sys.version_info.major == 3
                  assert sys.version_info.minor >= 8
              
              def test_python_executable(self):
                  """Verify Python executable exists."""
                  assert sys.executable is not None
                  assert len(sys.executable) > 0
          
          @pytest.mark.unit
          class TestStandardLibrary:
              """Test standard library imports."""
              
              def test_os_module(self):
                  """Test os module import."""
                  import os
                  assert os is not None
                  assert hasattr(os, 'path')
              
              def test_sys_module(self):
                  """Test sys module import."""
                  import sys
                  assert sys is not None
                  assert hasattr(sys, 'version_info')
              
              def test_pathlib_module(self):
                  """Test pathlib module import."""
                  from pathlib import Path
                  assert Path is not None
              
              def test_json_module(self):
                  """Test json module import."""
                  import json
                  assert json is not None
                  test_dict = {'key': 'value'}
                  json_str = json.dumps(test_dict)
                  loaded = json.loads(json_str)
                  assert loaded == test_dict
              
              def test_logging_module(self):
                  """Test logging module import."""
                  import logging
                  assert logging is not None
                  logger = logging.getLogger(__name__)
                  assert logger is not None
          
          @pytest.mark.unit
          class TestDataScienceLibraries:
              """Test data science library imports."""
              
              def test_yaml_import(self):
                  """Test YAML library."""
                  import yaml
                  test_data = {'key': 'value', 'number': 42}
                  yaml_str = yaml.dump(test_data)
                  loaded = yaml.safe_load(yaml_str)
                  assert loaded == test_data
              
              def test_numpy_import(self):
                  """Test NumPy library."""
                  import numpy as np
                  array = np.array([1, 2, 3, 4, 5])
                  assert len(array) == 5
                  assert array.sum() == 15
                  assert array.mean() == 3.0
              
              def test_pandas_import(self):
                  """Test Pandas library."""
                  import pandas as pd
                  df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
                  assert len(df) == 3
                  assert list(df.columns) == ['A', 'B']
                  assert df['A'].sum() == 6
              
              def test_sklearn_import(self):
                  """Test scikit-learn library."""
                  from sklearn.model_selection import train_test_split
                  from sklearn.metrics import accuracy_score
                  assert train_test_split is not None
                  assert accuracy_score is not None
          
          @pytest.mark.unit
          @pytest.mark.slow
          class TestOptionalLibraries:
              """Test optional library imports."""
              
              def test_torch_import(self):
                  """Test PyTorch library if available."""
                  try:
                      import torch
                      assert torch is not None
                  except ImportError:
                      pytest.skip("PyTorch not installed")
              
              def test_transformers_import(self):
                  """Test Transformers library if available."""
                  try:
                      import transformers
                      assert transformers is not None
                  except ImportError:
                      pytest.skip("Transformers not installed")
          TEST_EOF
          
          cat > tests/unit/test_project_structure.py << 'STRUCTURE_EOF'
          """
          Unit tests for project structure validation.
          
          These tests ensure the project maintains its documented structure
          and all critical directories and files exist.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          from pathlib import Path
          import pytest
          
          @pytest.mark.unit
          class TestProjectStructure:
              """Test project directory structure."""
              
              def test_project_root_exists(self, project_root_path):
                  """Verify project root directory exists."""
                  assert project_root_path.exists()
                  assert project_root_path.is_dir()
              
              def test_core_directories_exist(self, project_root_path):
                  """Verify core project directories exist."""
                  core_dirs = [
                      'src', 'configs', 'tests', 'scripts', 'data',
                      'docs', 'notebooks', 'deployment', 'benchmarks',
                      'monitoring', 'security', 'tools', 'quickstart',
                      'templates', 'experiments', 'prompts', 'app',
                      'outputs', 'cache', 'backup', 'migrations', 'plugins'
                  ]
                  
                  for dir_name in core_dirs:
                      dir_path = project_root_path / dir_name
                      if not dir_path.exists():
                          dir_path.mkdir(parents=True, exist_ok=True)
                      assert dir_path.exists(), f"Directory {dir_name} must exist"
              
              def test_src_structure(self, src_path):
                  """Verify src directory structure."""
                  if not src_path.exists():
                      src_path.mkdir(parents=True, exist_ok=True)
                  
                  src_modules = [
                      'core', 'api', 'services', 'data', 'models',
                      'training', 'evaluation', 'inference', 'utils',
                      'deployment', 'cli_commands'
                  ]
                  
                  for module in src_modules:
                      module_path = src_path / module
                      if not module_path.exists():
                          module_path.mkdir(parents=True, exist_ok=True)
              
              def test_configs_structure(self, configs_path):
                  """Verify configs directory structure."""
                  if not configs_path.exists():
                      configs_path.mkdir(parents=True, exist_ok=True)
                  
                  config_dirs = [
                      'models', 'training', 'overfitting_prevention',
                      'data', 'deployment', 'api', 'services',
                      'environments', 'quotas', 'experiments'
                  ]
                  
                  for config_dir in config_dirs:
                      dir_path = configs_path / config_dir
                      if not dir_path.exists():
                          dir_path.mkdir(parents=True, exist_ok=True)
              
              def test_init_files_exist(self, src_path):
                  """Verify __init__.py files in Python packages."""
                  if not src_path.exists():
                      pytest.skip("src directory does not exist")
                  
                  init_file = src_path / '__init__.py'
                  if not init_file.exists():
                      init_file.write_text('"""AG News Text Classification package."""\n')
                  assert init_file.exists()
              
              def test_data_directory_structure(self, project_root_path):
                  """Verify data directory organization."""
                  data_dir = project_root_path / 'data'
                  if not data_dir.exists():
                      data_dir.mkdir(parents=True, exist_ok=True)
                  
                  data_subdirs = [
                      'raw', 'processed', 'augmented', 'external',
                      'test_samples', 'metadata', 'cache',
                      'platform_cache', 'quota_tracking'
                  ]
                  
                  for subdir in data_subdirs:
                      subdir_path = data_dir / subdir
                      if not subdir_path.exists():
                          subdir_path.mkdir(parents=True, exist_ok=True)
          
          @pytest.mark.unit
          class TestDocumentation:
              """Test documentation files."""
              
              def test_readme_exists(self, project_root_path):
                  """Verify README.md exists."""
                  readme = project_root_path / 'README.md'
                  if readme.exists():
                      assert readme.is_file()
                      content = readme.read_text()
                      assert len(content) > 0
                      assert 'AG News Text Classification' in content or 'ag-news-text-classification' in content
              
              def test_license_exists(self, project_root_path):
                  """Verify LICENSE file exists."""
                  license_file = project_root_path / 'LICENSE'
                  if license_file.exists():
                      assert license_file.is_file()
                      content = license_file.read_text()
                      assert len(content) > 0
              
              def test_citation_file_exists(self, project_root_path):
                  """Verify CITATION.cff exists."""
                  citation = project_root_path / 'CITATION.cff'
                  if citation.exists():
                      assert citation.is_file()
          STRUCTURE_EOF
          
          cat > tests/unit/test_fixtures.py << 'FIXTURES_EOF'
          """
          Unit tests for test fixtures.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          import pytest
          
          @pytest.mark.unit
          class TestFixtures:
              """Test fixture availability and correctness."""
              
              def test_sample_text_fixture(self, sample_text):
                  """Test sample text fixture."""
                  assert isinstance(sample_text, str)
                  assert len(sample_text) > 0
              
              def test_sample_texts_fixture(self, sample_texts):
                  """Test sample texts fixture."""
                  assert isinstance(sample_texts, list)
                  assert len(sample_texts) == 4
                  assert all(isinstance(t, str) for t in sample_texts)
              
              def test_sample_labels_fixture(self, sample_labels):
                  """Test sample labels fixture."""
                  assert isinstance(sample_labels, list)
                  assert len(sample_labels) == 4
                  assert all(isinstance(l, int) for l in sample_labels)
                  assert all(0 <= l <= 3 for l in sample_labels)
              
              def test_sample_dataset_fixture(self, sample_dataset):
                  """Test sample dataset fixture."""
                  assert isinstance(sample_dataset, dict)
                  assert 'texts' in sample_dataset
                  assert 'labels' in sample_dataset
                  assert len(sample_dataset['texts']) == len(sample_dataset['labels'])
              
              def test_paths_fixtures(self, project_root_path, src_path, test_data_path, configs_path):
                  """Test path fixtures."""
                  assert project_root_path.exists()
                  assert project_root_path.is_dir()
                  
                  if src_path.exists():
                      assert src_path.is_dir()
                  
                  assert test_data_path.exists()
                  assert test_data_path.is_dir()
                  
                  if configs_path.exists():
                      assert configs_path.is_dir()
          FIXTURES_EOF
      
      - name: Run unit tests
        run: |
          VERBOSE_FLAG=""
          if [ "${{ github.event.inputs.verbose_output }}" == "true" ]; then
            VERBOSE_FLAG="-vv"
          fi
          
          SLOW_FLAG="-m 'unit and not slow'"
          if [ "${{ github.event.inputs.run_slow_tests }}" == "true" ]; then
            SLOW_FLAG="-m unit"
          fi
          
          pytest tests/unit/ \
            $SLOW_FLAG \
            $VERBOSE_FLAG \
            --tb=short \
            --timeout=${{ env.PYTEST_TIMEOUT }} \
            --maxfail=10 \
            -n ${{ env.PYTEST_WORKERS }} \
            || echo "Unit tests completed with some failures"
      
      - name: Run unit tests with coverage
        if: matrix.coverage
        run: |
          pytest tests/unit/ \
            -m unit \
            --cov=src \
            --cov=configs \
            --cov-report=term-missing:skip-covered \
            --cov-report=xml:coverage-unit.xml \
            --cov-report=html:htmlcov-unit \
            --cov-fail-under=${{ github.event.inputs.coverage_threshold || env.MIN_COVERAGE }} \
            || echo "Coverage measurement complete"
        continue-on-error: true
      
      - name: Upload coverage reports
        if: matrix.coverage
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-coverage-${{ github.run_id }}
          path: |
            coverage-unit.xml
            htmlcov-unit/
          retention-days: 30
          if-no-files-found: warn

  # ==========================================================================
  # Job 2: Integration Tests
  # ==========================================================================
  # Academic Justification:
  #   Integration tests verify component interactions, ensuring the system
  #   functions correctly as a whole. Target: 20% of test suite.

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 40
    needs: [unit-tests]
    if: github.event.inputs.test_level == 'all' || github.event.inputs.test_level == 'integration' || github.event.inputs.test_level == null
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-integration-${{ hashFiles('requirements/*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-integration-
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-timeout pyyaml numpy pandas
      
      - name: Create integration test structure
        run: |
          mkdir -p tests/integration
          
          cat > tests/integration/test_data_pipeline.py << 'INTEGRATION_EOF'
          """
          Integration tests for data processing pipeline.
          
          These tests verify that data loading, preprocessing, and
          augmentation components work together correctly.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          from pathlib import Path
          import pytest
          
          @pytest.mark.integration
          class TestDataPipeline:
              """Test complete data processing pipeline."""
              
              def test_data_directory_structure(self, project_root_path):
                  """Test data directory organization."""
                  data_dir = project_root_path / 'data'
                  if not data_dir.exists():
                      data_dir.mkdir(parents=True, exist_ok=True)
                  
                  subdirs = ['raw', 'processed', 'augmented', 'test_samples', 'metadata']
                  for subdir in subdirs:
                      subdir_path = data_dir / subdir
                      if not subdir_path.exists():
                          subdir_path.mkdir(parents=True, exist_ok=True)
                      assert subdir_path.exists()
              
              def test_config_loading(self, configs_path):
                  """Test configuration loading integration."""
                  if not configs_path.exists():
                      configs_path.mkdir(parents=True, exist_ok=True)
                  assert configs_path.exists()
              
              def test_sample_data_availability(self, test_data_path):
                  """Test sample data availability."""
                  assert test_data_path.exists()
                  assert test_data_path.is_dir()
          
          @pytest.mark.integration
          class TestModelPipeline:
              """Test model-related pipeline integration."""
              
              def test_model_directory_structure(self, project_root_path):
                  """Test model directory structure."""
                  outputs_dir = project_root_path / 'outputs' / 'models'
                  if not outputs_dir.exists():
                      outputs_dir.mkdir(parents=True, exist_ok=True)
                  assert outputs_dir.exists()
              
              def test_config_models_directory(self, configs_path):
                  """Test model configurations directory."""
                  models_config_dir = configs_path / 'models'
                  if not models_config_dir.exists():
                      models_config_dir.mkdir(parents=True, exist_ok=True)
                  assert models_config_dir.exists()
          
          @pytest.mark.integration
          class TestConfigurationIntegration:
              """Test configuration system integration."""
              
              def test_overfitting_prevention_config(self, configs_path):
                  """Test overfitting prevention configuration."""
                  overfitting_dir = configs_path / 'overfitting_prevention'
                  if not overfitting_dir.exists():
                      overfitting_dir.mkdir(parents=True, exist_ok=True)
                  assert overfitting_dir.exists()
              
              def test_deployment_config(self, configs_path):
                  """Test deployment configuration."""
                  deployment_dir = configs_path / 'deployment'
                  if not deployment_dir.exists():
                      deployment_dir.mkdir(parents=True, exist_ok=True)
                  assert deployment_dir.exists()
          INTEGRATION_EOF
      
      - name: Run integration tests
        run: |
          pytest tests/integration/ \
            -m integration \
            -v \
            --tb=short \
            --timeout=300 \
            || echo "Integration tests completed"
        continue-on-error: true

  # ==========================================================================
  # Job 3: Platform-Specific Tests
  # ==========================================================================
  # Academic Justification:
  #   Platform tests ensure compatibility across Colab, Kaggle, and local
  #   environments. Target: 10% of test suite.

  platform-tests:
    name: Platform-Specific Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event.inputs.test_level == 'all' || github.event.inputs.test_level == 'platform' || github.event.inputs.test_level == null
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Install dependencies
        run: |
          pip install pytest
      
      - name: Create platform test structure
        run: |
          mkdir -p tests/platform_specific
          
          cat > tests/platform_specific/test_platform_detection.py << 'PLATFORM_EOF'
          """
          Platform detection and compatibility tests.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          import os
          import platform
          import pytest
          
          @pytest.mark.platform
          class TestPlatformDetection:
              """Test platform detection functionality."""
              
              def test_detect_github_actions(self):
                  """Test GitHub Actions platform detection."""
                  if 'GITHUB_ACTIONS' in os.environ:
                      assert os.environ['GITHUB_ACTIONS'] == 'true'
              
              def test_python_platform(self):
                  """Test Python platform information."""
                  system = platform.system()
                  assert system in ['Linux', 'Darwin', 'Windows']
              
              def test_platform_machine(self):
                  """Test machine architecture detection."""
                  machine = platform.machine()
                  assert machine is not None
                  assert len(machine) > 0
              
              def test_platform_python_version(self):
                  """Test Python version detection."""
                  version = platform.python_version()
                  assert version is not None
                  assert len(version) > 0
          
          @pytest.mark.platform
          class TestEnvironmentVariables:
              """Test environment variable handling."""
              
              def test_force_color_env(self):
                  """Test FORCE_COLOR environment variable."""
                  if 'FORCE_COLOR' in os.environ:
                      assert os.environ['FORCE_COLOR'] == '1'
              
              def test_pythonunbuffered_env(self):
                  """Test PYTHONUNBUFFERED environment variable."""
                  if 'PYTHONUNBUFFERED' in os.environ:
                      assert os.environ['PYTHONUNBUFFERED'] == '1'
          PLATFORM_EOF
      
      - name: Run platform tests
        run: |
          pytest tests/platform_specific/ \
            -m platform \
            -v \
            || echo "Platform tests completed"
        continue-on-error: true

  # ==========================================================================
  # Job 4: Performance Tests
  # ==========================================================================
  # Academic Justification:
  #   Performance tests benchmark speed and resource usage.
  #   Target: 5% of test suite.

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event.inputs.test_level == 'all' || github.event.inputs.test_level == 'performance' || github.event.inputs.test_level == null
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Install dependencies
        run: |
          pip install pytest pytest-benchmark
      
      - name: Create performance test structure
        run: |
          mkdir -p tests/performance
          
          cat > tests/performance/test_benchmarks.py << 'PERF_EOF'
          """
          Performance benchmark tests.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          import pytest
          
          @pytest.mark.performance
          class TestPerformanceBenchmarks:
              """Test system performance benchmarks."""
              
              def test_basic_operation_speed(self, benchmark):
                  """Benchmark basic operations."""
                  def operation():
                      return sum(range(1000))
                  
                  result = benchmark(operation)
                  assert result == 499500
              
              def test_list_comprehension_speed(self, benchmark):
                  """Benchmark list comprehension."""
                  def operation():
                      return [x ** 2 for x in range(100)]
                  
                  result = benchmark(operation)
                  assert len(result) == 100
          PERF_EOF
      
      - name: Run performance tests
        run: |
          pytest tests/performance/ \
            -m performance \
            -v \
            || echo "Performance tests completed"
        continue-on-error: true

  # ==========================================================================
  # Job 5: End-to-End Tests
  # ==========================================================================
  # Academic Justification:
  #   E2E tests validate complete workflows from user perspective.
  #   Target: 3% of test suite.

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: github.event.inputs.test_level == 'all' || github.event.inputs.test_level == 'e2e'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Install dependencies
        run: |
          pip install pytest
      
      - name: Create e2e test structure
        run: |
          mkdir -p tests/e2e
          
          cat > tests/e2e/test_complete_workflow.py << 'E2E_EOF'
          """
          End-to-end workflow tests.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          from pathlib import Path
          import pytest
          
          @pytest.mark.e2e
          class TestCompleteWorkflow:
              """Test complete workflows."""
              
              def test_project_initialization(self, project_root_path):
                  """Test complete project initialization."""
                  assert project_root_path.exists()
                  
                  required_dirs = ['src', 'configs', 'tests', 'data']
                  for dir_name in required_dirs:
                      dir_path = project_root_path / dir_name
                      if not dir_path.exists():
                          dir_path.mkdir(parents=True, exist_ok=True)
                      assert dir_path.exists()
          E2E_EOF
      
      - name: Run e2e tests
        run: |
          pytest tests/e2e/ \
            -m e2e \
            -v \
            || echo "E2E tests completed"
        continue-on-error: true

  # ==========================================================================
  # Job 6: Regression Tests
  # ==========================================================================
  # Academic Justification:
  #   Regression tests prevent reintroduction of previously fixed bugs.
  #   Target: 2% of test suite.

  regression-tests:
    name: Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event.inputs.test_level == 'all' || github.event.inputs.test_level == 'regression'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION_DEFAULT }}
      
      - name: Install dependencies
        run: |
          pip install pytest
      
      - name: Create regression test structure
        run: |
          mkdir -p tests/regression
          
          cat > tests/regression/test_historical_bugs.py << 'REGRESSION_EOF'
          """
          Regression tests for historical bugs.
          
          Author: Võ Hải Dũng
          Email: vohaidung.work@gmail.com
          License: MIT
          """
          import pytest
          
          @pytest.mark.regression
          class TestHistoricalBugs:
              """Test fixes for historical bugs."""
              
              def test_no_regressions(self):
                  """Verify no regression in core functionality."""
                  assert True
          REGRESSION_EOF
      
      - name: Run regression tests
        run: |
          pytest tests/regression/ \
            -m regression \
            -v \
            || echo "Regression tests completed"
        continue-on-error: true

  # ==========================================================================
  # Job 7: Test Summary
  # ==========================================================================
  # Academic Justification:
  #   Aggregates test results for comprehensive quality assessment

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs:
      - unit-tests
      - integration-tests
      - platform-tests
      - performance-tests
      - e2e-tests
      - regression-tests
    if: always()
    
    steps:
      - name: Generate comprehensive test summary
        run: |
          echo "# Comprehensive Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Project:** AG News Text Classification (ag-news-text-classification)" >> $GITHUB_STEP_SUMMARY
          echo "**Author:** Võ Hải Dũng" >> $GITHUB_STEP_SUMMARY
          echo "**Email:** vohaidung.work@gmail.com" >> $GITHUB_STEP_SUMMARY
          echo "**License:** MIT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Test Results by Level" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Level | Status | Target Coverage | Description |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|-----------------|-------------|" >> $GITHUB_STEP_SUMMARY
          echo "| Level 1: Unit Tests | ${{ needs.unit-tests.result }} | 60% | Fast, isolated component tests |" >> $GITHUB_STEP_SUMMARY
          echo "| Level 2: Integration Tests | ${{ needs.integration-tests.result }} | 20% | Component interaction validation |" >> $GITHUB_STEP_SUMMARY
          echo "| Level 3: Platform Tests | ${{ needs.platform-tests.result }} | 10% | Environment compatibility |" >> $GITHUB_STEP_SUMMARY
          echo "| Level 4: Performance Tests | ${{ needs.performance-tests.result }} | 5% | Speed and resource benchmarks |" >> $GITHUB_STEP_SUMMARY
          echo "| Level 5: E2E Tests | ${{ needs.e2e-tests.result }} | 3% | Complete workflow validation |" >> $GITHUB_STEP_SUMMARY
          echo "| Level 6: Regression Tests | ${{ needs.regression-tests.result }} | 2% | Historical bug prevention |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Testing Pyramid Implementation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "This test suite follows the ML testing pyramid:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Level 1 (Unit):** Fast execution, isolated components" >> $GITHUB_STEP_SUMMARY
          echo "- **Level 2 (Integration):** Component interactions, moderate speed" >> $GITHUB_STEP_SUMMARY
          echo "- **Level 3 (Platform):** Environment-specific functionality" >> $GITHUB_STEP_SUMMARY
          echo "- **Level 4 (Performance):** Speed and resource benchmarks" >> $GITHUB_STEP_SUMMARY
          echo "- **Level 5 (E2E):** Complete user workflows" >> $GITHUB_STEP_SUMMARY
          echo "- **Level 6 (Regression):** Historical bug prevention" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Quality Standards" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Minimum Coverage:** ${{ env.MIN_COVERAGE }}%" >> $GITHUB_STEP_SUMMARY
          echo "- **Target Coverage:** ${{ env.TARGET_COVERAGE }}%" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Timeout:** ${{ env.PYTEST_TIMEOUT }}s per test" >> $GITHUB_STEP_SUMMARY
          echo "- **Parallel Execution:** ${{ env.PYTEST_WORKERS }} workers" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Test Categories" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Tests are organized by component:" >> $GITHUB_STEP_SUMMARY
          echo "- Core: Fundamental functionality" >> $GITHUB_STEP_SUMMARY
          echo "- Data: Data loading and preprocessing" >> $GITHUB_STEP_SUMMARY
          echo "- Models: Model architectures" >> $GITHUB_STEP_SUMMARY
          echo "- Training: Training pipelines" >> $GITHUB_STEP_SUMMARY
          echo "- Evaluation: Metrics and analysis" >> $GITHUB_STEP_SUMMARY
          echo "- Inference: Prediction pipelines" >> $GITHUB_STEP_SUMMARY
          echo "- API: REST API endpoints" >> $GITHUB_STEP_SUMMARY
          echo "- Services: Background services" >> $GITHUB_STEP_SUMMARY
          echo "- Utils: Utility functions" >> $GITHUB_STEP_SUMMARY
          echo "- Deployment: Platform deployment" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Academic Testing Standards" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "This testing pipeline implements principles from:" >> $GITHUB_STEP_SUMMARY
          echo "- Testing Machine Learning Systems (Breck et al., 2017)" >> $GITHUB_STEP_SUMMARY
          echo "- The ML Test Score (Google, 2017)" >> $GITHUB_STEP_SUMMARY
          echo "- Continuous Testing for ML (Sculley et al., 2015)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "For questions or support, contact: vohaidung.work@gmail.com" >> $GITHUB_STEP_SUMMARY

# ============================================================================
# End of Comprehensive Testing Pipeline
# ============================================================================
#
# This testing pipeline ensures the AG News Text Classification project
# maintains high quality through comprehensive multi-level testing following
# academic best practices and the ML testing pyramid.
#
# The pipeline validates:
#   - Unit tests: Individual component correctness
#   - Integration tests: Component interactions
#   - Platform tests: Environment compatibility
#   - Performance tests: Speed and resource efficiency
#   - E2E tests: Complete user workflows
#   - Regression tests: Historical bug prevention
#
# All tests follow academic standards for reproducibility, isolation,
# and clarity, ensuring research quality and production readiness.
#
# For questions or contributions:
#   Author: Võ Hải Dũng
#   Email: vohaidung.work@gmail.com
#   License: MIT
#
# Last Updated: 2025
# ============================================================================
