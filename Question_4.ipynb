{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Train and Evaluate a Keras-Based Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and parameters\n",
    "dataset_path = './images_dataSAT/'\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "INPUT_SHAPE = (64, 64, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Recursively walk through the dataset_path using the os.walk function to create a list fnames of all image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Create list of all image file names using os.walk\n",
    "fnames = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
    "    for filename in filenames:\n",
    "        # Filter for common image file extensions\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff', '.gif')):\n",
    "            full_path = os.path.join(dirpath, filename)\n",
    "            fnames.append(full_path)\n",
    "\n",
    "# Sort the list for consistency\n",
    "fnames = sorted(fnames)\n",
    "\n",
    "print(f\"Total number of image files found: {len(fnames)}\")\n",
    "print(f\"\\nFirst 5 image paths:\")\n",
    "for f in fnames[:5]:\n",
    "    print(f\"  {f}\")\n",
    "print(f\"\\nLast 5 image paths:\")\n",
    "for f in fnames[-5:]:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ImageDataGenerator with augmentation for training and rescaling for validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # 20% for validation\n",
    ")\n",
    "\n",
    "# Create training generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {train_generator.samples}\")\n",
    "print(f\"Class indices: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create the validation_generator from dataset_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Create validation_generator\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nValidation samples: {validation_generator.samples}\")\n",
    "print(f\"Class indices: {validation_generator.class_indices}\")\n",
    "print(f\"Number of validation batches: {len(validation_generator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a basic CNN model first to count layers\n",
    "model = Sequential([\n",
    "    # Conv Block 1\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=INPUT_SHAPE),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Conv Block 2\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Conv Block 3\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten\n",
    "    Flatten(),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Count the total number of layers in this CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Count total number of layers\n",
    "total_layers = len(model.layers)\n",
    "\n",
    "print(f\"Total number of layers in the CNN model: {total_layers}\")\n",
    "print(f\"\\nLayer details:\")\n",
    "print(\"=\" * 50)\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"Layer {i+1}: {layer.name} ({layer.__class__.__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create and compile a CNN model test_model with four Conv2D layers and five Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Create test_model with 4 Conv2D layers and 5 Dense layers\n",
    "test_model = Sequential([\n",
    "    # Conv2D Layer 1\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=INPUT_SHAPE),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Conv2D Layer 2\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Conv2D Layer 3\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Conv2D Layer 4\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten\n",
    "    Flatten(),\n",
    "    \n",
    "    # Dense Layer 1\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Dense Layer 2\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # Dense Layer 3\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Dense Layer 4\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Dense Layer 5 (Output)\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "test_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "test_model.summary()\n",
    "\n",
    "# Count Conv2D and Dense layers\n",
    "conv_count = sum(1 for layer in test_model.layers if isinstance(layer, Conv2D))\n",
    "dense_count = sum(1 for layer in test_model.layers if isinstance(layer, Dense))\n",
    "print(f\"\\nNumber of Conv2D layers: {conv_count}\")\n",
    "print(f\"Number of Dense layers: {dense_count}\")\n",
    "print(f\"Total number of layers: {len(test_model.layers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Create the checkpoint callback for the model with maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Create checkpoint callback for maximum accuracy\n",
    "checkpoint_path = 'best_model.keras'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Checkpoint callback created successfully.\")\n",
    "print(f\"  Filepath: {checkpoint_path}\")\n",
    "print(f\"  Monitor: val_accuracy\")\n",
    "print(f\"  Mode: max\")\n",
    "print(f\"  Save best only: True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = test_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Plot the graph for training loss and validation loss for the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Plot training loss and validation loss\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss vs Validation Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0].set_title('Training Loss vs Validation Loss', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Accuracy vs Validation Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
    "axes[1].set_title('Training Accuracy vs Validation Accuracy', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Training History', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"\\nFinal Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"\\nBest Validation Accuracy: {max(history.history['val_accuracy']):.4f} (Epoch {np.argmax(history.history['val_accuracy'])+1})\")\n",
    "print(f\"Lowest Validation Loss: {min(history.history['val_loss']):.4f} (Epoch {np.argmin(history.history['val_loss'])+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## All 6 tasks completed successfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
