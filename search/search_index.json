{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AG News Text Classification","text":"[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE) [![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/) [![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/) [![Transformers](https://img.shields.io/badge/\ud83e\udd17_Transformers-4.30+-yellow.svg)](https://huggingface.co/transformers/) [![arXiv](https://img.shields.io/badge/arXiv-2025.xxxxx-b31b1b.svg)](https://arxiv.org/) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)"},{"location":"#introduction","title":"Introduction","text":""},{"location":"#11-research-motivation-and-context","title":"1.1 Research Motivation and Context","text":"<p>Text classification constitutes one of the fundamental tasks in natural language processing, serving as a cornerstone for applications ranging from sentiment analysis and spam detection to news categorization and intent recognition. The task is formally defined as learning a mapping function from a document space to a discrete set of predefined categories, where the objective is to minimize prediction error on unseen examples drawn from the same underlying distribution as the training data.</p> <p>Over the past decade, the field has witnessed a paradigm shift from feature-engineering approaches based on bag-of-words representations and classical machine learning algorithms, to end-to-end neural architectures that learn hierarchical representations directly from raw text. The introduction of attention mechanisms and pre-trained transformer models has further revolutionized the landscape, enabling competitive accuracy on benchmark datasets through transfer learning from massive unsupervised corpora.</p> <p>Despite these advances, a fundamental tension persists between the capacity of modern neural architectures and the size of available labeled datasets. Contemporary state-of-the-art models such as BERT, RoBERTa, DeBERTa, and large language models like LLaMA and Mistral contain parameters numbering in the hundreds of millions to tens of billions, while supervised classification datasets typically provide only thousands to hundreds of thousands of labeled examples. This disparity creates a severe risk of overfitting, where models achieve near-perfect accuracy on training data yet fail to generalize to held-out test sets.</p> <p>This project addresses the challenge of developing text classification systems that achieve competitive accuracy while maintaining rigorous generalization guarantees. We focus specifically on the AG News dataset as an experimental testbed\u2014not because it represents the frontier of difficulty in modern NLP, but precisely because its moderate size and balanced structure provide an ideal controlled environment for studying the interplay between model capacity, training methodology, and generalization performance. The complete dataset characteristics and experimental protocols are detailed in Dataset.</p>"},{"location":"#12-the-generalization-challenge-in-modern-nlp","title":"1.2 The Generalization Challenge in Modern NLP","text":"<p>The core theoretical challenge in supervised machine learning is the minimization of expected risk, defined over an unknown data distribution. Let us denote the input space of text documents as \\(\\mathcal{X}\\) and the output space of class labels as \\(\\mathcal{Y}\\). Given a training dataset \\(\\mathcal{D}\\) consisting of \\(N\\) independently and identically distributed samples:</p> \\[\\mathcal{D} = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N)\\}\\] <p>where each pair \\((x_i, y_i)\\) is drawn from an unknown joint distribution \\(P\\) over \\(\\mathcal{X} \\times \\mathcal{Y}\\), the learning objective is to find a function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\) that minimizes the expected risk:</p> \\[R(f) = \\mathbb{E}_{(x,y) \\sim P}[\\ell(f(x), y)]\\] <p>Here, \\(\\ell\\) denotes a loss function quantifying the penalty for incorrect predictions. For classification tasks, this is commonly the zero-one loss, which equals one when the prediction differs from the true label and zero otherwise.</p> <p>Since the true distribution \\(P\\) is unknown and inaccessible, practical learning algorithms instead minimize the empirical risk computed on the observed training data:</p> \\[R_{\\text{emp}}(f) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(f(x_i), y_i)\\] <p>The fundamental question is whether a function \\(f\\) that achieves low empirical risk will also achieve low expected risk on new, unseen examples. The difference between these two quantities is termed the generalization gap:</p> \\[\\Delta(f) = R(f) - R_{\\text{emp}}(f)\\] <p>This generalization gap represents the core challenge addressed throughout this framework. When \\(\\Delta(f)\\) is large, the model has overfit to the training data and will perform poorly on new examples despite excellent training accuracy.</p> <p>Statistical learning theory provides bounds on this generalization gap. According to the Vapnik-Chervonenkis theory, with probability at least \\(1 - \\delta\\), the true risk is bounded by:</p> \\[R(f) \\leq R_{\\text{emp}}(f) + \\sqrt{\\frac{d \\log(2N/d) + \\log(4/\\delta)}{N}}\\] <p>where \\(d\\) represents the VC dimension, a measure of the model's capacity or expressiveness. </p> <p>Interpreting this bound: The inequality reveals a fundamental trade-off. The first term \\(R_{\\text{emp}}(f)\\) decreases as we use more expressive models with higher capacity (larger \\(d\\)), since such models can fit the training data better. However, the second term\u2014the generalization gap\u2014increases with model capacity \\(d\\) and decreases with dataset size \\(N\\). Models with high capacity can achieve low empirical risk but may suffer from a large generalization gap. Conversely, models with limited capacity have tighter generalization bounds but may be unable to capture the true underlying patterns, leading to high bias. The optimal model complexity minimizes the sum of both terms.</p> <p>In the context of modern transformer-based language models, the effective capacity is enormous. For instance, DeBERTa-v3-XLarge contains approximately 710 million parameters, while the AG News training set provides 120,000 labeled examples. This yields a parameter-to-sample ratio of approximately 5,917:1, meaning each parameter is informed by fewer than 0.0002 training samples on average. Classical statistical learning theory would suggest that such models should catastrophically overfit, memorizing training examples without learning generalizable patterns.</p> <p>However, empirical practice demonstrates that careful application of several techniques can mitigate this overfitting risk:</p> <ul> <li> <p>Transfer Learning: Pre-training on large unsupervised corpora allows models to learn general linguistic representations before fine-tuning on task-specific data. This effectively reduces the number of parameters that must be learned from the supervised dataset alone, as discussed in Section 1.6.2.</p> </li> <li> <p>Parameter-Efficient Fine-Tuning: Methods such as Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) constrain the fine-tuning process to modify only a small subset of model parameters, dramatically reducing effective capacity. Theoretical foundations are provided in Section 1.6.3.</p> </li> <li> <p>Regularization: Techniques including dropout, weight decay, and early stopping impose explicit or implicit penalties on model complexity, encouraging simpler solutions that generalize better. Our regularization strategies are detailed in configs/training/regularization/.</p> </li> <li> <p>Ensemble Methods: Combining predictions from multiple diverse models reduces variance and improves robustness, even when individual models may overfit. The theoretical basis is explained in Section 1.6.4.</p> </li> <li> <p>Knowledge Distillation: Training smaller student models to mimic the behavior of larger teacher models or ensembles preserves much of the performance gain while reducing inference cost and overfitting risk, as detailed in Section 1.6.5.</p> </li> </ul> <p>The challenge addressed by this project is to systematically combine these techniques within a unified framework that treats overfitting prevention not as an afterthought, but as a primary architectural consideration from the outset. The complete overfitting prevention architecture is documented in OVERFITTING_PREVENTION.md.</p>"},{"location":"#13-bridging-theory-and-practice","title":"1.3 Bridging Theory and Practice","text":"<p>A significant gap exists in the current landscape between theoretical understanding of generalization and practical implementation of text classification systems. Research papers often present novel architectures or training techniques with impressive benchmark results, yet the code repositories accompanying these papers frequently lack the infrastructure necessary to ensure that these results are reliable, reproducible, and generalizable.</p> <p>Common issues include:</p> <ul> <li> <p>Inadequate Validation Protocols: Using a single train-test split without proper validation set management, leading to potential overfitting to the validation set through repeated hyperparameter tuning.</p> </li> <li> <p>Test Set Contamination: Inadvertent exposure of test set information during development, such as through exploratory data analysis, error analysis on test samples, or repeated evaluation during model selection.</p> </li> <li> <p>Unreported Hyperparameter Sensitivity: Publishing results from a single random seed or configuration without acknowledging variance across runs, making results difficult to reproduce.</p> </li> <li> <p>Platform-Specific Assumptions: Code that assumes access to specific computational infrastructure (multi-GPU clusters, high-memory machines, fast storage) that is not available to many researchers and practitioners.</p> </li> <li> <p>Lack of Systematic Overfitting Detection: Relying on manual inspection of learning curves rather than automated monitoring systems that can detect subtle signs of overfitting early in training.</p> </li> </ul> <p>This project seeks to bridge this gap by providing not merely a collection of high-performing models, but a complete experimental framework that embeds best practices for generalization into every component of the system. The overfitting prevention mechanisms described in detail in OVERFITTING_PREVENTION.md are not separate modules to be optionally enabled, but rather integral parts of the training pipeline that operate by default.</p> <p>Furthermore, we recognize that reproducibility requires more than fixing random seeds. It requires comprehensive logging of the complete experimental environment, including hardware specifications, software versions, data preprocessing steps, and the full hyperparameter configuration. It requires statistical rigor in comparing models, using multiple random seeds and appropriate significance testing. And it requires transparent reporting of all results, including negative results and failed experiments, to provide an honest assessment of what approaches work and under what conditions.</p> <p>Our implementation of these principles includes:</p> <ul> <li> <p>Automated Environment Logging: Capture of hardware specifications, CUDA versions, library versions, and platform characteristics. See src/utils/experiment_tracking.py.</p> </li> <li> <p>Multi-Seed Evaluation: All reported results average over at least 3-5 random seeds with standard deviations. Configuration templates in configs/experiments/reproducibility/.</p> </li> <li> <p>Statistical Significance Testing: Paired t-tests with Bonferroni correction for comparing multiple models. Implementation in src/evaluation/metrics/.</p> </li> <li> <p>Comprehensive Experiment Tracking: Integration with TensorBoard, MLflow, and Weights &amp; Biases for complete audit trails. Setup guides in LOCAL_MONITORING_GUIDE.md.</p> </li> </ul>"},{"location":"#14-research-gaps-and-motivations","title":"1.4 Research Gaps and Motivations","text":"<p>Through extensive review of existing text classification implementations and research codebases, we have identified several specific gaps that motivated the design decisions in this project.</p>"},{"location":"#141-post-hoc-versus-preventive-overfitting-management","title":"1.4.1 Post-Hoc Versus Preventive Overfitting Management","text":"<p>Current Practice: The typical workflow in developing text classification systems treats overfitting as a problem to be diagnosed after it occurs. Researchers train models, observe divergence between training and validation metrics, and then apply corrective measures such as increasing regularization strength, reducing model capacity, or implementing early stopping.</p> <p>Limitation: This reactive approach has several drawbacks. First, it wastes computational resources by allowing training to proceed even when the configuration is almost certain to overfit. Second, it creates opportunities for inadvertent test set leakage, as researchers may be tempted to check test performance to gauge whether their overfitting mitigation strategies are working. Third, it lacks systematic principles for determining appropriate hyperparameter values, leading to ad-hoc choices that may not generalize across different datasets or model architectures.</p> <p>Our Approach: We implement a preventive overfitting management system that operates at multiple stages:</p> <ol> <li> <p>Pre-Training Validation: Before any GPU computation begins, the system analyzes the proposed configuration against a set of constraint rules based on dataset size, model capacity, and historical performance patterns. Configurations likely to result in severe overfitting are rejected with explanatory messages and recommended alternatives. Implementation in src/core/overfitting_prevention/validators/.</p> </li> <li> <p>Real-Time Monitoring: During training, multiple metrics are tracked to detect early warning signs of overfitting, including the train-validation gap in both loss and accuracy, the rate of change in these metrics, gradient magnitudes, and parameter norms. When predefined thresholds are exceeded, training can be automatically halted with diagnostic information. See src/core/overfitting_prevention/monitors/.</p> </li> <li> <p>Post-Training Analysis: After training completes, comprehensive overfitting risk scores are computed based on multiple factors, and detailed reports are generated comparing the observed behavior to expected patterns for the given configuration. Report generation in src/core/overfitting_prevention/reporting/.</p> </li> </ol> <p>The theoretical foundations and implementation details of this system are described in OVERFITTING_PREVENTION.md, while this document focuses on the high-level rationale and integration with the overall experimental workflow.</p>"},{"location":"#142-fragmented-state-of-the-art-techniques","title":"1.4.2 Fragmented State-of-the-Art Techniques","text":"<p>Current Practice: Achieving competitive results on text classification benchmarks increasingly requires combining multiple advanced techniques: large pre-trained transformers, parameter-efficient fine-tuning methods, ensemble approaches, and knowledge distillation. However, these techniques are often developed and published in separate research efforts, with implementations residing in different codebases using incompatible interfaces.</p> <p>Limitation: A researcher wishing to reproduce state-of-the-art results must manually integrate code from multiple sources, resolve dependency conflicts, adapt to different configuration formats, and navigate undocumented assumptions about data formats and training procedures. This integration burden is particularly challenging for researchers who are domain experts but not deep learning engineers.</p> <p>Our Approach: We provide a unified model zoo spanning classical baselines to large language models, with consistent interfaces and composable configurations. The system is organized into tiers that represent different points in the accuracy-efficiency trade-off space:</p> <ul> <li> <p>Tier 1 - Classical Baselines: Traditional machine learning approaches including Naive Bayes, Support Vector Machines, and Logistic Regression, serving as sanity checks and computational efficiency baselines. Implementations in experiments/baselines/classical/.</p> </li> <li> <p>Tier 2 - Standard Transformers: Full fine-tuning of models like BERT-Base and RoBERTa-Base, establishing the performance ceiling for conventional approaches. Configurations in configs/models/single/transformers/.</p> </li> <li> <p>Tier 3 - Large Transformers with PEFT: Models such as DeBERTa-v3-XLarge and DeBERTa-v2-XXLarge fine-tuned using Low-Rank Adaptation, demonstrating parameter-efficient scaling to larger architectures. Configurations in configs/models/recommended/tier_1_sota/.</p> </li> <li> <p>Tier 4 - Large Language Models: Instruction-tuned models like LLaMA 2 and Mistral fine-tuned with Quantized LoRA, representing the current frontier in transfer learning. Configurations in configs/models/recommended/tier_2_llm/.</p> </li> <li> <p>Tier 5 - Ensembles and Distillation: Multi-model ensembles achieving maximum accuracy, and distilled student models that compress ensemble knowledge into efficient single-model architectures. Configurations in configs/models/recommended/tier_3_ensemble/ and configs/models/recommended/tier_4_distilled/.</p> </li> </ul> <p>Detailed guidance on selecting appropriate models for different use cases is provided in SOTA_MODELS_GUIDE.md, while the system architecture enabling this modularity is documented in ARCHITECTURE.md.</p>"},{"location":"#143-platform-dependence-and-accessibility-barriers","title":"1.4.3 Platform Dependence and Accessibility Barriers","text":"<p>Current Practice: Much academic research in NLP assumes access to substantial computational resources, such as multi-GPU clusters with high-bandwidth interconnects, large amounts of RAM, and fast persistent storage. Code is often optimized for specific environments like institutional SLURM clusters or cloud platforms with particular instance types.</p> <p>Limitation: This creates significant barriers for independent researchers, students, and practitioners in resource-constrained settings. Even when pre-trained models are publicly available, fine-tuning them on custom datasets may be infeasible without access to expensive infrastructure. Furthermore, differences in hardware and software configurations across platforms frequently lead to reproducibility failures, where code that runs successfully in one environment produces errors or different results in another.</p> <p>Our Approach: We adopt a platform-agnostic design that automatically adapts to available computational resources:</p> <ol> <li> <p>Automatic Platform Detection: The system identifies the execution environment at runtime (Google Colab, Kaggle Notebooks, local CPU, local GPU, etc.) and loads appropriate configuration defaults. Implementation in src/deployment/platform_detector.py.</p> </li> <li> <p>Resource-Aware Model Selection: Based on detected GPU memory, CPU count, and time quotas, the system recommends models and training configurations that will complete within available resources. Decision logic in src/deployment/smart_selector.py.</p> </li> <li> <p>Checkpoint Resilience: Training state is periodically synchronized to persistent storage (Google Drive for Colab, Kaggle Datasets for Kaggle) so that sessions interrupted due to platform time limits can be seamlessly resumed. Implementation in src/deployment/checkpoint_manager.py.</p> </li> <li> <p>Quota Management: For platforms with usage limits (Colab's 12-hour sessions, Kaggle's weekly GPU quotas), the system tracks consumption and optimizes checkpoint intervals to maximize effective training time. Quota tracking in src/deployment/quota_tracker.py.</p> </li> </ol> <p>The platform adaptation mechanisms are detailed in PLATFORM_OPTIMIZATION_GUIDE.md, while platform-specific configurations are available in configs/environments/ and configs/training/platform_adaptive/.</p>"},{"location":"#144-test-set-integrity-and-experimental-validity","title":"1.4.4 Test Set Integrity and Experimental Validity","text":"<p>Current Practice: In many machine learning projects, the test set is treated as merely another data split, stored in the same directory structure as training and validation data and accessible to all code components. Researchers manually ensure that test data is not used during development, relying on discipline rather than technical safeguards.</p> <p>Limitation: Human error and the iterative nature of machine learning experimentation make test set leakage a persistent risk. Subtle forms of leakage can occur through:</p> <ul> <li> <p>Direct leakage: Accidentally including test samples in training batches due to indexing errors or incorrect data split logic.</p> </li> <li> <p>Indirect leakage: Making modeling decisions (feature selection, architecture choices, hyperparameter ranges) based on test set characteristics observed during exploratory analysis.</p> </li> <li> <p>Adaptive leakage: Trying multiple models or configurations and selecting based on test performance, effectively overfitting to the test set through the model selection process itself.</p> </li> <li> <p>Preprocessing leakage: Computing normalization statistics, vocabulary mappings, or other preprocessing parameters on the full dataset including test samples, then applying these to create test features.</p> </li> </ul> <p>Our Approach: We implement cryptographic test set protection mechanisms:</p> <ol> <li> <p>Hash-Based Integrity: Upon first loading the test set, a SHA-256 hash is computed over the sorted sample identifiers and stored in a protected file at data/processed/.test_set_hash. Every subsequent test evaluation verifies that the hash matches, detecting any modification or corruption of the test set. Implementation in src/core/overfitting_prevention/utils/hash_utils.py.</p> </li> <li> <p>Access Logging: All code paths that access test data are instrumented to log the timestamp, calling function, purpose statement, and stack trace to data/test_access_log.json. This creates an auditable record of test set usage. Implementation in src/core/overfitting_prevention/guards/test_set_guard.py.</p> </li> <li> <p>Access Control: The test set loader requires explicit authorization through configuration flags. By default, attempting to load test data during training or hyperparameter tuning raises an exception. Guard implementation in src/core/overfitting_prevention/guards/access_control.py.</p> </li> <li> <p>Budget Enforcement: A configurable limit on the number of test evaluations prevents adaptive overfitting through repeated model selection based on test performance. Configuration in configs/overfitting_prevention/validation/test_set_protection.yaml.</p> </li> </ol> <p>These mechanisms, described in detail in OVERFITTING_PREVENTION.md \u00a7 Test Set Protection, provide technical enforcement of best practices that would otherwise rely solely on researcher discipline.</p>"},{"location":"#15-core-contributions","title":"1.5 Core Contributions","text":"<p>This project makes several specific contributions to the practice of text classification:</p>"},{"location":"#1-comprehensive-overfitting-prevention-framework","title":"1. Comprehensive Overfitting Prevention Framework","text":"<p>We provide a multi-layered system for preventing, detecting, and diagnosing overfitting that operates throughout the experimental lifecycle. This includes pre-training configuration validation, real-time monitoring during training, post-training risk assessment, and test set protection mechanisms. The system is designed to be both technically rigorous\u2014implementing ideas from statistical learning theory and adaptive data analysis\u2014and practically usable, with clear error messages and actionable recommendations.</p> <p>Key components:</p> <ul> <li>Pre-training validators in src/core/overfitting_prevention/validators/</li> <li>Real-time monitors in src/core/overfitting_prevention/monitors/</li> <li>Constraint enforcers in src/core/overfitting_prevention/constraints/</li> <li>Test set guards in src/core/overfitting_prevention/guards/</li> <li>Automated recommenders in src/core/overfitting_prevention/recommendations/</li> </ul> <p>Complete documentation in OVERFITTING_PREVENTION.md.</p>"},{"location":"#2-unified-parameter-efficient-fine-tuning-infrastructure","title":"2. Unified Parameter-Efficient Fine-Tuning Infrastructure","text":"<p>We integrate multiple PEFT methods (LoRA, QLoRA, Adapters, Prefix Tuning, Prompt Tuning) within a consistent interface, enabling fair comparisons and hybrid approaches. Extensive hyperparameter search results for LoRA rank selection, target module choices, and regularization strategies are provided to guide users toward configurations that balance accuracy and efficiency for the AG News dataset.</p> <p>Implementations:</p> <ul> <li>LoRA: src/models/efficient/lora/</li> <li>QLoRA: src/models/efficient/qlora/</li> <li>Adapters: src/models/efficient/adapters/</li> <li>Prefix Tuning: src/models/efficient/prefix_tuning/</li> <li>Prompt Tuning: src/models/efficient/prompt_tuning/</li> </ul> <p>Configuration templates in configs/training/efficient/ and ablation studies in experiments/ablation_studies/.</p>"},{"location":"#3-multi-stage-sota-pipeline","title":"3. Multi-Stage SOTA Pipeline","text":"<p>We demonstrate a systematic progression from individual high-capacity models through ensemble aggregation to knowledge distillation, achieving competitive accuracy while maintaining interpretability and deployability. Each stage is fully reproducible with provided configurations and scripts, and ablation studies isolate the contribution of each component.</p> <p>Pipeline stages:</p> <ul> <li>Phase 1: XLarge model training with LoRA - experiments/sota_experiments/phase1_xlarge_lora.py</li> <li>Phase 2: LLM fine-tuning with QLoRA - experiments/sota_experiments/phase2_llm_qlora.py</li> <li>Phase 3: Knowledge distillation - experiments/sota_experiments/phase3_llm_distillation.py</li> <li>Phase 4: Ensemble aggregation - experiments/sota_experiments/phase4_ensemble_xlarge.py</li> <li>Phase 5: Ultimate SOTA - experiments/sota_experiments/phase5_ultimate_sota.py</li> </ul> <p>Complete pipeline guide in SOTA_MODELS_GUIDE.md.</p>"},{"location":"#4-platform-agnostic-reproducibility","title":"4. Platform-Agnostic Reproducibility","text":"<p>Through automatic platform detection, resource-aware configuration selection, and comprehensive environment logging, we ensure that experiments can be reproduced across diverse computational environments. This includes consumer laptops, cloud platforms (Google Colab, Kaggle Notebooks), and institutional compute clusters.</p> <p>Platform support:</p> <ul> <li>Detection system: src/deployment/platform_detector.py</li> <li>Smart configuration selection: src/deployment/smart_selector.py</li> <li>Platform-specific configs: configs/environments/</li> <li>Colab optimization: configs/training/platform_adaptive/colab_free_training.yaml</li> <li>Kaggle optimization: configs/training/platform_adaptive/kaggle_gpu_training.yaml</li> </ul> <p>Platform guide in PLATFORM_OPTIMIZATION_GUIDE.md.</p>"},{"location":"#5-extensive-documentation-and-educational-resources","title":"5. Extensive Documentation and Educational Resources","text":"<p>Beyond this technical README, we provide detailed guides targeted at different user expertise levels:</p> <ul> <li>Beginners: Step-by-step tutorials covering basic concepts and common workflows in docs/level_1_beginner/</li> <li>Practitioners: Best practice guides for model selection, hyperparameter tuning, and deployment in docs/best_practices/</li> <li>Researchers: In-depth theoretical treatments of overfitting prevention, ensemble methods, and distillation in OVERFITTING_PREVENTION.md and ARCHITECTURE.md</li> <li>Developers: API documentation and architecture descriptions for extending the codebase in docs/developer_guide/ and docs/api_reference/</li> </ul> <p>This progressive disclosure approach allows users to engage with the system at a level appropriate to their background and goals. Complete navigation guide in Section 1.8.</p>"},{"location":"#16-theoretical-foundations-overview","title":"1.6 Theoretical Foundations: Overview","text":"<p>This section provides a high-level overview of the theoretical principles underlying our implementation. Detailed mathematical treatments, proofs, and empirical validations are provided in specialized documentation referenced below.</p>"},{"location":"#161-statistical-learning-theory-and-capacity-control","title":"1.6.1 Statistical Learning Theory and Capacity Control","text":"<p>The fundamental question in supervised learning is how well a model trained on finite data will perform on new examples. Statistical learning theory, particularly the Vapnik-Chervonenkis framework, provides rigorous bounds on generalization error as a function of model capacity and sample size.</p> <p>For a hypothesis space with VC dimension \\(d\\), the generalization bound states that with probability at least \\(1 - \\delta\\), the true risk \\(R(f)\\) of any hypothesis \\(f\\) is bounded by:</p> \\[R(f) \\leq R_{\\text{emp}}(f) + \\sqrt{\\frac{d \\log(2N/d) + \\log(4/\\delta)}{N}}\\] <p>where:</p> <ul> <li>\\(R_{\\text{emp}}(f)\\) is the empirical risk (average loss on training data)</li> <li>\\(N\\) is the number of training samples</li> <li>\\(d\\) is the VC dimension (a measure of model complexity)</li> <li>\\(\\delta\\) is the confidence parameter</li> </ul> <p>Interpretation: This bound consists of two terms. The first term, empirical risk, can be made arbitrarily small by using sufficiently complex models that fit the training data well. However, the second term\u2014the generalization gap\u2014grows with model capacity (\\(d\\)) and shrinks with dataset size (\\(N\\)). The optimal model complexity minimizes the sum of these two terms.</p> <p>More intuitively, the bound tells us that as we increase model complexity \\(d\\), we can fit the training data better (reducing \\(R_{\\text{emp}}(f)\\)), but the gap between training and true performance grows (the second term increases). Conversely, with more training data \\(N\\), we can safely use more complex models since the generalization gap shrinks proportionally to \\(1/\\sqrt{N}\\).</p> <p>Practical Implications for This Project:</p> <ol> <li> <p>Parameter-Efficient Fine-Tuning: Methods like LoRA reduce the effective VC dimension by constraining the fine-tuning process to low-rank subspaces. For a transformer with \\(d_{\\text{model}}\\) hidden dimensions and rank \\(r \\ll d_{\\text{model}}\\), LoRA updates have far fewer degrees of freedom than full fine-tuning, leading to tighter generalization bounds. Implementation in src/models/efficient/lora/.</p> </li> <li> <p>Early Stopping: Rather than training until convergence on the training set, we halt training when validation performance plateaus. This implicitly limits effective capacity by restricting the number of gradient updates. Callback implementation in src/training/callbacks/early_stopping.py.</p> </li> <li> <p>Ensemble Methods: While individual models may have high capacity, ensemble averaging acts as a form of regularization. The variance reduction achieved through ensembling can be formalized through bias-variance decomposition as shown in Section 1.6.4.</p> </li> <li> <p>Validation-Based Model Selection: By selecting models based on validation rather than test performance, we avoid the adaptive overfitting problem identified by Dwork et al. (2015) in their work on preserving validity in adaptive data analysis. Our validation protocols are detailed in configs/overfitting_prevention/validation/.</p> </li> </ol> <p>Complete theoretical treatments including Rademacher complexity analysis, PAC learning bounds, and empirical process theory perspectives are provided in OVERFITTING_PREVENTION.md \u00a7 Theoretical Framework.</p>"},{"location":"#162-transfer-learning-and-pre-training","title":"1.6.2 Transfer Learning and Pre-Training","text":"<p>The remarkable success of transformer-based models on text classification despite limited labeled data is largely attributable to transfer learning through unsupervised pre-training. The pre-training phase learns general linguistic representations from massive unlabeled corpora, while fine-tuning specializes these representations to task-specific patterns.</p> <p>Mathematical Formulation: Let \\(\\theta\\) denote the model parameters. Pre-training solves:</p> \\[\\theta^* = \\arg\\min_{\\theta} \\mathcal{L}_{\\text{pretrain}}(\\theta; \\mathcal{D}_{\\text{unlabeled}})\\] <p>where \\(\\mathcal{L}_{\\text{pretrain}}\\) is an unsupervised objective such as masked language modeling. For masked language modeling, given a sequence of tokens \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_T)\\), we randomly mask a subset of positions \\(\\mathcal{M}\\) and train the model to predict the masked tokens:</p> \\[\\mathcal{L}_{\\text{MLM}} = -\\sum_{i \\in \\mathcal{M}} \\log P(x_i | \\mathbf{x}_{\\backslash \\mathcal{M}}; \\theta)\\] <p>where \\(\\mathbf{x}_{\\backslash \\mathcal{M}}\\) denotes the sequence with masked positions replaced by a special [MASK] token.</p> <p>Fine-tuning then solves:</p> \\[\\theta_{\\text{task}} = \\arg\\min_{\\theta} \\mathcal{L}_{\\text{task}}(\\theta; \\mathcal{D}_{\\text{labeled}})\\] <p>initialized at \\(\\theta^*\\) rather than random initialization. For classification, the task loss is typically cross-entropy:</p> \\[\\mathcal{L}_{\\text{task}} = -\\sum_{(x,y) \\in \\mathcal{D}} \\log P(y | x; \\theta)\\] <p>Why This Works: Pre-training on diverse text learns broadly useful features\u2014syntactic patterns, semantic relationships, world knowledge\u2014that transfer across tasks. Fine-tuning requires learning only task-specific decision boundaries, which can be accomplished with far less labeled data than learning both representations and decision boundaries from scratch.</p> <p>The effectiveness can be understood through a two-stage perspective: 1. In the pre-training stage, the model learns to encode general linguistic knowledge into its parameters \\(\\theta^*\\) 2. In the fine-tuning stage, only the final classification layer (and optionally, small adjustments to the encoder) needs to be learned from the limited labeled data</p> <p>Evidence: Empirical studies (Devlin et al., 2019; Liu et al., 2019; He et al., 2021) demonstrate that pre-trained transformers achieve competitive performance with hundreds to thousands of labeled examples, whereas comparable architectures trained from scratch require orders of magnitude more data.</p> <p>Application in This Project: All Tier 2 and higher models leverage publicly available pre-trained checkpoints from Hugging Face Hub. We additionally explore domain-adaptive pre-training on news corpora to further specialize representations to the AG News domain. </p> <p>Pre-trained model configurations: - Standard transformers: configs/models/single/transformers/ - Large language models: configs/models/single/llm/ - Domain adaptation scripts: scripts/domain_adaptation/</p> <p>The transfer learning pipeline is detailed in ARCHITECTURE.md \u00a7 Transfer Learning.</p>"},{"location":"#163-parameter-efficient-fine-tuning-theory","title":"1.6.3 Parameter-Efficient Fine-Tuning Theory","text":"<p>Full fine-tuning of large pre-trained models is parameter-inefficient: it requires storing and updating hundreds of millions to billions of parameters, most of which change only slightly from their pre-trained values. Parameter-efficient fine-tuning (PEFT) methods address this by constraining updates to low-dimensional subspaces or small adapter modules.</p> <p>Low-Rank Adaptation (LoRA): The key insight is that the weight updates during fine-tuning often have low intrinsic dimensionality. For a pre-trained weight matrix \\(W_0 \\in \\mathbb{R}^{d \\times k}\\), LoRA represents updates as a low-rank decomposition:</p> \\[W = W_0 + \\Delta W = W_0 + BA\\] <p>where \\(B \\in \\mathbb{R}^{d \\times r}\\), \\(A \\in \\mathbb{R}^{r \\times k}\\), and \\(r \\ll \\min(d, k)\\) is the rank.</p> <p>Detailed Explanation:  - \\(W_0\\) is the original pre-trained weight matrix, which remains frozen during training - \\(\\Delta W = BA\\) represents the update to the weights, factorized as a product of two smaller matrices - Matrix \\(B\\) has dimensions \\(d \\times r\\) and matrix \\(A\\) has dimensions \\(r \\times k\\) - The rank \\(r\\) controls the capacity of the adaptation; smaller \\(r\\) means fewer trainable parameters and stronger regularization</p> <p>During forward pass, instead of using \\(W_0 x\\), we compute: \\(\\(y = W_0 x + BAx = W_0 x + \\Delta W x\\)\\)</p> <p>Parameter Count: The original matrix has \\(d \\times k\\) parameters. LoRA introduces only \\(r(d + k)\\) trainable parameters while freezing \\(W_0\\). For typical values (\\(d = k = 768\\), \\(r = 8\\)), this represents a reduction from 589,824 to 12,288 trainable parameters\u2014a 48\u00d7 reduction.</p> <p>For example, in a standard transformer attention layer: - Original: Query, Key, Value, Output matrices each have \\(768 \\times 768 = 589,824\\) parameters - With LoRA (rank 8): Each adaptation has only \\(8 \\times (768 + 768) = 12,288\\) trainable parameters - Total reduction: From ~2.4M parameters per attention layer to ~50K trainable parameters</p> <p>Theoretical Justification: Li et al. (2018) demonstrated that the optimization landscape of neural networks lies approximately on low-dimensional manifolds. More recently, Aghajanyan et al. (2021) showed empirically that fine-tuning is intrinsically low-dimensional, with effective rank often much smaller than the explicit parameter count. This means that even though the full parameter space is high-dimensional, the optimization trajectory primarily moves in a low-dimensional subspace.</p> <p>Quantized LoRA (QLoRA): Building on LoRA, QLoRA applies 4-bit quantization to the frozen base model weights, reducing memory requirements by approximately 4\u00d7 while maintaining fine-tuning quality through additional techniques:</p> <ol> <li>4-bit NormalFloat Quantization: Uses a special data type optimized for normally distributed weights</li> <li>Double Quantization: Quantizes the quantization constants themselves to save additional memory</li> <li>Paged Optimizers: Uses NVIDIA unified memory to handle memory spikes during training</li> </ol> <p>The quantization function maps full-precision weights \\(W_0\\) to 4-bit representation: \\(\\(W_0^{\\text{quant}} = \\text{Quantize}(W_0, \\text{dtype}=\\text{NF4})\\)\\)</p> <p>During fine-tuning, only the LoRA adapters \\(B\\) and \\(A\\) are kept in full precision.</p> <p>Application in This Project: We provide extensive configurations for LoRA (ranks 4, 8, 16, 32, 64) and QLoRA (4-bit, 8-bit) across multiple target modules (query, key, value, output projections). </p> <p>Configuration files: - LoRA configs: configs/training/efficient/lora/ - QLoRA configs: configs/training/efficient/qlora/ - Rank ablation: experiments/ablation_studies/lora_rank_ablation.py - Target module ablation: configs/training/efficient/lora/lora_target_modules_experiments.yaml</p> <p>Theoretical analysis is provided in OVERFITTING_PREVENTION.md \u00a7 Parameter Efficiency.</p>"},{"location":"#164-ensemble-methods-and-diversity","title":"1.6.4 Ensemble Methods and Diversity","text":"<p>Ensemble methods combine predictions from multiple models to achieve better performance than any individual model. The effectiveness of ensembling is well-understood through bias-variance decomposition.</p> <p>Bias-Variance-Covariance Decomposition: For squared loss in regression, the expected error of an ensemble can be decomposed as:</p> \\[E_{\\text{ensemble}} = \\overline{\\text{Bias}^2} + \\frac{1}{M}\\overline{\\text{Var}} + \\frac{M-1}{M}\\overline{\\text{Cov}}\\] <p>where: - \\(M\\) is the number of models in the ensemble - \\(\\overline{\\text{Bias}^2}\\) is the average squared bias of individual models - \\(\\overline{\\text{Var}}\\) is the average variance of individual models - \\(\\overline{\\text{Cov}}\\) is the average covariance between model predictions</p> <p>Detailed Interpretation: </p> <p>This decomposition reveals three key insights:</p> <ol> <li> <p>Bias Term (\\(\\overline{\\text{Bias}^2}\\)): Represents systematic errors that all models in the ensemble share. Ensembling does not reduce bias\u2014if all models make the same systematic mistake, averaging them preserves that mistake. The bias term remains constant regardless of ensemble size.</p> </li> <li> <p>Variance Term (\\(\\frac{1}{M}\\overline{\\text{Var}}\\)): Represents random errors due to finite training data. This term is reduced by a factor of \\(M\\) through averaging. With \\(M=5\\) models, variance is reduced to 20% of the single-model variance. This is why ensembles are particularly effective when individual models have high variance (overfitting).</p> </li> <li> <p>Covariance Term (\\(\\frac{M-1}{M}\\overline{\\text{Cov}}\\)): Represents correlation between model errors. If models make identical errors, \\(\\overline{\\text{Cov}} = \\overline{\\text{Var}}\\), and the variance reduction benefit is completely negated. Maximum variance reduction occurs when models are diverse\u2014making errors on different examples, so \\(\\overline{\\text{Cov}} \\approx 0\\).</p> </li> </ol> <p>For classification, while the exact decomposition differs, the same principles apply: ensemble performance improves when individual models are both accurate (low bias) and diverse (low error correlation).</p> <p>Diversity Promotion Strategies: We employ several approaches to create diverse ensemble members:</p> <ol> <li>Different Architectures: DeBERTa, RoBERTa, ELECTRA, XLNet have different inductive biases:</li> <li>DeBERTa uses disentangled attention mechanisms</li> <li>RoBERTa uses dynamic masking during pre-training</li> <li>ELECTRA uses discriminative pre-training objectives</li> <li>XLNet uses permutation language modeling</li> </ol> <p>Configurations in configs/models/ensemble/</p> <ol> <li> <p>Different Initializations: Multiple random seeds create different optimization trajectories, even with the same architecture. Configuration templates in configs/experiments/reproducibility/seeds.yaml.</p> </li> <li> <p>Different Data Views: Varied data augmentation strategies (back-translation, paraphrasing, LLM-based generation) expose models to different perturbations of the training data. Augmentation configs in configs/data/augmentation/.</p> </li> <li> <p>Different Training Procedures: Varied learning rates, dropout rates, and regularization strengths create models with different bias-variance trade-offs. Configurations in configs/training/regularization/.</p> </li> </ol> <p>Aggregation Methods: We compare multiple ensemble aggregation strategies:</p> <ol> <li>Soft Voting: Average predicted probabilities across models before taking argmax:    \\(\\(\\hat{y} = \\arg\\max_c \\frac{1}{M} \\sum_{m=1}^{M} P_m(y=c|x)\\)\\)</li> </ol> <p>Implementation in src/models/ensemble/voting/soft_voting.py</p> <ol> <li>Weighted Voting: Learn optimal weights \\(w_m\\) on validation set:    \\(\\(\\hat{y} = \\arg\\max_c \\sum_{m=1}^{M} w_m P_m(y=c|x)\\)\\)</li> </ol> <p>where \\(\\sum_m w_m = 1\\) and \\(w_m \\geq 0\\). Weights can be learned using linear regression or more complex meta-learners. Implementation in src/models/ensemble/voting/weighted_voting.py.</p> <ol> <li>Stacking: Train a meta-classifier (logistic regression, gradient boosting) on concatenated predictions from base models:    \\(\\(\\hat{y} = f_{\\text{meta}}([P_1(y|x), P_2(y|x), \\ldots, P_M(y|x)])\\)\\)</li> </ol> <p>Implementation in src/models/ensemble/stacking/</p> <p>Implementation details and empirical comparisons are provided in src/models/ensemble/ with selection guidance in SOTA_MODELS_GUIDE.md \u00a7 Ensemble Selection.</p>"},{"location":"#165-knowledge-distillation","title":"1.6.5 Knowledge Distillation","text":"<p>Knowledge distillation (Hinton et al., 2015) addresses a key limitation of ensemble methods: while ensembles achieve high accuracy, they are computationally expensive at inference time, requiring multiple forward passes. Distillation trains a single student model to mimic an ensemble teacher, preserving much of the performance gain while drastically reducing inference cost.</p> <p>Distillation Loss: The student is trained on a combination of two objectives:</p> \\[\\mathcal{L}_{\\text{distill}} = \\alpha \\cdot \\mathcal{L}_{\\text{CE}}(y, p_{\\text{student}}) + (1-\\alpha) \\cdot \\mathcal{L}_{\\text{KL}}(p_{\\text{teacher}}, p_{\\text{student}})\\] <p>where: - \\(\\mathcal{L}_{\\text{CE}}\\) is cross-entropy loss with ground truth labels \\(y\\) - \\(\\mathcal{L}_{\\text{KL}}\\) is KL divergence between teacher and student probability distributions - \\(\\alpha \\in [0,1]\\) balances the two objectives</p> <p>Detailed Explanation:</p> <p>The cross-entropy term ensures the student learns from the ground truth labels: \\(\\(\\mathcal{L}_{\\text{CE}}(y, p_{\\text{student}}) = -\\sum_{c=1}^{C} \\mathbb{1}[y=c] \\log p_{\\text{student}}(c)\\)\\)</p> <p>where \\(C\\) is the number of classes and \\(\\mathbb{1}[y=c]\\) is 1 if the true label is \\(c\\), else 0.</p> <p>The KL divergence term transfers knowledge from the teacher: \\(\\(\\mathcal{L}_{\\text{KL}}(p_{\\text{teacher}}, p_{\\text{student}}) = \\sum_{c=1}^{C} p_{\\text{teacher}}(c) \\log \\frac{p_{\\text{teacher}}(c)}{p_{\\text{student}}(c)}\\)\\)</p> <p>The hyperparameter \\(\\alpha\\) controls the trade-off: - \\(\\alpha = 1\\): Pure supervised learning (ignores teacher) - \\(\\alpha = 0\\): Pure distillation (ignores ground truth) - Typical values: \\(\\alpha \\in [0.1, 0.5]\\) work well in practice</p> <p>Temperature Scaling: To transfer richer information, both teacher and student logits \\(z\\) are divided by temperature \\(T\\) before applying softmax:</p> \\[p_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}\\] <p>Interpretation: Higher temperatures (\\(T &gt; 1\\)) produce softer probability distributions. Consider an example with 4 classes:</p> <ul> <li>Hard prediction (\\(T=1\\)): \\([0.98, 0.01, 0.01, 0.00]\\)</li> <li>The model is very confident about class 1</li> <li> <p>Provides almost no information about relationships between other classes</p> </li> <li> <p>Soft prediction (\\(T=3\\)): \\([0.70, 0.15, 0.12, 0.03]\\)</p> </li> <li>Still predicts class 1, but with less confidence</li> <li>Reveals that class 2 and 3 are more similar to class 1 than class 4</li> <li>The relative ordering of classes 2 and 3 provides information about semantic similarity</li> </ul> <p>This softer distribution reveals the teacher's uncertainty and relative similarities between classes\u2014information lost when using only hard labels (one-hot vectors). The temperature effectively \"smooths\" the distribution, making it easier for the student to learn the fine-grained knowledge encoded in the teacher's predictions.</p> <p>During distillation training: \\(\\(p_{\\text{teacher}}(T) = \\text{softmax}(z_{\\text{teacher}}/T)\\)\\) \\(\\(p_{\\text{student}}(T) = \\text{softmax}(z_{\\text{student}}/T)\\)\\)</p> <p>And the KL divergence is computed at temperature \\(T\\), then scaled by \\(T^2\\) to ensure gradient magnitudes match the cross-entropy term: \\(\\(\\mathcal{L}_{\\text{KL}} = T^2 \\cdot \\text{KL}(p_{\\text{teacher}}(T) \\| p_{\\text{student}}(T))\\)\\)</p> <p>Why It Works: The soft labels from the teacher ensemble contain information about class similarities and ambiguous cases that is not present in the one-hot ground truth labels. By training on this richer supervision signal, the student learns to mimic not just the teacher's final predictions but its confidence calibration and inter-class relationships.</p> <p>For example, in AG News classification: - Ground truth might simply say: \"This is a Sports article\" - Teacher's soft labels might say: \"90% Sports, 7% Business (because it discusses sports contracts), 2% World, 1% Sci/Tech\" - The student learns both the correct class AND the semantic relationships between categories</p> <p>Multi-Stage Distillation in This Project:</p> <ol> <li> <p>Stage 1 (LLM \u2192 Transformer): Distill knowledge from large language models (LLaMA 2-13B, Mistral-7B) into large transformers (DeBERTa-v3-Large), achieving 40\u00d7 parameter reduction with minimal accuracy loss. Scripts in scripts/training/distillation/distill_from_llama.py and scripts/training/distillation/distill_from_mistral.py.</p> </li> <li> <p>Stage 2 (Ensemble \u2192 Single): Distill an ensemble of 5-7 diverse DeBERTa-Large models into a single DeBERTa-Large student, capturing ensemble benefits without ensemble inference cost. Configuration in configs/training/advanced/knowledge_distillation/ensemble_distillation.yaml.</p> </li> <li> <p>Stage 3 (Compression): Apply INT8 quantization to the distilled model for 4\u00d7 size reduction and faster CPU inference. Scripts in scripts/optimization/quantization_optimization.py.</p> </li> </ol> <p>Experimental results demonstrating accuracy retention through this pipeline are provided in experiments/sota_experiments/phase3_llm_distillation.py. Theoretical analysis and best practices are detailed in SOTA_MODELS_GUIDE.md \u00a7 Knowledge Distillation.</p>"},{"location":"#17-scope-and-limitations","title":"1.7 Scope and Limitations","text":"<p>To establish appropriate expectations and clarify the boundaries of this project, we explicitly state what is and is not within scope.</p>"},{"location":"#171-within-scope","title":"1.7.1 Within Scope","text":"<p>This project provides:</p> <ul> <li> <p>Complete Text Classification Pipeline: End-to-end workflow from raw text preprocessing through model training, evaluation, and deployment for supervised classification tasks. Pipeline documentation in ARCHITECTURE.md.</p> </li> <li> <p>Rigorous Experimental Protocols: Infrastructure for conducting statistically valid experiments including proper train-validation-test splits, multiple random seed evaluation, significance testing, and prevention of data leakage. Protocols detailed in OVERFITTING_PREVENTION.md.</p> </li> <li> <p>Comprehensive Model Coverage: Implementations spanning classical machine learning baselines (Naive Bayes, SVM) through modern transformers (BERT, RoBERTa, DeBERTa) to large language models (LLaMA, Mistral) with parameter-efficient fine-tuning. Complete model zoo in configs/models/.</p> </li> <li> <p>Advanced Training Techniques: Support for ensemble methods, knowledge distillation, adversarial training, data augmentation, and multi-stage training pipelines. Training strategies in configs/training/.</p> </li> <li> <p>Platform Portability: Configurations and tooling for running experiments on consumer laptops, cloud platforms (Google Colab, Kaggle), and institutional compute clusters with automatic resource adaptation. Platform guide in PLATFORM_OPTIMIZATION_GUIDE.md.</p> </li> <li> <p>Extensive Documentation: Technical documentation targeted at multiple expertise levels from beginners to advanced researchers, including theoretical treatments, API references, and step-by-step tutorials. Documentation index in docs/.</p> </li> </ul>"},{"location":"#172-explicitly-out-of-scope","title":"1.7.2 Explicitly Out of Scope","text":"<p>This project does not provide:</p> <ul> <li> <p>Novel Architectural Contributions: We implement and combine existing published techniques rather than proposing new model architectures or training algorithms. Original research contributions are in the domain of systematic evaluation and rigorous experimental methodology.</p> </li> <li> <p>Multilingual Classification: Focus is on English-language text classification as determined by the AG News dataset. Extensions to multilingual scenarios would require different pre-trained models and evaluation protocols.</p> </li> <li> <p>Streaming or Real-Time Inference: The system is designed for batch processing and offline evaluation. While inference latency is measured and reported in benchmarks/efficiency/, we do not optimize for real-time serving constraints or provide production-grade serving infrastructure.</p> </li> <li> <p>Adversarial Robustness Certification: While we include adversarial training as an optional technique in configs/training/advanced/adversarial_training.yaml, we do not provide formally verified robustness guarantees or certified defense mechanisms against adversarial examples.</p> </li> <li> <p>Automatic Data Collection: We assume users have access to their data. The project provides tools for processing and analyzing data in src/data/, but does not include web scraping, data purchasing, or crowdsourcing annotation pipelines.</p> </li> <li> <p>Production Deployment Infrastructure: While we provide basic Docker configurations in deployment/docker/ and deployment guidelines in docs/user_guide/local_deployment.md, comprehensive production infrastructure (Kubernetes orchestration, auto-scaling, monitoring, A/B testing) is beyond scope.</p> </li> <li> <p>Graphical User Interfaces: All interfaces are command-line based or notebook-based. We do not provide web dashboards or GUI tools for non-technical users, though Streamlit and Gradio apps are available in app/ for demonstration purposes.</p> </li> </ul>"},{"location":"#173-assumptions-and-prerequisites","title":"1.7.3 Assumptions and Prerequisites","text":"<p>Data Assumptions:</p> <ul> <li> <p>Balanced or Near-Balanced Classes: Many techniques (ensemble diversity metrics, stratified sampling) assume approximately balanced class distributions. Highly imbalanced datasets may require additional techniques like class weighting or resampling not extensively covered in this framework.</p> </li> <li> <p>Text Length: Default configurations assume document length of 512 tokens or fewer, matching common transformer limits. Longer documents require alternative architectures (Longformer, BigBird) available in configs/models/single/transformers/longformer/ or truncation strategies.</p> </li> <li> <p>Clean Labels: We assume labels are accurate and consistent. Label noise handling is not explicitly addressed beyond standard regularization techniques.</p> </li> <li> <p>Single-Label Classification: The focus is on assigning each document to exactly one category. Multi-label classification (assigning multiple categories per document) requires modifications to loss functions and evaluation metrics.</p> </li> </ul> <p>Computational Assumptions:</p> <ul> <li> <p>GPU Access: Training transformer-based models requires GPU acceleration. While CPU-only training is technically possible, it is prohibitively slow for models beyond simple baselines. Access to cloud platforms with GPU quotas or local GPU hardware is assumed. CPU-optimized configs available in configs/models/recommended/tier_5_free_optimized/cpu_friendly/.</p> </li> <li> <p>Internet Connectivity: Downloading pre-trained model checkpoints requires stable internet access and sufficient bandwidth. Models range from hundreds of megabytes (BERT-Base) to tens of gigabytes (LLaMA-70B).</p> </li> <li> <p>Storage Capacity: Training runs generate checkpoints, logs, and cached preprocessed data. We recommend at least 50GB of available disk space for a typical experiment series.</p> </li> </ul> <p>User Knowledge Assumptions:</p> <ul> <li> <p>Python Programming: Users should be comfortable with Python syntax, including functions, classes, imports, and common libraries (NumPy, pandas).</p> </li> <li> <p>Command-Line Interfaces: Basic familiarity with terminal commands, file paths, and environment variables.</p> </li> <li> <p>Machine Learning Fundamentals: Understanding of core concepts including train-validation-test splits, overfitting, hyperparameters, and evaluation metrics (accuracy, precision, recall, F1).</p> </li> <li> <p>Deep Learning Basics (for advanced features): Familiarity with neural network architectures, backpropagation, optimization algorithms, and regularization techniques.</p> </li> </ul> <p>Users without these prerequisites are encouraged to consult the beginner-level tutorials in docs/level_1_beginner/ which provide gentler introductions with more extensive explanations.</p>"},{"location":"#174-known-limitations","title":"1.7.4 Known Limitations","text":"<p>Limitation 1: Quadratic Complexity of Self-Attention</p> <p>Transformer architectures compute pairwise attention between all tokens, resulting in \\(O(n^2)\\) memory and computational complexity where \\(n\\) is sequence length. This limits practical sequence lengths to 512-1024 tokens for standard models.</p> <p>Mitigation: We provide configurations for efficient transformers (Longformer, BigBird) that use sparse attention patterns to achieve \\(O(n)\\) complexity in configs/models/single/transformers/longformer/. However, these are not the primary focus and have received less extensive tuning.</p> <p>Future Direction: Integration of recent linear attention mechanisms (Performers, FNet, Linformer) could enable processing of longer documents without quadratic scaling. Tracked in ROADMAP.md.</p> <p>Limitation 2: Memory Requirements for Large Models</p> <p>Even with parameter-efficient fine-tuning and quantization, the largest models (LLaMA-70B, Mixtral-8x7B) require 40-80GB of GPU memory. This exceeds the capacity of consumer GPUs and requires either model parallelism across multiple devices or cloud instances with high-memory GPUs.</p> <p>Mitigation: QLoRA with 4-bit quantization reduces memory requirements by approximately 4\u00d7, making models up to 13B parameters accessible on consumer GPUs with 16-24GB VRAM. Configurations in configs/training/efficient/qlora/. For larger models, we provide configurations for gradient checkpointing and CPU offloading that trade computation time for memory in configs/training/efficient/.</p> <p>Future Direction: Exploration of more aggressive quantization (INT4, INT2) and structured pruning could further reduce memory footprints. Research directions in ROADMAP.md.</p> <p>Limitation 3: Ensemble Inference Overhead</p> <p>While ensemble methods achieve the highest accuracy in our experiments (results in benchmarks/accuracy/ensemble_results.json), they require multiple forward passes at inference time, increasing latency proportionally to ensemble size. A 7-model ensemble is 7\u00d7 slower than a single model.</p> <p>Mitigation: Knowledge distillation compresses ensemble knowledge into a single student model, retaining typically 90-95% of ensemble improvement while restoring single-model inference speed. Distillation configurations in configs/training/advanced/knowledge_distillation/.</p> <p>Future Direction: Investigation of fast ensemble approximation methods such as dropout ensembles or BatchEnsemble could provide accuracy benefits closer to full ensembles with minimal computational overhead. Tracked in ROADMAP.md.</p> <p>Limitation 4: Platform-Specific Quota Management</p> <p>Our platform detection and quota management systems use heuristics based on typical platform limits (Colab 12-hour sessions, Kaggle 30-hour GPU weekly quotas). These limits may change over time and vary based on account type (free tier versus paid subscriptions).</p> <p>Mitigation: Conservative checkpoint intervals and explicit quota tracking provide safety margins in src/deployment/quota_tracker.py. Users can override automatic settings if they have specific information about their quota limits through configs/quotas/.</p> <p>Future Direction: Integration with platform APIs (when available) could provide real-time quota information rather than relying on hardcoded assumptions. Tracked in ROADMAP.md.</p> <p>Limitation 5: English-Only Evaluation</p> <p>All experiments are conducted on the English-language AG News dataset. While the underlying transformer models (mBERT, XLM-R) support multilingual text, we have not evaluated cross-lingual transfer or performance on non-English datasets.</p> <p>Mitigation: The framework architecture is language-agnostic, and extending to other languages primarily requires different datasets and evaluation protocols rather than code modifications. The modular design in src/ supports such extensions.</p> <p>Future Direction: Systematic evaluation on multilingual benchmarks (XNLI, PAWS-X) and investigation of cross-lingual transfer learning techniques. Multilingual support tracked in ROADMAP.md.</p>"},{"location":"#18-organization-of-documentation","title":"1.8 Organization of Documentation","text":"<p>This project provides extensive documentation organized according to progressive disclosure principles, allowing users to engage at levels appropriate to their expertise and goals.</p>"},{"location":"#181-entry-points-by-user-type","title":"1.8.1 Entry Points by User Type","text":"<p>New Users Seeking Quick Start:</p> <ul> <li>QUICK_START.md: Minimal setup to first working model in under 10 minutes</li> <li>quickstart/auto_start.py: Single-command demo requiring no configuration</li> <li>docs/getting_started/installation.md: Comprehensive installation guide for different platforms</li> <li>quickstart/decision_tree.py: Interactive CLI for guided model selection</li> </ul> <p>Practitioners Seeking Best Practices:</p> <ul> <li>SOTA_MODELS_GUIDE.md: Model selection flowcharts and performance comparisons</li> <li>docs/user_guide/: Detailed guides on data preparation, training, evaluation, and deployment</li> <li>docs/best_practices/: Domain-specific recommendations for model selection, hyperparameter tuning, and avoiding common pitfalls</li> <li>notebooks/01_tutorials/: Interactive Jupyter tutorials with executable examples</li> </ul> <p>Researchers Seeking Theoretical Depth:</p> <ul> <li>OVERFITTING_PREVENTION.md: Complete theoretical framework including proofs and statistical guarantees</li> <li>ARCHITECTURE.md: Detailed system architecture with transformer internals and optimization techniques</li> <li>docs/level_3_advanced/: Advanced topics including custom model development and research workflows</li> <li>experiments/: Ablation studies, hyperparameter searches, and baseline comparisons with full experimental protocols</li> </ul> <p>Developers Seeking to Extend the System:</p> <ul> <li>docs/developer_guide/: Architecture overview, coding standards, and extension points</li> <li>docs/api_reference/: Complete API documentation for all modules</li> <li>CONTRIBUTING.md: Guidelines for contributing code, documentation, and bug reports</li> <li>ARCHITECTURE.md: Design patterns and architectural decisions</li> </ul>"},{"location":"#182-document-hierarchy-and-specialization","title":"1.8.2 Document Hierarchy and Specialization","text":"<p>To avoid duplication and maintain a single source of truth, different documents have clearly defined scopes:</p> <p>README.md (This Document): - High-level introduction to the project and its motivation - Overview of theoretical foundations with links to detailed treatments - Dataset description and experimental setup (Section 2) - Quick start instructions and navigation guide</p> <p>OVERFITTING_PREVENTION.md: - Complete mathematical treatment of generalization theory - Detailed description of overfitting prevention architecture - Empirical validation of prevention mechanisms - Decision trees for selecting prevention strategies</p> <p>ARCHITECTURE.md: - System architecture and component interactions - Detailed explanations of transformer architectures - Optimization techniques and efficiency considerations - Design patterns and extension points</p> <p>SOTA_MODELS_GUIDE.md: - Model zoo overview and tier descriptions - Performance benchmarks and accuracy-efficiency trade-offs - Hyperparameter recommendations for each model family - Selection flowcharts for different use cases</p> <p>PLATFORM_OPTIMIZATION_GUIDE.md: - Platform detection mechanisms - Resource profiling and quota management - Checkpoint synchronization strategies - Platform-specific optimization techniques</p> <p>User Guides in docs/user_guide/: - Step-by-step instructions for common workflows - Code examples and configuration templates - Troubleshooting common issues - Best practices for specific tasks</p> <p>API Reference in docs/api_reference/: - Function and class signatures - Parameter descriptions and types - Return value specifications - Usage examples for each API component</p> <p>This hierarchical organization ensures that:</p> <ol> <li>Users can find information at the appropriate level of detail</li> <li>Each piece of information has a single authoritative source</li> <li>Updates to concepts need to be made in only one location</li> <li>Cross-references guide users to related information in other documents</li> </ol> <p>For a visual overview of the documentation structure, see docs/00_START_HERE.md, which provides a guided navigation map based on your learning objectives and experience level.</p>"},{"location":"#project-structure","title":"Project Structure","text":"<p>The repository is organized as follows:</p> Text Only<pre><code>ag-news-text-classification/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 CITATION.cff\n\u251c\u2500\u2500 CHANGELOG.md\n\u251c\u2500\u2500 ARCHITECTURE.md\n\u251c\u2500\u2500 PERFORMANCE.md\n\u251c\u2500\u2500 SECURITY.md\n\u251c\u2500\u2500 TROUBLESHOOTING.md\n\u251c\u2500\u2500 SOTA_MODELS_GUIDE.md\n\u251c\u2500\u2500 OVERFITTING_PREVENTION.md\n\u251c\u2500\u2500 ROADMAP.md\n\u251c\u2500\u2500 FREE_DEPLOYMENT_GUIDE.md\n\u251c\u2500\u2500 PLATFORM_OPTIMIZATION_GUIDE.md\n\u251c\u2500\u2500 IDE_SETUP_GUIDE.md\n\u251c\u2500\u2500 LOCAL_MONITORING_GUIDE.md\n\u251c\u2500\u2500 QUICK_START.md\n\u251c\u2500\u2500 HEALTH_CHECK.md\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 MANIFEST.in\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 poetry.lock\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 install.sh\n\u251c\u2500\u2500 .env.example\n\u251c\u2500\u2500 .env.test\n\u251c\u2500\u2500 .env.local\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .gitattributes\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 .editorconfig\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 .flake8\n\u251c\u2500\u2500 commitlint.config.js\n\u2502\n\u251c\u2500\u2500 requirements/\n\u2502   \u251c\u2500\u2500 base.txt\n\u2502   \u251c\u2500\u2500 ml.txt\n\u2502   \u251c\u2500\u2500 llm.txt\n\u2502   \u251c\u2500\u2500 efficient.txt\n\u2502   \u251c\u2500\u2500 local_prod.txt\n\u2502   \u251c\u2500\u2500 dev.txt\n\u2502   \u251c\u2500\u2500 data.txt\n\u2502   \u251c\u2500\u2500 ui.txt\n\u2502   \u251c\u2500\u2500 docs.txt\n\u2502   \u251c\u2500\u2500 minimal.txt\n\u2502   \u251c\u2500\u2500 research.txt\n\u2502   \u251c\u2500\u2500 robustness.txt\n\u2502   \u251c\u2500\u2500 all_local.txt\n\u2502   \u251c\u2500\u2500 colab.txt\n\u2502   \u251c\u2500\u2500 kaggle.txt\n\u2502   \u251c\u2500\u2500 free_tier.txt\n\u2502   \u251c\u2500\u2500 platform_minimal.txt\n\u2502   \u251c\u2500\u2500 local_monitoring.txt\n\u2502   \u2514\u2500\u2500 lock/\n\u2502       \u251c\u2500\u2500 base.lock\n\u2502       \u251c\u2500\u2500 ml.lock\n\u2502       \u251c\u2500\u2500 llm.lock\n\u2502       \u251c\u2500\u2500 all.lock\n\u2502       \u2514\u2500\u2500 README.md\n\u2502\n\u251c\u2500\u2500 .devcontainer/\n\u2502   \u251c\u2500\u2500 devcontainer.json\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u2502\n\u251c\u2500\u2500 .husky/\n\u2502   \u251c\u2500\u2500 pre-commit\n\u2502   \u2514\u2500\u2500 commit-msg\n\u2502\n\u251c\u2500\u2500 .ide/\n\u2502   \u251c\u2500\u2500 SOURCE_OF_TRUTH.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 vscode/\n\u2502   \u2502   \u251c\u2500\u2500 settings.json\n\u2502   \u2502   \u251c\u2500\u2500 launch.json\n\u2502   \u2502   \u251c\u2500\u2500 tasks.json\n\u2502   \u2502   \u251c\u2500\u2500 extensions.json\n\u2502   \u2502   \u2514\u2500\u2500 snippets/\n\u2502   \u2502       \u251c\u2500\u2500 python.json\n\u2502   \u2502       \u2514\u2500\u2500 yaml.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 pycharm/\n\u2502   \u2502   \u251c\u2500\u2500 .idea/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 workspace.xml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 misc.xml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 modules.xml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 inspectionProfiles/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 runConfigurations/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 train_model.xml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 run_tests.xml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 start_api.xml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 codeStyles/\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 Project.xml\n\u2502   \u2502   \u251c\u2500\u2500 README_PYCHARM.md\n\u2502   \u2502   \u2514\u2500\u2500 settings.zip\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 jupyter/\n\u2502   \u2502   \u251c\u2500\u2500 jupyter_notebook_config.py\n\u2502   \u2502   \u251c\u2500\u2500 jupyter_lab_config.py\n\u2502   \u2502   \u251c\u2500\u2500 custom/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 custom.css\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 custom.js\n\u2502   \u2502   \u251c\u2500\u2500 nbextensions_config.json\n\u2502   \u2502   \u251c\u2500\u2500 lab/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 user-settings/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 workspaces/\n\u2502   \u2502   \u2514\u2500\u2500 kernels/\n\u2502   \u2502       \u2514\u2500\u2500 ag-news/\n\u2502   \u2502           \u2514\u2500\u2500 kernel.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 vim/\n\u2502   \u2502   \u251c\u2500\u2500 .vimrc\n\u2502   \u2502   \u251c\u2500\u2500 coc-settings.json\n\u2502   \u2502   \u251c\u2500\u2500 ultisnips/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 python.snippets\n\u2502   \u2502   \u2514\u2500\u2500 README_VIM.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 neovim/\n\u2502   \u2502   \u251c\u2500\u2500 init.lua\n\u2502   \u2502   \u251c\u2500\u2500 lua/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 plugins.lua\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lsp.lua\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 keymaps.lua\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ag-news/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 config.lua\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 commands.lua\n\u2502   \u2502   \u251c\u2500\u2500 coc-settings.json\n\u2502   \u2502   \u2514\u2500\u2500 README_NEOVIM.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 sublime/\n\u2502   \u2502   \u251c\u2500\u2500 ag-news.sublime-project\n\u2502   \u2502   \u251c\u2500\u2500 ag-news.sublime-workspace\n\u2502   \u2502   \u251c\u2500\u2500 Preferences.sublime-settings\n\u2502   \u2502   \u251c\u2500\u2500 Python.sublime-settings\n\u2502   \u2502   \u251c\u2500\u2500 snippets/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pytorch-model.sublime-snippet\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 lora-config.sublime-snippet\n\u2502   \u2502   \u251c\u2500\u2500 build_systems/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Train Model.sublime-build\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Run Tests.sublime-build\n\u2502   \u2502   \u2514\u2500\u2500 README_SUBLIME.md\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 cloud_ides/\n\u2502       \u251c\u2500\u2500 gitpod/\n\u2502       \u2502   \u251c\u2500\u2500 .gitpod.yml\n\u2502       \u2502   \u2514\u2500\u2500 .gitpod.Dockerfile\n\u2502       \u251c\u2500\u2500 codespaces/\n\u2502       \u2502   \u2514\u2500\u2500 .devcontainer.json\n\u2502       \u251c\u2500\u2500 colab/\n\u2502       \u2502   \u251c\u2500\u2500 colab_setup.py\n\u2502       \u2502   \u2514\u2500\u2500 drive_mount.py\n\u2502       \u2514\u2500\u2500 kaggle/\n\u2502           \u2514\u2500\u2500 kaggle_setup.py\n\u2502\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 pipeline.png\n\u2502   \u251c\u2500\u2500 api_architecture.png\n\u2502   \u251c\u2500\u2500 local_deployment_flow.png\n\u2502   \u251c\u2500\u2500 overfitting_prevention_flow.png\n\u2502   \u251c\u2500\u2500 sota_model_architecture.png\n\u2502   \u251c\u2500\u2500 decision_tree.png\n\u2502   \u251c\u2500\u2500 platform_detection_flow.png\n\u2502   \u251c\u2500\u2500 auto_training_workflow.png\n\u2502   \u251c\u2500\u2500 quota_management_diagram.png\n\u2502   \u2514\u2500\u2500 progressive_disclosure.png\n\u2502\n\u251c\u2500\u2500 configs/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config_loader.py\n\u2502   \u251c\u2500\u2500 config_validator.py\n\u2502   \u251c\u2500\u2500 config_schema.py\n\u2502   \u251c\u2500\u2500 constants.py\n\u2502   \u251c\u2500\u2500 compatibility_matrix.yaml\n\u2502   \u251c\u2500\u2500 smart_defaults.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 rest_config.yaml\n\u2502   \u2502   \u251c\u2500\u2500 auth_config.yaml\n\u2502   \u2502   \u2514\u2500\u2500 rate_limit_config.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 prediction_service.yaml\n\u2502   \u2502   \u251c\u2500\u2500 training_service.yaml\n\u2502   \u2502   \u251c\u2500\u2500 data_service.yaml\n\u2502   \u2502   \u251c\u2500\u2500 model_service.yaml\n\u2502   \u2502   \u2514\u2500\u2500 local_monitoring.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 environments/\n\u2502   \u2502   \u251c\u2500\u2500 dev.yaml\n\u2502   \u2502   \u251c\u2500\u2500 local_prod.yaml\n\u2502   \u2502   \u251c\u2500\u2500 colab.yaml\n\u2502   \u2502   \u2514\u2500\u2500 kaggle.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 features/\n\u2502   \u2502   \u2514\u2500\u2500 feature_flags.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 secrets/\n\u2502   \u2502   \u251c\u2500\u2500 secrets.template.yaml\n\u2502   \u2502   \u2514\u2500\u2500 local_secrets.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 templates/\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 deberta_template.yaml.j2\n\u2502   \u2502   \u251c\u2500\u2500 roberta_template.yaml.j2\n\u2502   \u2502   \u251c\u2500\u2500 llm_template.yaml.j2\n\u2502   \u2502   \u251c\u2500\u2500 ensemble_template.yaml.j2\n\u2502   \u2502   \u2514\u2500\u2500 training_template.yaml.j2\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 generation/\n\u2502   \u2502   \u251c\u2500\u2500 model_specs.yaml\n\u2502   \u2502   \u251c\u2500\u2500 training_specs.yaml\n\u2502   \u2502   \u2514\u2500\u2500 ensemble_specs.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 SELECTION_GUIDE.md\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 recommended/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ag_news_best_practices.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 quick_start.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 balanced.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sota_accuracy.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 tier_1_sota/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v3_xlarge_lora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v2_xxlarge_qlora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_large_lora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 electra_large_lora.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 xlnet_large_lora.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 tier_2_llm/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama2_7b_qlora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama2_13b_qlora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama3_8b_qlora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral_7b_qlora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mixtral_8x7b_qlora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 falcon_7b_qlora.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 phi_3_qlora.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 mpt_7b_qlora.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 tier_3_ensemble/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 xlarge_ensemble.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llm_ensemble.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 hybrid_ensemble.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 open_source_llm_ensemble.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 tier_4_distilled/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama_distilled_deberta.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral_distilled_roberta.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ensemble_distilled.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 tier_5_free_optimized/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 auto_selected/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 colab_free_auto.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 colab_pro_auto.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 kaggle_auto.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 local_auto.yaml\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 platform_matrix.yaml\n\u2502   \u2502   \u2502       \u2502\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 platform_specific/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 colab_optimized.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 kaggle_tpu_optimized.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 local_cpu_optimized.yaml\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 local_gpu_optimized.yaml\n\u2502   \u2502   \u2502       \u2502\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 colab_friendly/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 deberta_large_lora_colab.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 distilroberta_efficient.yaml\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 ensemble_lightweight.yaml\n\u2502   \u2502   \u2502       \u2502\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 cpu_friendly/\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 distilled_cpu_optimized.yaml\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 quantized_int8.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 single/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 transformers/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta/\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v3_base.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v3_large.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v3_xlarge.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v2_xlarge.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v2_xxlarge.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 deberta_sliding_window.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta/\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_base.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_large.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_large_mnli.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 xlm_roberta_large.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 electra/\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 electra_base.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 electra_large.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 electra_discriminator.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 xlnet/\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 xlnet_base.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 xlnet_large.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 longformer/\n\u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 longformer_base.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 longformer_large.yaml\n\u2502   \u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 t5/\n\u2502   \u2502   \u2502   \u2502       \u251c\u2500\u2500 t5_base.yaml\n\u2502   \u2502   \u2502   \u2502       \u251c\u2500\u2500 t5_large.yaml\n\u2502   \u2502   \u2502   \u2502       \u251c\u2500\u2500 t5_3b.yaml\n\u2502   \u2502   \u2502   \u2502       \u2514\u2500\u2500 flan_t5_xl.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 llm/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 llama/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 llama2_7b.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 llama2_13b.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 llama2_70b.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 llama3_8b.yaml\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 llama3_70b.yaml\n\u2502   \u2502   \u2502       \u2502\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 mistral/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 mistral_7b.yaml\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 mistral_7b_instruct.yaml\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 mixtral_8x7b.yaml\n\u2502   \u2502   \u2502       \u2502\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 falcon/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 falcon_7b.yaml\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 falcon_40b.yaml\n\u2502   \u2502   \u2502       \u2502\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 mpt/\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 mpt_7b.yaml\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 mpt_30b.yaml\n\u2502   \u2502   \u2502       \u2502\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 phi/\n\u2502   \u2502   \u2502           \u251c\u2500\u2500 phi_2.yaml\n\u2502   \u2502   \u2502           \u2514\u2500\u2500 phi_3.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 ensemble/\n\u2502   \u2502       \u251c\u2500\u2500 ENSEMBLE_SELECTION_GUIDE.yaml\n\u2502   \u2502       \u251c\u2500\u2500 presets/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 quick_start.yaml\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 sota_accuracy.yaml\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 balanced.yaml\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 voting/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 soft_voting_xlarge.yaml\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 weighted_voting_llm.yaml\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 rank_voting_hybrid.yaml\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 stacking/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 stacking_xlarge_xgboost.yaml\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 stacking_llm_lightgbm.yaml\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 stacking_hybrid_catboost.yaml\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 blending/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 blending_xlarge.yaml\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 dynamic_blending_llm.yaml\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u2514\u2500\u2500 advanced/\n\u2502   \u2502           \u251c\u2500\u2500 bayesian_ensemble_xlarge.yaml\n\u2502   \u2502           \u251c\u2500\u2500 snapshot_ensemble_llm.yaml\n\u2502   \u2502           \u2514\u2500\u2500 multi_level_ensemble.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 standard/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_training.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mixed_precision.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 distributed.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 platform_adaptive/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 colab_free_training.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 colab_pro_training.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kaggle_gpu_training.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kaggle_tpu_training.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 local_gpu_training.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 local_cpu_training.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 efficient/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lora/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_config.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_xlarge.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_llm.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_rank_experiments.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 lora_target_modules_experiments.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 qlora/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 qlora_4bit.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 qlora_8bit.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 qlora_nf4.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 qlora_llm.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 adapters/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adapter_houlsby.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adapter_pfeiffer.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adapter_parallel.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adapter_fusion.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 adapter_stacking.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prefix_tuning/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 prefix_tuning.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 prefix_tuning_llm.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 prefix_length_experiments.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt_tuning/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 soft_prompt_tuning.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 p_tuning_v2.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 prompt_length_experiments.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ia3/\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ia3_config.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 combined/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 lora_plus_adapters.yaml\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 qlora_plus_prompt.yaml\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 multi_method_fusion.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 tpu/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kaggle_tpu_v3.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 tpu_optimization.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 advanced/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 curriculum_learning.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 adversarial_training.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 multitask_learning.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 contrastive_learning.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 knowledge_distillation/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 standard_distillation.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama_distillation.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral_distillation.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llm_to_xlarge_distillation.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 xlarge_to_large_distillation.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ensemble_distillation.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 self_distillation.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 meta_learning.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 instruction_tuning/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 alpaca_style.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dolly_style.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 vicuna_style.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 custom_instructions.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 multi_stage/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 stage_manager.yaml\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 progressive_training.yaml\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 iterative_refinement.yaml\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 base_to_xlarge_progressive.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 regularization/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dropout_strategies/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 standard_dropout.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 variational_dropout.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dropconnect.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adaptive_dropout.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 monte_carlo_dropout.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 scheduled_dropout.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 advanced_regularization/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 r_drop.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mixout.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 spectral_normalization.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 gradient_penalty.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 weight_decay_schedule.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 elastic_weight_consolidation.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data_regularization/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mixup.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cutmix.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cutout.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 manifold_mixup.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 augmax.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 combined/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 heavy_regularization.yaml\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 xlarge_safe_config.yaml\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 llm_safe_config.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 safe/\n\u2502   \u2502       \u251c\u2500\u2500 xlarge_safe_training.yaml\n\u2502   \u2502       \u251c\u2500\u2500 llm_safe_training.yaml\n\u2502   \u2502       \u251c\u2500\u2500 ensemble_safe_training.yaml\n\u2502   \u2502       \u2514\u2500\u2500 ultra_safe_training.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 overfitting_prevention/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 constraints/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_size_constraints.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 xlarge_constraints.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 llm_constraints.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ensemble_constraints.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 training_constraints.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 parameter_efficiency_requirements.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 monitoring/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 realtime_monitoring.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 thresholds.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 metrics_to_track.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 reporting_schedule.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 validation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cross_validation_strategy.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 holdout_validation.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_set_protection.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data_split_rules.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 hyperparameter_tuning_rules.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 recommendations/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dataset_specific/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ag_news_recommendations.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 small_dataset.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 medium_dataset.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 large_dataset.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_recommendations/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 xlarge_models.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llm_models.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 model_selection_guide.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 technique_recommendations/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 lora_recommendations.yaml\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 qlora_recommendations.yaml\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 distillation_recommendations.yaml\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 ensemble_recommendations.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 safe_defaults/\n\u2502   \u2502       \u251c\u2500\u2500 xlarge_safe_defaults.yaml\n\u2502   \u2502       \u251c\u2500\u2500 llm_safe_defaults.yaml\n\u2502   \u2502       \u2514\u2500\u2500 beginner_safe_defaults.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 preprocessing/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 standard.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 advanced.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 llm_preprocessing.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 instruction_formatting.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 domain_specific.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 augmentation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 safe_augmentation.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 basic_augmentation.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 back_translation.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 paraphrase_generation.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 llm_augmentation/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama_augmentation.yaml\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral_augmentation.yaml\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 controlled_generation.yaml\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mixup_strategies.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 adversarial_augmentation.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 contrast_sets.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 selection/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 coreset_selection.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 influence_functions.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 active_selection.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 validation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 stratified_split.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 k_fold_cv.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 nested_cv.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 time_based_split.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 holdout_validation.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 external/\n\u2502   \u2502       \u251c\u2500\u2500 news_corpus.yaml\n\u2502   \u2502       \u251c\u2500\u2500 wikipedia.yaml\n\u2502   \u2502       \u251c\u2500\u2500 domain_adaptive_pretraining.yaml\n\u2502   \u2502       \u2514\u2500\u2500 synthetic_data/\n\u2502   \u2502           \u251c\u2500\u2500 llm_generated.yaml\n\u2502   \u2502           \u2514\u2500\u2500 quality_filtering.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 deployment/\n\u2502   \u2502   \u251c\u2500\u2500 local/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 docker_local.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 api_local.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 inference_local.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 free_tier/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 colab_deployment.yaml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 kaggle_deployment.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 huggingface_spaces.yaml\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 platform_profiles/\n\u2502   \u2502       \u251c\u2500\u2500 colab_profile.yaml\n\u2502   \u2502       \u251c\u2500\u2500 kaggle_profile.yaml\n\u2502   \u2502       \u251c\u2500\u2500 gitpod_profile.yaml\n\u2502   \u2502       \u251c\u2500\u2500 codespaces_profile.yaml\n\u2502   \u2502       \u2514\u2500\u2500 hf_spaces_profile.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 quotas/\n\u2502   \u2502   \u251c\u2500\u2500 quota_limits.yaml\n\u2502   \u2502   \u251c\u2500\u2500 quota_tracking.yaml\n\u2502   \u2502   \u2514\u2500\u2500 platform_quotas.yaml\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 experiments/\n\u2502       \u251c\u2500\u2500 baselines/\n\u2502       \u2502   \u251c\u2500\u2500 classical_ml.yaml\n\u2502       \u2502   \u2514\u2500\u2500 transformer_baseline.yaml\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 ablations/\n\u2502       \u2502   \u251c\u2500\u2500 model_size_ablation.yaml\n\u2502       \u2502   \u251c\u2500\u2500 data_amount.yaml\n\u2502       \u2502   \u251c\u2500\u2500 lora_rank_ablation.yaml\n\u2502       \u2502   \u251c\u2500\u2500 qlora_bits_ablation.yaml\n\u2502       \u2502   \u251c\u2500\u2500 regularization_ablation.yaml\n\u2502       \u2502   \u251c\u2500\u2500 augmentation_impact.yaml\n\u2502       \u2502   \u251c\u2500\u2500 ensemble_size_ablation.yaml\n\u2502       \u2502   \u251c\u2500\u2500 ensemble_components.yaml\n\u2502       \u2502   \u251c\u2500\u2500 prompt_ablation.yaml\n\u2502       \u2502   \u2514\u2500\u2500 distillation_temperature_ablation.yaml\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 hyperparameter_search/\n\u2502       \u2502   \u251c\u2500\u2500 lora_search.yaml\n\u2502       \u2502   \u251c\u2500\u2500 qlora_search.yaml\n\u2502       \u2502   \u251c\u2500\u2500 regularization_search.yaml\n\u2502       \u2502   \u2514\u2500\u2500 ensemble_weights_search.yaml\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 sota_experiments/\n\u2502       \u2502   \u251c\u2500\u2500 phase1_xlarge_models.yaml\n\u2502       \u2502   \u251c\u2500\u2500 phase2_llm_models.yaml\n\u2502       \u2502   \u251c\u2500\u2500 phase3_llm_distillation.yaml\n\u2502       \u2502   \u251c\u2500\u2500 phase4_ensemble_sota.yaml\n\u2502       \u2502   \u251c\u2500\u2500 phase5_ultimate_sota.yaml\n\u2502       \u2502   \u2514\u2500\u2500 phase6_production_sota.yaml\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500 reproducibility/\n\u2502           \u251c\u2500\u2500 seeds.yaml\n\u2502           \u2514\u2500\u2500 hardware_specs.yaml\n\u2502\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2502   \u251c\u2500\u2500 ag_news/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train.csv\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test.csv\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 README.md\n\u2502   \u2502   \u2514\u2500\u2500 .gitkeep\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 validation/\n\u2502   \u2502   \u251c\u2500\u2500 test/\n\u2502   \u2502   \u251c\u2500\u2500 stratified_folds/\n\u2502   \u2502   \u251c\u2500\u2500 instruction_formatted/\n\u2502   \u2502   \u2514\u2500\u2500 .test_set_hash\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 augmented/\n\u2502   \u2502   \u251c\u2500\u2500 back_translated/\n\u2502   \u2502   \u251c\u2500\u2500 paraphrased/\n\u2502   \u2502   \u251c\u2500\u2500 synthetic/\n\u2502   \u2502   \u251c\u2500\u2500 llm_generated/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 llama2/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 mixtral/\n\u2502   \u2502   \u251c\u2500\u2500 mixup/\n\u2502   \u2502   \u2514\u2500\u2500 contrast_sets/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 external/\n\u2502   \u2502   \u251c\u2500\u2500 news_corpus/\n\u2502   \u2502   \u251c\u2500\u2500 pretrain_data/\n\u2502   \u2502   \u2514\u2500\u2500 distillation_data/\n\u2502   \u2502       \u251c\u2500\u2500 llama_outputs/\n\u2502   \u2502       \u251c\u2500\u2500 mistral_outputs/\n\u2502   \u2502       \u2514\u2500\u2500 teacher_ensemble_outputs/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 pseudo_labeled/\n\u2502   \u251c\u2500\u2500 selected_subsets/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 test_samples/\n\u2502   \u2502   \u251c\u2500\u2500 api_test_cases.json\n\u2502   \u2502   \u2514\u2500\u2500 mock_responses.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 metadata/\n\u2502   \u2502   \u251c\u2500\u2500 split_info.json\n\u2502   \u2502   \u251c\u2500\u2500 statistics.json\n\u2502   \u2502   \u251c\u2500\u2500 leakage_check.json\n\u2502   \u2502   \u2514\u2500\u2500 model_predictions/\n\u2502   \u2502       \u251c\u2500\u2500 xlarge_predictions.json\n\u2502   \u2502       \u251c\u2500\u2500 llm_predictions.json\n\u2502   \u2502       \u2514\u2500\u2500 ensemble_predictions.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 test_access_log.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 platform_cache/\n\u2502   \u2502   \u251c\u2500\u2500 colab_cache/\n\u2502   \u2502   \u251c\u2500\u2500 kaggle_cache/\n\u2502   \u2502   \u2514\u2500\u2500 local_cache/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 quota_tracking/\n\u2502   \u2502   \u251c\u2500\u2500 quota_history.json\n\u2502   \u2502   \u251c\u2500\u2500 session_logs.json\n\u2502   \u2502   \u2514\u2500\u2500 platform_usage.db\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 cache/\n\u2502       \u251c\u2500\u2500 local_cache/\n\u2502       \u251c\u2500\u2500 model_cache/\n\u2502       \u2514\u2500\u2500 huggingface_cache/\n\u2502\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 __version__.py\n\u2502   \u251c\u2500\u2500 cli.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 cli_commands/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 auto_train.py\n\u2502   \u2502   \u251c\u2500\u2500 choose_platform.py\n\u2502   \u2502   \u251c\u2500\u2500 check_quota.py\n\u2502   \u2502   \u2514\u2500\u2500 platform_info.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 registry.py\n\u2502   \u2502   \u251c\u2500\u2500 factory.py\n\u2502   \u2502   \u251c\u2500\u2500 types.py\n\u2502   \u2502   \u251c\u2500\u2500 exceptions.py\n\u2502   \u2502   \u251c\u2500\u2500 interfaces.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 health/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 health_checker.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dependency_checker.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 gpu_checker.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 config_checker.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 data_checker.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 auto_fix/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 config_fixer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dependency_fixer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cache_cleaner.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ide_sync_fixer.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 overfitting_prevention/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 validators/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 test_set_validator.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 config_validator.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 data_leakage_detector.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 hyperparameter_validator.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 split_validator.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 model_size_validator.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 lora_config_validator.py\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 ensemble_validator.py\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 monitors/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 training_monitor.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 overfitting_detector.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 complexity_monitor.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 benchmark_comparator.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 metrics_tracker.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 gradient_monitor.py\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 lora_rank_monitor.py\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 constraints/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 model_constraints.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 ensemble_constraints.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 augmentation_constraints.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 training_constraints.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 constraint_enforcer.py\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 parameter_efficiency_enforcer.py\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 guards/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 test_set_guard.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 validation_guard.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 experiment_guard.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 access_control.py\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 parameter_freeze_guard.py\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 recommendations/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 model_recommender.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 config_recommender.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 prevention_recommender.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 ensemble_recommender.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 lora_recommender.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 distillation_recommender.py\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 parameter_efficiency_recommender.py\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u251c\u2500\u2500 reporting/\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 overfitting_reporter.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 risk_scorer.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 comparison_reporter.py\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 html_report_generator.py\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 parameter_efficiency_reporter.py\n\u2502   \u2502       \u2502\n\u2502   \u2502       \u2514\u2500\u2500 utils/\n\u2502   \u2502           \u251c\u2500\u2500 __init__.py\n\u2502   \u2502           \u251c\u2500\u2500 hash_utils.py\n\u2502   \u2502           \u251c\u2500\u2500 statistical_tests.py\n\u2502   \u2502           \u2514\u2500\u2500 visualization_utils.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 deployment/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 platform_detector.py\n\u2502   \u2502   \u251c\u2500\u2500 smart_selector.py\n\u2502   \u2502   \u251c\u2500\u2500 cache_manager.py\n\u2502   \u2502   \u251c\u2500\u2500 checkpoint_manager.py\n\u2502   \u2502   \u251c\u2500\u2500 quota_tracker.py\n\u2502   \u2502   \u251c\u2500\u2500 storage_sync.py\n\u2502   \u2502   \u251c\u2500\u2500 session_manager.py\n\u2502   \u2502   \u2514\u2500\u2500 resource_monitor.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 base/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_handler.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 auth.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 rate_limiter.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 error_handler.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cors_handler.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 request_validator.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 rest/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 app.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 routers/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 classification.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 training.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 models.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 data.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 health.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 metrics.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 overfitting.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llm.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 platform.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 admin.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 schemas/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 request_schemas.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 response_schemas.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 error_schemas.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 common_schemas.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 middleware/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 logging_middleware.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 metrics_middleware.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 security_middleware.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dependencies.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 validators.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 websocket_handler.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 local/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 simple_api.py\n\u2502   \u2502       \u251c\u2500\u2500 batch_api.py\n\u2502   \u2502       \u2514\u2500\u2500 streaming_api.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base_service.py\n\u2502   \u2502   \u251c\u2500\u2500 service_registry.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prediction_service.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 training_service.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data_service.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_management_service.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 llm_service.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 local/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 local_cache_service.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 local_queue_service.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 file_storage_service.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 monitoring/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 monitoring_router.py\n\u2502   \u2502       \u251c\u2500\u2500 tensorboard_service.py\n\u2502   \u2502       \u251c\u2500\u2500 mlflow_service.py\n\u2502   \u2502       \u251c\u2500\u2500 wandb_service.py\n\u2502   \u2502       \u251c\u2500\u2500 local_metrics_service.py\n\u2502   \u2502       \u2514\u2500\u2500 logging_service.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 datasets/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ag_news.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 external_news.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 combined_dataset.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prompted_dataset.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 instruction_dataset.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 distillation_dataset.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 preprocessing/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 text_cleaner.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 tokenization.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 feature_extraction.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sliding_window.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt_formatter.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 instruction_formatter.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 augmentation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_augmenter.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 back_translation.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 paraphrase.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 token_replacement.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mixup.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cutmix.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 adversarial.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 contrast_set_generator.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 llm_augmenter/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 llama_augmenter.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 mistral_augmenter.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 controlled_generation.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 sampling/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 balanced_sampler.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 curriculum_sampler.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 active_learning.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 uncertainty_sampling.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 coreset_sampler.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 selection/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 influence_function.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 gradient_matching.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 diversity_selection.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 quality_filtering.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 validation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 split_strategies.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cross_validator.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 nested_cross_validator.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 holdout_manager.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 loaders/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 dataloader.py\n\u2502   \u2502       \u251c\u2500\u2500 dynamic_batching.py\n\u2502   \u2502       \u2514\u2500\u2500 prefetch_loader.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 base/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_model.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_wrapper.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 complexity_tracker.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 pooling_strategies.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 transformers/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v3_base.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v3_large.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v3_xlarge.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v2_xlarge.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_v2_xxlarge.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 deberta_sliding_window.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 deberta_hierarchical.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_base.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_large.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_large_mnli.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_enhanced.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 roberta_domain.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 xlm_roberta_large.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 electra/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 electra_base.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 electra_large.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 electra_discriminator.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 xlnet/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 xlnet_base.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 xlnet_large.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 xlnet_classifier.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 longformer/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 longformer_large.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 longformer_global.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 t5/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 t5_base.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 t5_large.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 t5_3b.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 flan_t5_xl.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 t5_classifier.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 llm/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 llama/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama2_7b.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama2_13b.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama2_70b.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama3_8b.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama3_70b.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 llama_for_classification.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral_7b.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral_7b_instruct.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mixtral_8x7b.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 mistral_for_classification.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 falcon/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 falcon_7b.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 falcon_40b.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 falcon_for_classification.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mpt/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mpt_7b.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mpt_30b.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 mpt_for_classification.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 phi/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 phi_2.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 phi_3.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 phi_for_classification.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 prompt_based/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt_model.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 soft_prompt.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 instruction_model.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 template_manager.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 efficient/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lora/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_model.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_config.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_layers.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_utils.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 rank_selection.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 target_modules_selector.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 qlora/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 qlora_model.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 qlora_config.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 quantization.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 dequantization.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 adapters/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adapter_model.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adapter_config.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 houlsby_adapter.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 pfeiffer_adapter.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 parallel_adapter.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adapter_fusion.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 adapter_stacking.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prefix_tuning/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 prefix_tuning_model.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 prefix_encoder.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 prefix_length_selector.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt_tuning/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 soft_prompt_model.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt_encoder.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 p_tuning_v2.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 prompt_initialization.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ia3/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ia3_model.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 quantization/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 int8_quantization.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 dynamic_quantization.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pruning/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 magnitude_pruning.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 combined/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 lora_plus_adapter.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 multi_method_model.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 ensemble/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_ensemble.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ensemble_selector.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 voting/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 soft_voting.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 hard_voting.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 weighted_voting.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 rank_averaging.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 confidence_weighted_voting.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 stacking/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 stacking_classifier.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 meta_learners.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cross_validation_stacking.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 neural_stacking.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 blending/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 blending_ensemble.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 dynamic_blending.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 advanced/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 bayesian_ensemble.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 snapshot_ensemble.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 multi_level_ensemble.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 mixture_of_experts.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 diversity/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 diversity_calculator.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 diversity_optimizer.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 ensemble_pruning.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 heads/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 classification_head.py\n\u2502   \u2502       \u251c\u2500\u2500 multitask_head.py\n\u2502   \u2502       \u251c\u2500\u2500 hierarchical_head.py\n\u2502   \u2502       \u251c\u2500\u2500 attention_head.py\n\u2502   \u2502       \u251c\u2500\u2500 prompt_head.py\n\u2502   \u2502       \u2514\u2500\u2500 adaptive_head.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 trainers/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 standard_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 distributed_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 apex_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 safe_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 auto_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 qlora_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 adapter_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 instruction_trainer.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 multi_stage_trainer.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 strategies/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 curriculum/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 curriculum_learning.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 self_paced.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 competence_based.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 adversarial/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 fgm.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 pgd.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 freelb.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 smart.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 regularization/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 r_drop.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mixout.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 spectral_norm.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adaptive_dropout.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 gradient_penalty.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 elastic_weight_consolidation.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 sharpness_aware_minimization.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 distillation/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 knowledge_distillation.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 feature_distillation.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 self_distillation.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 llama_distillation.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 mistral_distillation.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ensemble_distillation.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 progressive_distillation.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 meta/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 maml.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 reptile.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt_based/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 prompt_tuning.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 prefix_tuning.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 p_tuning.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 soft_prompt_tuning.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 tpu_training.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 adaptive_training.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 multi_stage/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 stage_manager.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 progressive_training.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 iterative_refinement.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 base_to_xlarge_progression.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 objectives/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 losses/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 focal_loss.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 label_smoothing.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 contrastive_loss.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 triplet_loss.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 custom_ce_loss.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 instruction_loss.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 distillation_loss.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 regularizers/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 l2_regularizer.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 gradient_penalty.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 complexity_regularizer.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 parameter_norm_regularizer.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 optimization/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 optimizers/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 adamw_custom.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lamb.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 lookahead.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 sam.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 adafactor.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 schedulers/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cosine_warmup.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 polynomial_decay.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 cyclic_scheduler.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 inverse_sqrt_scheduler.py\n\u2502   \u2502   \u2502   \u2502\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 gradient/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 gradient_accumulation.py\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 gradient_clipping.py\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 gradient_checkpointing.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 callbacks/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 early_stopping.py\n\u2502   \u2502       \u251c\u2500\u2500 model_checkpoint.py\n\u2502   \u2502       \u251c\u2500\u2500 tensorboard_logger.py\n\u2502   \u2502       \u251c\u2500\u2500 wandb_logger.py\n\u2502   \u2502       \u251c\u2500\u2500 mlflow_logger.py\n\u2502   \u2502       \u251c\u2500\u2500 learning_rate_monitor.py\n\u2502   \u2502       \u251c\u2500\u2500 overfitting_monitor.py\n\u2502   \u2502       \u251c\u2500\u2500 complexity_regularizer_callback.py\n\u2502   \u2502       \u251c\u2500\u2500 test_protection_callback.py\n\u2502   \u2502       \u251c\u2500\u2500 lora_rank_callback.py\n\u2502   \u2502       \u251c\u2500\u2500 memory_monitor_callback.py\n\u2502   \u2502       \u251c\u2500\u2500 colab_callback.py\n\u2502   \u2502       \u251c\u2500\u2500 kaggle_callback.py\n\u2502   \u2502       \u251c\u2500\u2500 platform_callback.py\n\u2502   \u2502       \u251c\u2500\u2500 quota_callback.py\n\u2502   \u2502       \u2514\u2500\u2500 session_callback.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 evaluation/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 metrics/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 classification_metrics.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 overfitting_metrics.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 diversity_metrics.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 efficiency_metrics.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 analysis/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 error_analysis.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 overfitting_analysis.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train_val_test_comparison.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_rank_analysis.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ensemble_analysis.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 visualizations/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 training_curves.py\n\u2502   \u2502       \u251c\u2500\u2500 confusion_matrix.py\n\u2502   \u2502       \u251c\u2500\u2500 attention_visualization.py\n\u2502   \u2502       \u2514\u2500\u2500 lora_weight_visualization.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 inference/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 predictors/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 single_predictor.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ensemble_predictor.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lora_predictor.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 qlora_predictor.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 optimization/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_quantization.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_pruning.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 onnx_export.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 openvino_optimization.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 serving/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 local_server.py\n\u2502   \u2502       \u251c\u2500\u2500 batch_predictor.py\n\u2502   \u2502       \u2514\u2500\u2500 streaming_predictor.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 io_utils.py\n\u2502       \u251c\u2500\u2500 logging_config.py\n\u2502       \u251c\u2500\u2500 reproducibility.py\n\u2502       \u251c\u2500\u2500 distributed_utils.py\n\u2502       \u251c\u2500\u2500 memory_utils.py\n\u2502       \u251c\u2500\u2500 profiling_utils.py\n\u2502       \u251c\u2500\u2500 experiment_tracking.py\n\u2502       \u251c\u2500\u2500 prompt_utils.py\n\u2502       \u251c\u2500\u2500 api_utils.py\n\u2502       \u251c\u2500\u2500 local_utils.py\n\u2502       \u251c\u2500\u2500 platform_utils.py\n\u2502       \u251c\u2500\u2500 resource_utils.py\n\u2502       \u2514\u2500\u2500 quota_utils.py\n\u2502\n\u251c\u2500\u2500 experiments/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 experiment_runner.py\n\u2502   \u251c\u2500\u2500 experiment_tagger.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 hyperparameter_search/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 optuna_search.py\n\u2502   \u2502   \u251c\u2500\u2500 ray_tune_search.py\n\u2502   \u2502   \u251c\u2500\u2500 hyperband.py\n\u2502   \u2502   \u251c\u2500\u2500 bayesian_optimization.py\n\u2502   \u2502   \u251c\u2500\u2500 lora_rank_search.py\n\u2502   \u2502   \u2514\u2500\u2500 ensemble_weight_search.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 benchmarks/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 speed_benchmark.py\n\u2502   \u2502   \u251c\u2500\u2500 memory_benchmark.py\n\u2502   \u2502   \u251c\u2500\u2500 accuracy_benchmark.py\n\u2502   \u2502   \u251c\u2500\u2500 robustness_benchmark.py\n\u2502   \u2502   \u251c\u2500\u2500 sota_comparison.py\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_benchmark.py\n\u2502   \u2502   \u2514\u2500\u2500 parameter_efficiency_benchmark.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 baselines/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 classical/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 naive_bayes.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 svm_baseline.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 random_forest.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 logistic_regression.py\n\u2502   \u2502   \u2514\u2500\u2500 neural/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 lstm_baseline.py\n\u2502   \u2502       \u251c\u2500\u2500 cnn_baseline.py\n\u2502   \u2502       \u2514\u2500\u2500 bert_vanilla.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 ablation_studies/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 component_ablation.py\n\u2502   \u2502   \u251c\u2500\u2500 data_ablation.py\n\u2502   \u2502   \u251c\u2500\u2500 model_size_ablation.py\n\u2502   \u2502   \u251c\u2500\u2500 feature_ablation.py\n\u2502   \u2502   \u251c\u2500\u2500 lora_rank_ablation.py\n\u2502   \u2502   \u251c\u2500\u2500 qlora_bits_ablation.py\n\u2502   \u2502   \u251c\u2500\u2500 regularization_ablation.py\n\u2502   \u2502   \u251c\u2500\u2500 prompt_ablation.py\n\u2502   \u2502   \u2514\u2500\u2500 distillation_temperature_ablation.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 sota_experiments/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 phase1_xlarge_lora.py\n\u2502   \u2502   \u251c\u2500\u2500 phase2_llm_qlora.py\n\u2502   \u2502   \u251c\u2500\u2500 phase3_llm_distillation.py\n\u2502   \u2502   \u251c\u2500\u2500 phase4_ensemble_xlarge.py\n\u2502   \u2502   \u251c\u2500\u2500 phase5_ultimate_sota.py\n\u2502   \u2502   \u251c\u2500\u2500 single_model_sota.py\n\u2502   \u2502   \u251c\u2500\u2500 ensemble_sota.py\n\u2502   \u2502   \u251c\u2500\u2500 full_pipeline_sota.py\n\u2502   \u2502   \u251c\u2500\u2500 production_sota.py\n\u2502   \u2502   \u251c\u2500\u2500 prompt_based_sota.py\n\u2502   \u2502   \u2514\u2500\u2500 compare_all_approaches.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 results/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 experiment_tracker.py\n\u2502       \u251c\u2500\u2500 result_aggregator.py\n\u2502       \u2514\u2500\u2500 leaderboard_generator.py\n\u2502\n\u251c\u2500\u2500 monitoring/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 local/\n\u2502   \u2502   \u251c\u2500\u2500 docker-compose.local.yml\n\u2502   \u2502   \u251c\u2500\u2500 tensorboard_config.yaml\n\u2502   \u2502   \u251c\u2500\u2500 mlflow_config.yaml\n\u2502   \u2502   \u2514\u2500\u2500 setup_local_monitoring.sh\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 dashboards/\n\u2502   \u2502   \u251c\u2500\u2500 tensorboard/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 scalar_config.json\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 image_config.json\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 custom_scalars.json\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 mlflow/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 experiment_dashboard.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 model_registry.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 wandb/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 training_dashboard.json\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 overfitting_dashboard.json\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 parameter_efficiency_dashboard.json\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 platform_dashboard.json\n\u2502   \u2502   \u2514\u2500\u2500 quota_dashboard.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 metrics/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 custom_metrics.py\n\u2502   \u2502   \u251c\u2500\u2500 metric_collectors.py\n\u2502   \u2502   \u251c\u2500\u2500 local_metrics.py\n\u2502   \u2502   \u251c\u2500\u2500 model_metrics.py\n\u2502   \u2502   \u251c\u2500\u2500 training_metrics.py\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_metrics.py\n\u2502   \u2502   \u251c\u2500\u2500 platform_metrics.py\n\u2502   \u2502   \u2514\u2500\u2500 quota_metrics.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 logs_analysis/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 log_parser.py\n\u2502   \u2502   \u251c\u2500\u2500 anomaly_detector.py\n\u2502   \u2502   \u2514\u2500\u2500 log_aggregator.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 scripts/\n\u2502       \u251c\u2500\u2500 start_tensorboard.sh\n\u2502       \u251c\u2500\u2500 start_mlflow.sh\n\u2502       \u251c\u2500\u2500 start_wandb.sh\n\u2502       \u251c\u2500\u2500 monitor_platform.sh\n\u2502       \u251c\u2500\u2500 export_metrics.py\n\u2502       \u251c\u2500\u2500 export_quota_metrics.py\n\u2502       \u2514\u2500\u2500 generate_report.py\n\u2502\n\u251c\u2500\u2500 security/\n\u2502   \u251c\u2500\u2500 local_auth/\n\u2502   \u2502   \u251c\u2500\u2500 simple_token.py\n\u2502   \u2502   \u2514\u2500\u2500 local_rbac.py\n\u2502   \u251c\u2500\u2500 data_privacy/\n\u2502   \u2502   \u251c\u2500\u2500 pii_detector.py\n\u2502   \u2502   \u2514\u2500\u2500 data_masking.py\n\u2502   \u2514\u2500\u2500 model_security/\n\u2502       \u251c\u2500\u2500 adversarial_defense.py\n\u2502       \u2514\u2500\u2500 model_checksum.py\n\u2502\n\u251c\u2500\u2500 plugins/\n\u2502   \u251c\u2500\u2500 custom_models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 plugin_interface.py\n\u2502   \u251c\u2500\u2500 data_sources/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 custom_loaders/\n\u2502   \u251c\u2500\u2500 evaluators/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 custom_metrics/\n\u2502   \u2514\u2500\u2500 processors/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 custom_preprocessors/\n\u2502\n\u251c\u2500\u2500 migrations/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 001_initial_schema.py\n\u2502   \u2502   \u2514\u2500\u2500 migration_runner.py\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 version_converter.py\n\u2502   \u2502   \u2514\u2500\u2500 compatibility_layer.py\n\u2502   \u2514\u2500\u2500 configs/\n\u2502       \u2514\u2500\u2500 config_migrator.py\n\u2502\n\u251c\u2500\u2500 cache/\n\u2502   \u251c\u2500\u2500 local/\n\u2502   \u2502   \u251c\u2500\u2500 disk_cache.py\n\u2502   \u2502   \u251c\u2500\u2500 memory_cache.py\n\u2502   \u2502   \u2514\u2500\u2500 lru_cache.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 sqlite/\n\u2502       \u2514\u2500\u2500 cache_db_schema.sql\n\u2502\n\u251c\u2500\u2500 backup/\n\u2502   \u251c\u2500\u2500 strategies/\n\u2502   \u2502   \u251c\u2500\u2500 incremental_backup.yaml\n\u2502   \u2502   \u2514\u2500\u2500 local_backup.yaml\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u251c\u2500\u2500 backup_local.sh\n\u2502   \u2502   \u2514\u2500\u2500 restore_local.sh\n\u2502   \u2514\u2500\u2500 recovery/\n\u2502       \u2514\u2500\u2500 local_recovery_plan.md\n\u2502\n\u251c\u2500\u2500 quickstart/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 SIMPLE_START.md\n\u2502   \u251c\u2500\u2500 setup_wizard.py\n\u2502   \u251c\u2500\u2500 interactive_cli.py\n\u2502   \u251c\u2500\u2500 decision_tree.py\n\u2502   \u251c\u2500\u2500 minimal_example.py\n\u2502   \u251c\u2500\u2500 train_simple.py\n\u2502   \u251c\u2500\u2500 evaluate_simple.py\n\u2502   \u251c\u2500\u2500 demo_app.py\n\u2502   \u251c\u2500\u2500 local_api_quickstart.py\n\u2502   \u251c\u2500\u2500 auto_start.py\n\u2502   \u251c\u2500\u2500 auto_train_demo.py\n\u2502   \u251c\u2500\u2500 colab_notebook.ipynb\n\u2502   \u251c\u2500\u2500 kaggle_notebook.ipynb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 use_cases/\n\u2502   \u2502   \u251c\u2500\u2500 quick_demo_5min.py\n\u2502   \u2502   \u251c\u2500\u2500 auto_demo_2min.py\n\u2502   \u2502   \u251c\u2500\u2500 research_experiment_30min.py\n\u2502   \u2502   \u251c\u2500\u2500 production_deployment_1hr.py\n\u2502   \u2502   \u251c\u2500\u2500 learning_exploration.py\n\u2502   \u2502   \u2514\u2500\u2500 platform_comparison_demo.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 docker_quickstart/\n\u2502       \u251c\u2500\u2500 Dockerfile.local\n\u2502       \u2514\u2500\u2500 docker-compose.local.yml\n\u2502\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 experiment/\n\u2502   \u2502   \u251c\u2500\u2500 experiment_template.py\n\u2502   \u2502   \u2514\u2500\u2500 config_template.yaml\n\u2502   \u251c\u2500\u2500 model/\n\u2502   \u2502   \u251c\u2500\u2500 model_template.py\n\u2502   \u2502   \u2514\u2500\u2500 README_template.md\n\u2502   \u251c\u2500\u2500 dataset/\n\u2502   \u2502   \u2514\u2500\u2500 dataset_template.py\n\u2502   \u251c\u2500\u2500 evaluation/\n\u2502   \u2502   \u2514\u2500\u2500 metric_template.py\n\u2502   \u2514\u2500\u2500 ide/\n\u2502       \u251c\u2500\u2500 pycharm_run_config.xml\n\u2502       \u251c\u2500\u2500 vscode_task.json\n\u2502       \u2514\u2500\u2500 jupyter_template.ipynb\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 setup/\n\u2502   \u2502   \u251c\u2500\u2500 download_all_data.py\n\u2502   \u2502   \u251c\u2500\u2500 setup_local_environment.sh\n\u2502   \u2502   \u251c\u2500\u2500 setup_platform.py\n\u2502   \u2502   \u251c\u2500\u2500 setup_colab.sh\n\u2502   \u2502   \u251c\u2500\u2500 setup_kaggle.sh\n\u2502   \u2502   \u251c\u2500\u2500 verify_installation.py\n\u2502   \u2502   \u251c\u2500\u2500 verify_dependencies.py\n\u2502   \u2502   \u251c\u2500\u2500 verify_platform.py\n\u2502   \u2502   \u251c\u2500\u2500 optimize_for_platform.sh\n\u2502   \u2502   \u2514\u2500\u2500 download_pretrained_models.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data_preparation/\n\u2502   \u2502   \u251c\u2500\u2500 prepare_ag_news.py\n\u2502   \u2502   \u251c\u2500\u2500 prepare_external_data.py\n\u2502   \u2502   \u251c\u2500\u2500 create_augmented_data.py\n\u2502   \u2502   \u251c\u2500\u2500 create_instruction_data.py\n\u2502   \u2502   \u251c\u2500\u2500 generate_with_llama.py\n\u2502   \u2502   \u251c\u2500\u2500 generate_with_mistral.py\n\u2502   \u2502   \u251c\u2500\u2500 generate_pseudo_labels.py\n\u2502   \u2502   \u251c\u2500\u2500 create_data_splits.py\n\u2502   \u2502   \u251c\u2500\u2500 generate_contrast_sets.py\n\u2502   \u2502   \u251c\u2500\u2500 select_quality_data.py\n\u2502   \u2502   \u251c\u2500\u2500 verify_data_splits.py\n\u2502   \u2502   \u2514\u2500\u2500 register_test_set.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 single_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train_xlarge_lora.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train_xxlarge_qlora.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train_llm_qlora.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 train_with_adapters.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 ensemble/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train_xlarge_ensemble.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 train_llm_ensemble.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 train_hybrid_ensemble.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 distillation/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 distill_from_llama.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 distill_from_mistral.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 distill_from_ensemble.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 progressive_distillation.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 instruction_tuning/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 instruction_tuning_llama.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 instruction_tuning_mistral.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 multi_stage/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_to_xlarge.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 pretrain_finetune_distill.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 auto_train.sh\n\u2502   \u2502   \u251c\u2500\u2500 train_all_models.sh\n\u2502   \u2502   \u251c\u2500\u2500 train_single_model.py\n\u2502   \u2502   \u251c\u2500\u2500 train_ensemble.py\n\u2502   \u2502   \u251c\u2500\u2500 train_local.py\n\u2502   \u2502   \u251c\u2500\u2500 resume_training.py\n\u2502   \u2502   \u2514\u2500\u2500 train_with_prompts.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 domain_adaptation/\n\u2502   \u2502   \u251c\u2500\u2500 pretrain_on_news.py\n\u2502   \u2502   \u251c\u2500\u2500 download_news_corpus.py\n\u2502   \u2502   \u2514\u2500\u2500 run_dapt.sh\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 evaluation/\n\u2502   \u2502   \u251c\u2500\u2500 evaluate_all_models.py\n\u2502   \u2502   \u251c\u2500\u2500 evaluate_with_guard.py\n\u2502   \u2502   \u251c\u2500\u2500 final_evaluation.py\n\u2502   \u2502   \u251c\u2500\u2500 generate_reports.py\n\u2502   \u2502   \u251c\u2500\u2500 create_leaderboard.py\n\u2502   \u2502   \u251c\u2500\u2500 check_overfitting.py\n\u2502   \u2502   \u251c\u2500\u2500 evaluate_parameter_efficiency.py\n\u2502   \u2502   \u251c\u2500\u2500 statistical_analysis.py\n\u2502   \u2502   \u2514\u2500\u2500 evaluate_contrast_sets.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 optimization/\n\u2502   \u2502   \u251c\u2500\u2500 hyperparameter_search.py\n\u2502   \u2502   \u251c\u2500\u2500 lora_rank_search.py\n\u2502   \u2502   \u251c\u2500\u2500 ensemble_optimization.py\n\u2502   \u2502   \u251c\u2500\u2500 quantization_optimization.py\n\u2502   \u2502   \u251c\u2500\u2500 architecture_search.py\n\u2502   \u2502   \u2514\u2500\u2500 prompt_optimization.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 deployment/\n\u2502   \u2502   \u251c\u2500\u2500 export_models.py\n\u2502   \u2502   \u251c\u2500\u2500 optimize_for_inference.py\n\u2502   \u2502   \u251c\u2500\u2500 create_docker_local.sh\n\u2502   \u2502   \u251c\u2500\u2500 deploy_to_local.py\n\u2502   \u2502   \u251c\u2500\u2500 deploy_auto.py\n\u2502   \u2502   \u2514\u2500\u2500 deploy_to_hf_spaces.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 overfitting_prevention/\n\u2502   \u2502   \u251c\u2500\u2500 get_model_recommendations.py\n\u2502   \u2502   \u251c\u2500\u2500 validate_experiment_config.py\n\u2502   \u2502   \u251c\u2500\u2500 check_data_leakage.py\n\u2502   \u2502   \u251c\u2500\u2500 monitor_training_live.py\n\u2502   \u2502   \u2514\u2500\u2500 generate_overfitting_report.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 platform/\n\u2502   \u2502   \u251c\u2500\u2500 colab/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mount_drive.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 setup_colab.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 keep_alive.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 kaggle/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 setup_kaggle.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 setup_tpu.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 create_dataset.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 local/\n\u2502   \u2502       \u251c\u2500\u2500 detect_gpu.py\n\u2502   \u2502       \u2514\u2500\u2500 optimize_local.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 monitoring/\n\u2502   \u2502   \u251c\u2500\u2500 monitor_quota.py\n\u2502   \u2502   \u2514\u2500\u2500 monitor_session.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 ide/\n\u2502   \u2502   \u251c\u2500\u2500 setup_pycharm.py\n\u2502   \u2502   \u251c\u2500\u2500 setup_vscode.py\n\u2502   \u2502   \u251c\u2500\u2500 setup_jupyter.py\n\u2502   \u2502   \u251c\u2500\u2500 setup_vim.py\n\u2502   \u2502   \u2514\u2500\u2500 setup_all_ides.sh\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 local/\n\u2502   \u2502   \u251c\u2500\u2500 start_local_api.sh\n\u2502   \u2502   \u251c\u2500\u2500 start_monitoring.sh\n\u2502   \u2502   \u251c\u2500\u2500 cleanup_cache.sh\n\u2502   \u2502   \u2514\u2500\u2500 backup_experiments.sh\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 ci/\n\u2502       \u251c\u2500\u2500 run_tests.sh\n\u2502       \u251c\u2500\u2500 run_benchmarks.sh\n\u2502       \u251c\u2500\u2500 build_docker_local.sh\n\u2502       \u251c\u2500\u2500 test_local_deployment.sh\n\u2502       \u251c\u2500\u2500 check_docs_sync.py\n\u2502       \u2514\u2500\u2500 verify_all.sh\n\u2502\n\u251c\u2500\u2500 prompts/\n\u2502   \u251c\u2500\u2500 classification/\n\u2502   \u2502   \u251c\u2500\u2500 zero_shot.txt\n\u2502   \u2502   \u251c\u2500\u2500 few_shot.txt\n\u2502   \u2502   \u2514\u2500\u2500 chain_of_thought.txt\n\u2502   \u251c\u2500\u2500 instruction/\n\u2502   \u2502   \u251c\u2500\u2500 base_instruction.txt\n\u2502   \u2502   \u251c\u2500\u2500 detailed_instruction.txt\n\u2502   \u2502   \u2514\u2500\u2500 task_specific.txt\n\u2502   \u2514\u2500\u2500 distillation/\n\u2502       \u251c\u2500\u2500 llm_prompts.txt\n\u2502       \u2514\u2500\u2500 explanation_prompts.txt\n\u2502\n\u251c\u2500\u2500 notebooks/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 00_setup/\n\u2502   \u2502   \u251c\u2500\u2500 00_auto_setup.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 00_local_setup.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 01_colab_setup.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 02_kaggle_setup.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 03_vscode_setup.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 04_pycharm_setup.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 05_jupyterlab_setup.ipynb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 01_tutorials/\n\u2502   \u2502   \u251c\u2500\u2500 00_auto_training_tutorial.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 00_environment_setup.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 01_data_loading_basics.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 02_preprocessing_tutorial.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 03_model_training_basics.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 04_lora_tutorial.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 05_qlora_tutorial.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 06_distillation_tutorial.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 07_ensemble_tutorial.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 08_overfitting_prevention.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 09_safe_training_workflow.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 10_evaluation_tutorial.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 11_prompt_engineering.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 12_instruction_tuning.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 13_local_api_usage.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 14_monitoring_setup.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 15_platform_optimization.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 16_quota_management.ipynb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 02_exploratory/\n\u2502   \u2502   \u251c\u2500\u2500 01_data_exploration.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 02_model_size_analysis.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 03_parameter_efficiency_analysis.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 04_data_statistics.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 05_label_distribution.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 06_text_length_analysis.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 07_vocabulary_analysis.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 08_contrast_set_exploration.ipynb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 03_experiments/\n\u2502   \u2502   \u251c\u2500\u2500 01_baseline_experiments.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 02_xlarge_lora_experiments.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 03_llm_qlora_experiments.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 04_ensemble_experiments.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 05_distillation_experiments.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 06_sota_experiments.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 07_ablation_studies.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 08_sota_reproduction.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 09_prompt_experiments.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 10_single_model_experiments.ipynb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 04_analysis/\n\u2502   \u2502   \u251c\u2500\u2500 01_error_analysis.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 02_overfitting_analysis.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 03_lora_rank_analysis.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 04_ensemble_diversity_analysis.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 05_parameter_efficiency_comparison.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 06_model_interpretability.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 07_attention_visualization.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 08_embedding_analysis.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 09_failure_cases.ipynb\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 05_deployment/\n\u2502   \u2502   \u251c\u2500\u2500 01_model_export.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 02_quantization.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 03_local_serving.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 04_model_optimization.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 05_inference_pipeline.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 06_api_demo.ipynb\n\u2502   \u2502   \u2514\u2500\u2500 07_hf_spaces_deploy.ipynb\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 06_platform_specific/\n\u2502       \u251c\u2500\u2500 local/\n\u2502       \u2502   \u251c\u2500\u2500 auto_training_local.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 cpu_training.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 gpu_training.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 multi_gpu_local.ipynb\n\u2502       \u2502   \u2514\u2500\u2500 inference_demo.ipynb\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 colab/\n\u2502       \u2502   \u251c\u2500\u2500 auto_training_colab.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 quick_start_colab.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 full_training_colab.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 drive_optimization.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 keep_alive_demo.ipynb\n\u2502       \u2502   \u2514\u2500\u2500 inference_demo_colab.ipynb\n\u2502       \u2502\n\u2502       \u251c\u2500\u2500 kaggle/\n\u2502       \u2502   \u251c\u2500\u2500 auto_training_kaggle.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 kaggle_submission.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 kaggle_training.ipynb\n\u2502       \u2502   \u251c\u2500\u2500 tpu_training.ipynb\n\u2502       \u2502   \u2514\u2500\u2500 dataset_caching.ipynb\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500 huggingface/\n\u2502           \u2514\u2500\u2500 spaces_demo.ipynb\n\u2502\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 streamlit_app.py\n\u2502   \u251c\u2500\u2500 gradio_app.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 pages/\n\u2502   \u2502   \u251c\u2500\u2500 01_Home.py\n\u2502   \u2502   \u251c\u2500\u2500 02_Single_Prediction.py\n\u2502   \u2502   \u251c\u2500\u2500 03_Batch_Analysis.py\n\u2502   \u2502   \u251c\u2500\u2500 04_Model_Comparison.py\n\u2502   \u2502   \u251c\u2500\u2500 05_Overfitting_Dashboard.py\n\u2502   \u2502   \u251c\u2500\u2500 06_Model_Recommender.py\n\u2502   \u2502   \u251c\u2500\u2500 07_Parameter_Efficiency_Dashboard.py\n\u2502   \u2502   \u251c\u2500\u2500 08_Interpretability.py\n\u2502   \u2502   \u251c\u2500\u2500 09_Performance_Dashboard.py\n\u2502   \u2502   \u251c\u2500\u2500 10_Real_Time_Demo.py\n\u2502   \u2502   \u251c\u2500\u2500 11_Model_Selection.py\n\u2502   \u2502   \u251c\u2500\u2500 12_Documentation.py\n\u2502   \u2502   \u251c\u2500\u2500 13_Prompt_Testing.py\n\u2502   \u2502   \u251c\u2500\u2500 14_Local_Monitoring.py\n\u2502   \u2502   \u251c\u2500\u2500 15_IDE_Setup_Guide.py\n\u2502   \u2502   \u251c\u2500\u2500 16_Experiment_Tracker.py\n\u2502   \u2502   \u251c\u2500\u2500 17_Platform_Info.py\n\u2502   \u2502   \u251c\u2500\u2500 18_Quota_Dashboard.py\n\u2502   \u2502   \u251c\u2500\u2500 19_Platform_Selector.py\n\u2502   \u2502   \u2514\u2500\u2500 20_Auto_Train_UI.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 prediction_component.py\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_monitor.py\n\u2502   \u2502   \u251c\u2500\u2500 lora_config_selector.py\n\u2502   \u2502   \u251c\u2500\u2500 ensemble_builder.py\n\u2502   \u2502   \u251c\u2500\u2500 visualization_component.py\n\u2502   \u2502   \u251c\u2500\u2500 model_selector.py\n\u2502   \u2502   \u251c\u2500\u2500 file_uploader.py\n\u2502   \u2502   \u251c\u2500\u2500 result_display.py\n\u2502   \u2502   \u251c\u2500\u2500 performance_monitor.py\n\u2502   \u2502   \u251c\u2500\u2500 prompt_builder.py\n\u2502   \u2502   \u251c\u2500\u2500 ide_configurator.py\n\u2502   \u2502   \u251c\u2500\u2500 platform_info_component.py\n\u2502   \u2502   \u251c\u2500\u2500 quota_monitor_component.py\n\u2502   \u2502   \u2514\u2500\u2500 resource_gauge.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 session_manager.py\n\u2502   \u2502   \u251c\u2500\u2500 caching.py\n\u2502   \u2502   \u251c\u2500\u2500 theming.py\n\u2502   \u2502   \u2514\u2500\u2500 helpers.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 assets/\n\u2502       \u251c\u2500\u2500 css/\n\u2502       \u2502   \u2514\u2500\u2500 custom.css\n\u2502       \u251c\u2500\u2500 js/\n\u2502       \u2502   \u2514\u2500\u2500 custom.js\n\u2502       \u2514\u2500\u2500 images/\n\u2502           \u251c\u2500\u2500 logo.png\n\u2502           \u2514\u2500\u2500 banner.png\n\u2502\n\u251c\u2500\u2500 outputs/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 checkpoints/\n\u2502   \u2502   \u251c\u2500\u2500 pretrained/\n\u2502   \u2502   \u251c\u2500\u2500 fine_tuned/\n\u2502   \u2502   \u251c\u2500\u2500 lora_adapters/\n\u2502   \u2502   \u251c\u2500\u2500 qlora_adapters/\n\u2502   \u2502   \u251c\u2500\u2500 ensembles/\n\u2502   \u2502   \u251c\u2500\u2500 distilled/\n\u2502   \u2502   \u251c\u2500\u2500 optimized/\n\u2502   \u2502   \u251c\u2500\u2500 exported/\n\u2502   \u2502   \u2514\u2500\u2500 prompted/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 results/\n\u2502   \u2502   \u251c\u2500\u2500 experiments/\n\u2502   \u2502   \u251c\u2500\u2500 benchmarks/\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_reports/\n\u2502   \u2502   \u251c\u2500\u2500 parameter_efficiency_reports/\n\u2502   \u2502   \u251c\u2500\u2500 ablations/\n\u2502   \u2502   \u2514\u2500\u2500 reports/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 analysis/\n\u2502   \u2502   \u251c\u2500\u2500 error_analysis/\n\u2502   \u2502   \u251c\u2500\u2500 interpretability/\n\u2502   \u2502   \u2514\u2500\u2500 statistical/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 logs/\n\u2502   \u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u251c\u2500\u2500 tensorboard/\n\u2502   \u2502   \u251c\u2500\u2500 mlflow/\n\u2502   \u2502   \u251c\u2500\u2500 wandb/\n\u2502   \u2502   \u2514\u2500\u2500 local/\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 profiling/\n\u2502   \u2502   \u251c\u2500\u2500 memory/\n\u2502   \u2502   \u251c\u2500\u2500 speed/\n\u2502   \u2502   \u2514\u2500\u2500 traces/\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 artifacts/\n\u2502       \u251c\u2500\u2500 figures/\n\u2502       \u251c\u2500\u2500 tables/\n\u2502       \u251c\u2500\u2500 lora_visualizations/\n\u2502       \u2514\u2500\u2500 presentations/\n\u2502\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 00_START_HERE.md\n\u2502   \u251c\u2500\u2500 limitations.md\n\u2502   \u251c\u2500\u2500 ethical_considerations.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 getting_started/\n\u2502   \u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u2502   \u251c\u2500\u2500 local_setup.md\n\u2502   \u2502   \u251c\u2500\u2500 ide_setup.md\n\u2502   \u2502   \u251c\u2500\u2500 quickstart.md\n\u2502   \u2502   \u251c\u2500\u2500 auto_mode.md\n\u2502   \u2502   \u251c\u2500\u2500 platform_detection.md\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_prevention_quickstart.md\n\u2502   \u2502   \u251c\u2500\u2500 choosing_model.md\n\u2502   \u2502   \u251c\u2500\u2500 choosing_platform.md\n\u2502   \u2502   \u251c\u2500\u2500 free_deployment.md\n\u2502   \u2502   \u2514\u2500\u2500 troubleshooting.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 level_1_beginner/\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 01_installation.md\n\u2502   \u2502   \u251c\u2500\u2500 02_first_model.md\n\u2502   \u2502   \u251c\u2500\u2500 03_evaluation.md\n\u2502   \u2502   \u251c\u2500\u2500 04_deployment.md\n\u2502   \u2502   \u2514\u2500\u2500 quick_demo.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 level_2_intermediate/\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 01_lora_qlora.md\n\u2502   \u2502   \u251c\u2500\u2500 02_ensemble.md\n\u2502   \u2502   \u251c\u2500\u2500 03_distillation.md\n\u2502   \u2502   \u2514\u2500\u2500 04_optimization.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 level_3_advanced/\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 01_sota_pipeline.md\n\u2502   \u2502   \u251c\u2500\u2500 02_custom_models.md\n\u2502   \u2502   \u2514\u2500\u2500 03_research_workflow.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 platform_guides/\n\u2502   \u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2502   \u251c\u2500\u2500 colab_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 colab_advanced.md\n\u2502   \u2502   \u251c\u2500\u2500 kaggle_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 kaggle_tpu.md\n\u2502   \u2502   \u251c\u2500\u2500 local_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 gitpod_guide.md\n\u2502   \u2502   \u2514\u2500\u2500 platform_comparison.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 user_guide/\n\u2502   \u2502   \u251c\u2500\u2500 data_preparation.md\n\u2502   \u2502   \u251c\u2500\u2500 model_training.md\n\u2502   \u2502   \u251c\u2500\u2500 auto_training.md\n\u2502   \u2502   \u251c\u2500\u2500 lora_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 qlora_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 distillation_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 ensemble_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_prevention.md\n\u2502   \u2502   \u251c\u2500\u2500 safe_training_practices.md\n\u2502   \u2502   \u251c\u2500\u2500 evaluation.md\n\u2502   \u2502   \u251c\u2500\u2500 local_deployment.md\n\u2502   \u2502   \u251c\u2500\u2500 quota_management.md\n\u2502   \u2502   \u251c\u2500\u2500 platform_optimization.md\n\u2502   \u2502   \u251c\u2500\u2500 prompt_engineering.md\n\u2502   \u2502   \u2514\u2500\u2500 advanced_techniques.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 developer_guide/\n\u2502   \u2502   \u251c\u2500\u2500 architecture.md\n\u2502   \u2502   \u251c\u2500\u2500 adding_models.md\n\u2502   \u2502   \u251c\u2500\u2500 custom_datasets.md\n\u2502   \u2502   \u251c\u2500\u2500 local_api_development.md\n\u2502   \u2502   \u2514\u2500\u2500 contributing.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 api_reference/\n\u2502   \u2502   \u251c\u2500\u2500 rest_api.md\n\u2502   \u2502   \u251c\u2500\u2500 data_api.md\n\u2502   \u2502   \u251c\u2500\u2500 models_api.md\n\u2502   \u2502   \u251c\u2500\u2500 training_api.md\n\u2502   \u2502   \u251c\u2500\u2500 lora_api.md\n\u2502   \u2502   \u251c\u2500\u2500 ensemble_api.md\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_prevention_api.md\n\u2502   \u2502   \u251c\u2500\u2500 platform_api.md\n\u2502   \u2502   \u251c\u2500\u2500 quota_api.md\n\u2502   \u2502   \u2514\u2500\u2500 evaluation_api.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 ide_guides/\n\u2502   \u2502   \u251c\u2500\u2500 vscode_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 pycharm_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 jupyter_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 vim_guide.md\n\u2502   \u2502   \u251c\u2500\u2500 sublime_guide.md\n\u2502   \u2502   \u2514\u2500\u2500 comparison.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 tutorials/\n\u2502   \u2502   \u251c\u2500\u2500 basic_usage.md\n\u2502   \u2502   \u251c\u2500\u2500 xlarge_model_tutorial.md\n\u2502   \u2502   \u251c\u2500\u2500 llm_tutorial.md\n\u2502   \u2502   \u251c\u2500\u2500 distillation_tutorial.md\n\u2502   \u2502   \u251c\u2500\u2500 sota_pipeline_tutorial.md\n\u2502   \u2502   \u251c\u2500\u2500 local_training_tutorial.md\n\u2502   \u2502   \u251c\u2500\u2500 free_deployment_tutorial.md\n\u2502   \u2502   \u2514\u2500\u2500 best_practices.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 best_practices/\n\u2502   \u2502   \u251c\u2500\u2500 model_selection.md\n\u2502   \u2502   \u251c\u2500\u2500 parameter_efficient_finetuning.md\n\u2502   \u2502   \u251c\u2500\u2500 avoiding_overfitting.md\n\u2502   \u2502   \u251c\u2500\u2500 local_optimization.md\n\u2502   \u2502   \u2514\u2500\u2500 ensemble_building.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 examples/\n\u2502   \u2502   \u251c\u2500\u2500 00_hello_world.md\n\u2502   \u2502   \u251c\u2500\u2500 01_train_baseline.md\n\u2502   \u2502   \u251c\u2500\u2500 02_sota_pipeline.md\n\u2502   \u2502   \u2514\u2500\u2500 03_custom_model.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 cheatsheets/\n\u2502   \u2502   \u251c\u2500\u2500 model_selection_cheatsheet.pdf\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_prevention_checklist.pdf\n\u2502   \u2502   \u251c\u2500\u2500 free_deployment_comparison.pdf\n\u2502   \u2502   \u251c\u2500\u2500 platform_comparison_chart.pdf\n\u2502   \u2502   \u251c\u2500\u2500 auto_train_cheatsheet.pdf\n\u2502   \u2502   \u251c\u2500\u2500 quota_limits_reference.pdf\n\u2502   \u2502   \u2514\u2500\u2500 cli_commands.pdf\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 troubleshooting/\n\u2502   \u2502   \u251c\u2500\u2500 platform_issues.md\n\u2502   \u2502   \u2514\u2500\u2500 quota_issues.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 architecture/\n\u2502   \u2502   \u251c\u2500\u2500 decisions/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 001-model-selection.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 002-ensemble-strategy.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 003-local-first-design.md\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 004-overfitting-prevention.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 005-parameter-efficiency.md\n\u2502   \u2502   \u251c\u2500\u2500 diagrams/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 system-overview.puml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data-flow.puml\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 local-deployment.puml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 overfitting-prevention-flow.puml\n\u2502   \u2502   \u2514\u2500\u2500 patterns/\n\u2502   \u2502       \u251c\u2500\u2500 factory-pattern.md\n\u2502   \u2502       \u2514\u2500\u2500 strategy-pattern.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 operations/\n\u2502   \u2502   \u251c\u2500\u2500 runbooks/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 local_deployment.md\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 troubleshooting.md\n\u2502   \u2502   \u2514\u2500\u2500 sops/\n\u2502   \u2502       \u251c\u2500\u2500 model-update.md\n\u2502   \u2502       \u2514\u2500\u2500 data-refresh.md\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 _static/\n\u2502       \u2514\u2500\u2500 custom.css\n\u2502\n\u251c\u2500\u2500 deployment/\n\u2502   \u251c\u2500\u2500 docker/\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile.local\n\u2502   \u2502   \u251c\u2500\u2500 Dockerfile.gpu.local\n\u2502   \u2502   \u251c\u2500\u2500 docker-compose.local.yml\n\u2502   \u2502   \u2514\u2500\u2500 .dockerignore\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 auto_deploy/\n\u2502   \u2502   \u251c\u2500\u2500 auto_deploy.py\n\u2502   \u2502   \u251c\u2500\u2500 platform_deploy.sh\n\u2502   \u2502   \u2514\u2500\u2500 README.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 platform_specific/\n\u2502   \u2502   \u251c\u2500\u2500 colab_deploy.md\n\u2502   \u2502   \u251c\u2500\u2500 kaggle_deploy.md\n\u2502   \u2502   \u2514\u2500\u2500 local_deploy.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 huggingface/\n\u2502   \u2502   \u251c\u2500\u2500 spaces_config.yaml\n\u2502   \u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2502   \u251c\u2500\u2500 app.py\n\u2502   \u2502   \u2514\u2500\u2500 README.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 streamlit_cloud/\n\u2502   \u2502   \u251c\u2500\u2500 .streamlit/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 config.toml\n\u2502   \u2502   \u2514\u2500\u2500 requirements.txt\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 local/\n\u2502       \u251c\u2500\u2500 systemd/\n\u2502       \u2502   \u251c\u2500\u2500 ag-news-api.service\n\u2502       \u2502   \u2514\u2500\u2500 ag-news-monitor.service\n\u2502       \u251c\u2500\u2500 nginx/\n\u2502       \u2502   \u2514\u2500\u2500 ag-news.conf\n\u2502       \u2514\u2500\u2500 scripts/\n\u2502           \u251c\u2500\u2500 start_all.sh\n\u2502           \u2514\u2500\u2500 stop_all.sh\n\u2502\n\u251c\u2500\u2500 benchmarks/\n\u2502   \u251c\u2500\u2500 accuracy/\n\u2502   \u2502   \u251c\u2500\u2500 model_comparison.json\n\u2502   \u2502   \u251c\u2500\u2500 xlarge_models.json\n\u2502   \u2502   \u251c\u2500\u2500 llm_models.json\n\u2502   \u2502   \u251c\u2500\u2500 ensemble_results.json\n\u2502   \u2502   \u2514\u2500\u2500 sota_benchmarks.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 efficiency/\n\u2502   \u2502   \u251c\u2500\u2500 parameter_efficiency.json\n\u2502   \u2502   \u251c\u2500\u2500 memory_usage.json\n\u2502   \u2502   \u251c\u2500\u2500 training_time.json\n\u2502   \u2502   \u251c\u2500\u2500 inference_speed.json\n\u2502   \u2502   \u2514\u2500\u2500 platform_comparison.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 robustness/\n\u2502   \u2502   \u251c\u2500\u2500 adversarial_results.json\n\u2502   \u2502   \u251c\u2500\u2500 ood_detection.json\n\u2502   \u2502   \u2514\u2500\u2500 contrast_set_results.json\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 overfitting/\n\u2502       \u251c\u2500\u2500 train_val_gaps.json\n\u2502       \u251c\u2500\u2500 lora_ranks.json\n\u2502       \u2514\u2500\u2500 prevention_effectiveness.json\n\u2502\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 unit/\n\u2502   \u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_preprocessing.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_augmentation.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_dataloader.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_contrast_sets.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_transformers.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_ensemble.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_efficient.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_prompt_models.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_trainers.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_auto_trainer.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_strategies.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_callbacks.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_multi_stage.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 deployment/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_platform_detector.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_smart_selector.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_cache_manager.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_checkpoint_manager.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_quota_tracker.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_rest_api.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_local_api.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_auth.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_prevention/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_validators.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_monitors.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_constraints.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_guards.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_recommenders.py\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 utils/\n\u2502   \u2502       \u251c\u2500\u2500 test_memory_utils.py\n\u2502   \u2502       \u2514\u2500\u2500 test_utilities.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 integration/\n\u2502   \u2502   \u251c\u2500\u2500 test_full_pipeline.py\n\u2502   \u2502   \u251c\u2500\u2500 test_auto_train_flow.py\n\u2502   \u2502   \u251c\u2500\u2500 test_ensemble_pipeline.py\n\u2502   \u2502   \u251c\u2500\u2500 test_inference_pipeline.py\n\u2502   \u2502   \u251c\u2500\u2500 test_local_api_flow.py\n\u2502   \u2502   \u251c\u2500\u2500 test_prompt_pipeline.py\n\u2502   \u2502   \u251c\u2500\u2500 test_llm_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_platform_workflows.py\n\u2502   \u2502   \u251c\u2500\u2500 test_quota_tracking_flow.py\n\u2502   \u2502   \u2514\u2500\u2500 test_overfitting_prevention_flow.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 platform_specific/\n\u2502   \u2502   \u251c\u2500\u2500 test_colab_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_kaggle_integration.py\n\u2502   \u2502   \u2514\u2500\u2500 test_local_integration.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 performance/\n\u2502   \u2502   \u251c\u2500\u2500 test_model_speed.py\n\u2502   \u2502   \u251c\u2500\u2500 test_memory_usage.py\n\u2502   \u2502   \u251c\u2500\u2500 test_accuracy_benchmarks.py\n\u2502   \u2502   \u251c\u2500\u2500 test_local_performance.py\n\u2502   \u2502   \u251c\u2500\u2500 test_sla_compliance.py\n\u2502   \u2502   \u2514\u2500\u2500 test_throughput.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 e2e/\n\u2502   \u2502   \u251c\u2500\u2500 test_complete_workflow.py\n\u2502   \u2502   \u251c\u2500\u2500 test_user_scenarios.py\n\u2502   \u2502   \u251c\u2500\u2500 test_local_deployment.py\n\u2502   \u2502   \u251c\u2500\u2500 test_free_deployment.py\n\u2502   \u2502   \u251c\u2500\u2500 test_quickstart_pipeline.py\n\u2502   \u2502   \u251c\u2500\u2500 test_sota_pipeline.py\n\u2502   \u2502   \u251c\u2500\u2500 test_auto_train_colab.py\n\u2502   \u2502   \u251c\u2500\u2500 test_auto_train_kaggle.py\n\u2502   \u2502   \u2514\u2500\u2500 test_quota_enforcement.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 regression/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 test_model_accuracy.py\n\u2502   \u2502   \u251c\u2500\u2500 test_ensemble_diversity.py\n\u2502   \u2502   \u251c\u2500\u2500 test_inference_speed.py\n\u2502   \u2502   \u2514\u2500\u2500 baseline_results.json\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 chaos/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 test_fault_tolerance.py\n\u2502   \u2502   \u251c\u2500\u2500 test_corrupted_config.py\n\u2502   \u2502   \u251c\u2500\u2500 test_oom_handling.py\n\u2502   \u2502   \u2514\u2500\u2500 test_network_failures.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 compatibility/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 test_torch_versions.py\n\u2502   \u2502   \u251c\u2500\u2500 test_transformers_versions.py\n\u2502   \u2502   \u2514\u2500\u2500 test_cross_platform.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 fixtures/\n\u2502       \u251c\u2500\u2500 sample_data.py\n\u2502       \u251c\u2500\u2500 mock_models.py\n\u2502       \u251c\u2500\u2500 test_configs.py\n\u2502       \u2514\u2500\u2500 local_fixtures.py\n\u2502\n\u251c\u2500\u2500 .github/\n\u2502   \u251c\u2500\u2500 workflows/\n\u2502   \u2502   \u251c\u2500\u2500 ci.yml\n\u2502   \u2502   \u251c\u2500\u2500 tests.yml\n\u2502   \u2502   \u251c\u2500\u2500 documentation.yml\n\u2502   \u2502   \u251c\u2500\u2500 benchmarks.yml\n\u2502   \u2502   \u251c\u2500\u2500 overfitting_checks.yml\n\u2502   \u2502   \u251c\u2500\u2500 docs_sync_check.yml\n\u2502   \u2502   \u251c\u2500\u2500 local_deployment_test.yml\n\u2502   \u2502   \u251c\u2500\u2500 dependency_updates.yml\n\u2502   \u2502   \u251c\u2500\u2500 compatibility_matrix.yml\n\u2502   \u2502   \u251c\u2500\u2500 regression_tests.yml\n\u2502   \u2502   \u251c\u2500\u2500 test_platform_detection.yml\n\u2502   \u2502   \u251c\u2500\u2500 test_auto_train.yml\n\u2502   \u2502   \u2514\u2500\u2500 platform_compatibility.yml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 ISSUE_TEMPLATE/\n\u2502   \u2502   \u251c\u2500\u2500 bug_report.md\n\u2502   \u2502   \u251c\u2500\u2500 feature_request.md\n\u2502   \u2502   \u251c\u2500\u2500 ide_support_request.md\n\u2502   \u2502   \u2514\u2500\u2500 overfitting_report.md\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 PULL_REQUEST_TEMPLATE.md\n\u2502   \u2514\u2500\u2500 dependabot.yml\n\u2502\n\u2514\u2500\u2500 tools/\n    \u2502\n    \u251c\u2500\u2500 profiling/\n    \u2502   \u251c\u2500\u2500 memory_profiler.py\n    \u2502   \u251c\u2500\u2500 speed_profiler.py\n    \u2502   \u251c\u2500\u2500 parameter_counter.py\n    \u2502   \u2514\u2500\u2500 local_profiler.py\n    \u2502\n    \u251c\u2500\u2500 debugging/\n    \u2502   \u251c\u2500\u2500 model_debugger.py\n    \u2502   \u251c\u2500\u2500 overfitting_debugger.py\n    \u2502   \u251c\u2500\u2500 lora_debugger.py\n    \u2502   \u251c\u2500\u2500 data_validator.py\n    \u2502   \u251c\u2500\u2500 platform_debugger.py\n    \u2502   \u251c\u2500\u2500 quota_debugger.py\n    \u2502   \u2514\u2500\u2500 local_debugger.py\n    \u2502\n    \u251c\u2500\u2500 visualization/\n    \u2502   \u251c\u2500\u2500 training_monitor.py\n    \u2502   \u251c\u2500\u2500 lora_weight_plotter.py\n    \u2502   \u251c\u2500\u2500 ensemble_diversity_plotter.py\n    \u2502   \u2514\u2500\u2500 result_plotter.py\n    \u2502\n    \u251c\u2500\u2500 config_tools/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 config_generator.py\n    \u2502   \u251c\u2500\u2500 config_explainer.py\n    \u2502   \u251c\u2500\u2500 config_comparator.py\n    \u2502   \u251c\u2500\u2500 config_optimizer.py\n    \u2502   \u251c\u2500\u2500 sync_manager.py\n    \u2502   \u251c\u2500\u2500 auto_sync.sh\n    \u2502   \u2514\u2500\u2500 validate_all_configs.py\n    \u2502\n    \u251c\u2500\u2500 platform_tools/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 detector_tester.py\n    \u2502   \u251c\u2500\u2500 quota_simulator.py\n    \u2502   \u2514\u2500\u2500 platform_benchmark.py\n    \u2502\n    \u251c\u2500\u2500 cost_tools/\n    \u2502   \u251c\u2500\u2500 cost_estimator.py\n    \u2502   \u2514\u2500\u2500 cost_comparator.py\n    \u2502\n    \u251c\u2500\u2500 ide_tools/\n    \u2502   \u251c\u2500\u2500 pycharm_config_generator.py\n    \u2502   \u251c\u2500\u2500 vscode_tasks_generator.py\n    \u2502   \u251c\u2500\u2500 jupyter_kernel_setup.py\n    \u2502   \u251c\u2500\u2500 vim_plugin_installer.sh\n    \u2502   \u251c\u2500\u2500 universal_ide_generator.py\n    \u2502   \u2514\u2500\u2500 sync_ide_configs.py\n    \u2502\n    \u251c\u2500\u2500 compatibility/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 compatibility_checker.py\n    \u2502   \u251c\u2500\u2500 version_matrix_tester.py\n    \u2502   \u2514\u2500\u2500 upgrade_path_finder.py\n    \u2502\n    \u251c\u2500\u2500 automation/\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 health_check_runner.py\n    \u2502   \u251c\u2500\u2500 auto_fix_runner.py\n    \u2502   \u251c\u2500\u2500 batch_config_generator.py\n    \u2502   \u251c\u2500\u2500 platform_health.py\n    \u2502   \u2514\u2500\u2500 nightly_tasks.sh\n    \u2502\n    \u2514\u2500\u2500 cli_helpers/\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 rich_console.py\n        \u251c\u2500\u2500 progress_bars.py\n        \u251c\u2500\u2500 interactive_prompts.py\n        \u2514\u2500\u2500 ascii_art.py\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"ARCHITECTURE/","title":"ARCHITECTURE","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"ARCHITECTURE/#overview","title":"Overview","text":"<p>Comprehensive guide for ARCHITECTURE.</p>"},{"location":"ARCHITECTURE/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"ARCHITECTURE/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"ARCHITECTURE/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"CHANGELOG/","title":"AG News Text Classification - Changelog","text":"<p>All notable changes to the AG News Text Classification project will be documented in this file.</p> <p>The format is based on Keep a Changelog (https://keepachangelog.com/en/1.0.0/), and this project adheres to Semantic Versioning (https://semver.org/spec/v2.0.0.html).</p>"},{"location":"CHANGELOG/#project-information","title":"Project Information","text":"<ul> <li>Project Name: AG News Text Classification (ag-news-text-classification)</li> <li>Author: V\u00f5 H\u1ea3i D\u0169ng</li> <li>Email: vohaidung.work@gmail.com</li> <li>License: MIT</li> <li>Repository: https://github.com/VoHaiDung/ag-news-text-classification</li> </ul>"},{"location":"CHANGELOG/#version-history","title":"Version History","text":""},{"location":"CHANGELOG/#unreleased","title":"[Unreleased]","text":""},{"location":"CHANGELOG/#planned-features","title":"Planned Features","text":"<ul> <li>Multi-language support for non-English news classification</li> <li>Real-time learning pipeline for continuous model improvement</li> <li>AutoML integration for automated hyperparameter optimization</li> <li>Federated learning support for privacy-preserving distributed training</li> <li>Mobile deployment optimization (TensorFlow Lite, Core ML, ONNX Mobile)</li> <li>Active learning pipeline for efficient data labeling strategies</li> <li>Graph neural networks for hierarchical topic modeling</li> <li>Temporal analysis for news trend detection and evolution</li> <li>Cross-dataset transfer learning for domain adaptation</li> <li>Online learning capabilities for streaming data</li> </ul>"},{"location":"CHANGELOG/#under-research","title":"Under Research","text":"<ul> <li>GPT-4 based knowledge distillation for compact student models</li> <li>Chain-of-thought prompting for enhanced interpretability</li> <li>Constitutional AI techniques for bias detection and mitigation</li> <li>Retrieval-augmented generation for context-aware classification</li> <li>Multi-modal learning combining text, images, and metadata</li> <li>Zero-shot learning for emerging news categories</li> <li>Few-shot learning with prototypical networks and matching networks</li> <li>Continual learning without catastrophic forgetting</li> <li>Adversarial training for improved robustness</li> <li>Neural architecture search for optimal model design</li> </ul>"},{"location":"CHANGELOG/#100-2025-09-19","title":"[1.0.0] - 2025-09-19","text":""},{"location":"CHANGELOG/#initial-release","title":"Initial Release","text":"<p>This is the first stable release of the AG News Text Classification framework, representing comprehensive research and development work in text classification, overfitting prevention, and parameter-efficient fine-tuning. The project achieves state-of-the-art performance on the AG News dataset while maintaining strong generalization through advanced prevention mechanisms.</p>"},{"location":"CHANGELOG/#added","title":"Added","text":""},{"location":"CHANGELOG/#core-framework-architecture","title":"Core Framework Architecture","text":""},{"location":"CHANGELOG/#project-structure","title":"Project Structure","text":"<ul> <li>Comprehensive project organization with 15 top-level directories</li> <li>Modular architecture implementing separation of concerns principle</li> <li>Type-annotated codebase with comprehensive docstrings following Google style</li> <li>Centralized error handling with custom exception hierarchy</li> <li>Structured logging system using loguru with rotation and retention policies</li> <li>Health check system validating dependencies, GPU availability, and configuration</li> <li>Auto-fix utilities for automatic resolution of common configuration issues</li> <li>Command-line interface with typer and rich for enhanced user experience</li> <li>Interactive setup wizard with platform detection and optimal configuration</li> </ul>"},{"location":"CHANGELOG/#configuration-system","title":"Configuration System","text":"<ul> <li>Hierarchical YAML-based configuration with 300+ files</li> <li>Configuration validation using Pydantic schemas</li> <li>Template-based configuration generation with Jinja2</li> <li>Configuration loader with environment variable substitution</li> <li>Smart defaults system adapting to platform and resources</li> <li>Feature flags for experimental functionality</li> <li>Environment-specific configurations (dev, local_prod, colab, kaggle)</li> <li>Configuration compatibility matrix ensuring valid combinations</li> <li>Constants management for magic number elimination</li> <li>Secrets management with template-based approach</li> </ul>"},{"location":"CHANGELOG/#data-processing-pipeline","title":"Data Processing Pipeline","text":""},{"location":"CHANGELOG/#dataset-management","title":"Dataset Management","text":"<ul> <li>AG News dataset loader with automatic download and caching</li> <li>Dataset wrapper with stratified sampling and balancing</li> <li>External news corpus integration for domain-adaptive pretraining</li> <li>Combined dataset support for multi-source training</li> <li>Prompted dataset wrapper for few-shot learning scenarios</li> <li>Instruction-formatted dataset for instruction-tuned LLMs</li> <li>Distillation dataset with teacher model soft labels</li> <li>Platform-specific caching strategies (Drive for Colab, Datasets for Kaggle)</li> </ul>"},{"location":"CHANGELOG/#preprocessing","title":"Preprocessing","text":"<ul> <li>Text cleaning with HTML tag removal and normalization</li> <li>Tokenization with HuggingFace tokenizers and caching</li> <li>Feature extraction for classical ML models</li> <li>Sliding window approach for long document handling</li> <li>Prompt formatting for zero-shot and few-shot scenarios</li> <li>Instruction formatting following Alpaca, Dolly, and Vicuna templates</li> <li>Tokenization statistics and vocabulary analysis</li> </ul>"},{"location":"CHANGELOG/#data-augmentation","title":"Data Augmentation","text":"<ul> <li>Back-translation using MarianMT models with multiple language pivots</li> <li>Paraphrasing with T5 and PEGASUS models</li> <li>Token-level augmentation (synonym replacement, random insertion/deletion/swap)</li> <li>Contextual word embeddings for synonym generation</li> <li>Sentence-level mixup and manifold mixup</li> <li>Cutmix and cutout for text sequences</li> <li>Adversarial augmentation with gradient-based perturbations</li> <li>Contrast set generation for robustness evaluation</li> <li>LLM-based augmentation using LLaMA and Mistral</li> <li>Controlled generation with temperature and top-p sampling</li> <li>Quality filtering using perplexity and semantic similarity</li> <li>Diversity enforcement through nucleus sampling</li> <li>Augmentation constraints preventing over-augmentation</li> </ul>"},{"location":"CHANGELOG/#data-validation-and-quality-control","title":"Data Validation and Quality Control","text":"<ul> <li>Stratified data splitting with reproducible random seeds</li> <li>Cross-validation strategies (k-fold, stratified k-fold, nested CV)</li> <li>Holdout validation set management</li> <li>Time-based splitting for temporal datasets</li> <li>Data leakage detection using statistical tests</li> <li>Test set protection with SHA-256 hashing</li> <li>Test access logging and auditing</li> <li>Split information metadata tracking</li> <li>Data quality metrics (completeness, consistency, validity)</li> </ul>"},{"location":"CHANGELOG/#data-selection-and-sampling","title":"Data Selection and Sampling","text":"<ul> <li>Coreset selection using k-center greedy algorithm</li> <li>Influence function based sample selection</li> <li>Gradient matching for dataset distillation</li> <li>Diversity-based selection maximizing feature coverage</li> <li>Quality filtering using confidence thresholds</li> <li>Uncertainty sampling for active learning</li> <li>Curriculum sampling with difficulty estimation</li> <li>Balanced sampling maintaining class distribution</li> </ul>"},{"location":"CHANGELOG/#model-architecture-support","title":"Model Architecture Support","text":""},{"location":"CHANGELOG/#transformer-models","title":"Transformer Models","text":"<ul> <li>DeBERTa implementation</li> <li>DeBERTa v3 base (184M parameters)</li> <li>DeBERTa v3 large (435M parameters)</li> <li>DeBERTa v3 xlarge (900M parameters)</li> <li>DeBERTa v2 xlarge (900M parameters)</li> <li>DeBERTa v2 xxlarge (1.5B parameters)</li> <li>Sliding window attention for long sequences</li> <li>Hierarchical attention for document-level classification</li> <li>RoBERTa variants</li> <li>RoBERTa base (125M parameters)</li> <li>RoBERTa large (355M parameters)</li> <li>RoBERTa large MNLI (domain-adapted)</li> <li>XLM-RoBERTa large for multilingual support</li> <li>Enhanced RoBERTa with additional pretraining</li> <li>Domain-adapted RoBERTa on news corpus</li> <li>ELECTRA models</li> <li>ELECTRA base (110M parameters)</li> <li>ELECTRA large (335M parameters)</li> <li>Discriminator-based classification head</li> <li>XLNet architectures</li> <li>XLNet base (110M parameters)</li> <li>XLNet large (340M parameters)</li> <li>Custom classifier head with pooling strategies</li> <li>Longformer for long documents</li> <li>Longformer base (149M parameters)</li> <li>Longformer large (435M parameters)</li> <li>Global attention mechanism for classification tokens</li> <li>T5 encoder-decoder models</li> <li>T5 base (220M parameters)</li> <li>T5 large (770M parameters)</li> <li>T5 3B (3B parameters)</li> <li>FLAN-T5 XL (3B parameters, instruction-tuned)</li> <li>Custom classification head for encoder representations</li> </ul>"},{"location":"CHANGELOG/#large-language-models","title":"Large Language Models","text":"<ul> <li>LLaMA family</li> <li>LLaMA 2 7B (7B parameters)</li> <li>LLaMA 2 13B (13B parameters)</li> <li>LLaMA 2 70B (70B parameters)</li> <li>LLaMA 3 8B (8B parameters)</li> <li>LLaMA 3 70B (70B parameters)</li> <li>Classification adapter for decoder-only architecture</li> <li>Mistral family</li> <li>Mistral 7B base (7B parameters)</li> <li>Mistral 7B Instruct (instruction-tuned)</li> <li>Mixtral 8x7B (47B parameters, sparse mixture of experts)</li> <li>Classification head for causal language models</li> <li>Falcon models</li> <li>Falcon 7B (7B parameters)</li> <li>Falcon 40B (40B parameters)</li> <li>Custom classification adapter</li> <li>MPT series</li> <li>MPT 7B (7B parameters)</li> <li>MPT 30B (30B parameters)</li> <li>Classification wrapper for decoder models</li> <li>Phi models</li> <li>Phi 2 (2.7B parameters)</li> <li>Phi 3 (3.8B parameters)</li> <li>Lightweight classification head</li> </ul>"},{"location":"CHANGELOG/#prompt-based-models","title":"Prompt-based Models","text":"<ul> <li>Soft prompt tuning with learnable prompt embeddings</li> <li>Prefix tuning with virtual tokens prepended to input</li> <li>P-tuning v2 with deep prompt tuning across layers</li> <li>Instruction-following models with template-based prompting</li> <li>Template manager for zero-shot and few-shot scenarios</li> </ul>"},{"location":"CHANGELOG/#classical-baseline-models","title":"Classical Baseline Models","text":"<ul> <li>Naive Bayes with TF-IDF features</li> <li>Support Vector Machines with RBF kernel</li> <li>Random Forest with 100-500 estimators</li> <li>Logistic Regression with L2 regularization</li> <li>Gradient boosting (XGBoost, LightGBM, CatBoost)</li> </ul>"},{"location":"CHANGELOG/#model-base-components","title":"Model Base Components","text":"<ul> <li>Base model wrapper with consistent interface</li> <li>Model registry for dynamic model loading</li> <li>Model factory pattern for instantiation</li> <li>Complexity tracker monitoring parameter counts and FLOPs</li> <li>Pooling strategies (CLS token, mean pooling, max pooling, attention pooling)</li> <li>Classification heads (linear, multi-layer perceptron, hierarchical, attention-based)</li> <li>Multitask heads for auxiliary task learning</li> <li>Adaptive heads adjusting to task complexity</li> </ul>"},{"location":"CHANGELOG/#parameter-efficient-fine-tuning-methods","title":"Parameter-Efficient Fine-Tuning Methods","text":""},{"location":"CHANGELOG/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<ul> <li>LoRA implementation for attention layers</li> <li>Configurable rank values (r=4, 8, 16, 32, 64, 128, 256)</li> <li>Alpha scaling parameter for initialization</li> <li>Target module selection (query, key, value, output projections)</li> <li>Rank selection utilities based on task complexity</li> <li>Target module selector optimizing for parameter efficiency</li> <li>LoRA layer implementation with trainable A and B matrices</li> <li>Weight merging for inference optimization</li> <li>LoRA adapter saving and loading</li> <li>Rank search experiments for optimal configuration</li> <li>LoRA-specific configurations per model architecture</li> </ul>"},{"location":"CHANGELOG/#qlora-quantized-lora","title":"QLoRA (Quantized LoRA)","text":"<ul> <li>4-bit quantization with NF4 (Normal Float 4)</li> <li>8-bit quantization for memory-constrained environments</li> <li>Double quantization for additional memory savings</li> <li>Compute dtype configuration (fp16, bf16, fp32)</li> <li>Quantization configuration per model</li> <li>Dequantization utilities for inference</li> <li>Memory-efficient training enabling 70B models on consumer GPUs</li> </ul>"},{"location":"CHANGELOG/#adapter-modules","title":"Adapter Modules","text":"<ul> <li>Houlsby adapters with bottleneck architecture</li> <li>Pfeiffer adapters with optimized placement</li> <li>Parallel adapters for concurrent processing</li> <li>Adapter fusion combining multiple task-specific adapters</li> <li>Adapter stacking for hierarchical feature learning</li> <li>Adapter configuration with reduction factor tuning</li> <li>Adapter-specific training procedures</li> </ul>"},{"location":"CHANGELOG/#prefix-tuning","title":"Prefix Tuning","text":"<ul> <li>Prefix encoder generating virtual tokens</li> <li>Prefix length optimization (10-200 tokens)</li> <li>Per-layer prefix parameters</li> <li>Reparameterization for training stability</li> <li>Prefix tuning for encoder-decoder and decoder-only models</li> </ul>"},{"location":"CHANGELOG/#prompt-tuning","title":"Prompt Tuning","text":"<ul> <li>Soft prompt embeddings learned end-to-end</li> <li>Prompt initialization strategies (random, vocabulary sampling, task-specific)</li> <li>Prompt length experiments (5-100 tokens)</li> <li>P-tuning v2 with deep prompts across all layers</li> <li>Prompt encoder for complex prompt structures</li> </ul>"},{"location":"CHANGELOG/#ia3-infused-adapter-by-inhibiting-and-amplifying-inner-activations","title":"IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations)","text":"<ul> <li>Learned rescaling vectors for efficient adaptation</li> <li>Minimal parameter overhead (under 0.01% of model parameters)</li> <li>Integration with attention and feedforward layers</li> </ul>"},{"location":"CHANGELOG/#combined-methods","title":"Combined Methods","text":"<ul> <li>LoRA with adapter modules for complementary benefits</li> <li>QLoRA with prompt tuning for extreme efficiency</li> <li>Multi-method fusion optimizing multiple objectives</li> <li>Adapter switching for multi-task scenarios</li> </ul>"},{"location":"CHANGELOG/#ensemble-learning-framework","title":"Ensemble Learning Framework","text":""},{"location":"CHANGELOG/#voting-ensembles","title":"Voting Ensembles","text":"<ul> <li>Soft voting with probability averaging across models</li> <li>Hard voting using majority rule for predictions</li> <li>Weighted voting with learnable or performance-based weights</li> <li>Rank averaging for robust aggregation</li> <li>Confidence-weighted voting prioritizing certain predictions</li> <li>Temperature scaling for calibrated probabilities</li> </ul>"},{"location":"CHANGELOG/#stacking-ensembles","title":"Stacking Ensembles","text":"<ul> <li>Two-level stacking with cross-validation predictions</li> <li>Meta-learner implementations</li> <li>XGBoost meta-learner with tree-based learning</li> <li>LightGBM for fast gradient boosting</li> <li>CatBoost handling categorical features</li> <li>Neural network meta-learner for complex relationships</li> <li>Cross-validation stacking preventing overfitting</li> <li>Feature engineering for meta-learner inputs</li> <li>Regularization in meta-learner training</li> </ul>"},{"location":"CHANGELOG/#blending-ensembles","title":"Blending Ensembles","text":"<ul> <li>Holdout-based blending with separate validation set</li> <li>Dynamic blending with adaptive weights</li> <li>Calibration-aware blending for probabilistic outputs</li> </ul>"},{"location":"CHANGELOG/#advanced-ensemble-methods","title":"Advanced Ensemble Methods","text":"<ul> <li>Bayesian model averaging with posterior model probabilities</li> <li>Snapshot ensembles from single training run</li> <li>Multi-level ensembles with hierarchical structure</li> <li>Mixture of experts with gating network</li> <li>Negative correlation learning for diversity</li> <li>Ensemble pruning removing redundant models</li> </ul>"},{"location":"CHANGELOG/#diversity-optimization","title":"Diversity Optimization","text":"<ul> <li>Diversity metrics (disagreement, Q-statistic, correlation coefficient)</li> <li>Diversity calculator for ensemble analysis</li> <li>Diversity optimizer maximizing ensemble diversity</li> <li>Ensemble pruning based on diversity-accuracy trade-off</li> <li>Component contribution analysis identifying important models</li> </ul>"},{"location":"CHANGELOG/#ensemble-selection","title":"Ensemble Selection","text":"<ul> <li>Ensemble selector choosing optimal subset</li> <li>Greedy forward selection based on validation performance</li> <li>Diversity-aware selection balancing accuracy and disagreement</li> <li>Size-constrained selection for deployment efficiency</li> </ul>"},{"location":"CHANGELOG/#training-infrastructure","title":"Training Infrastructure","text":""},{"location":"CHANGELOG/#standard-training","title":"Standard Training","text":"<ul> <li>Base trainer with training loop abstraction</li> <li>Standard trainer for single-model training</li> <li>Mixed precision training (FP16, BF16) with automatic mixed precision</li> <li>Gradient checkpointing reducing memory consumption by 40-50%</li> <li>Gradient accumulation for large effective batch sizes</li> <li>Distributed training with DistributedDataParallel (DDP)</li> <li>Fully Sharded Data Parallel (FSDP) for large model training</li> <li>DeepSpeed integration (ZeRO stage 1, 2, 3)</li> <li>APEX mixed precision for older CUDA versions</li> </ul>"},{"location":"CHANGELOG/#specialized-trainers","title":"Specialized Trainers","text":"<ul> <li>Safe trainer with overfitting prevention mechanisms</li> <li>Auto trainer with automatic platform detection and optimization</li> <li>LoRA trainer optimizing for adapter training</li> <li>QLoRA trainer with quantization-aware training</li> <li>Adapter trainer for various adapter architectures</li> <li>Prompt trainer for soft prompt optimization</li> <li>Instruction trainer for instruction-tuned models</li> <li>Multi-stage trainer orchestrating progressive training</li> </ul>"},{"location":"CHANGELOG/#advanced-training-strategies","title":"Advanced Training Strategies","text":"<ul> <li>Curriculum learning</li> <li>Self-paced curriculum with automatic difficulty scoring</li> <li>Competence-based scheduling</li> <li>Transfer teacher curriculum from larger model</li> <li>Data difficulty estimation using loss and confidence</li> <li>Adversarial training</li> <li>Fast Gradient Method (FGM) for adversarial perturbations</li> <li>Projected Gradient Descent (PGD) with multiple steps</li> <li>FreeLB with adversarial training in latent space</li> <li>SMART (Smoothness-inducing Adversarial Regularization)</li> <li>Regularization techniques</li> <li>R-Drop with KL divergence between two forward passes</li> <li>Mixout randomly replacing fine-tuned weights with pretrained</li> <li>Spectral normalization for weight matrix constraints</li> <li>Adaptive dropout adjusting rate during training</li> <li>Gradient penalty for smoothness</li> <li>Elastic Weight Consolidation (EWC) for continual learning</li> <li>Sharpness-Aware Minimization (SAM) for flatter minima</li> <li>Knowledge distillation</li> <li>Standard distillation with temperature scaling</li> <li>Feature-based distillation matching intermediate representations</li> <li>Self-distillation from model's own predictions</li> <li>LLaMA distillation to smaller encoder models</li> <li>Mistral distillation with instruction preservation</li> <li>Ensemble distillation from multiple teacher models</li> <li>Progressive distillation with iterative compression</li> <li>Multi-teacher distillation aggregating knowledge</li> <li>Meta-learning</li> <li>Model-Agnostic Meta-Learning (MAML) for few-shot adaptation</li> <li>Reptile for simplified meta-learning</li> <li>Prototypical networks for metric learning</li> <li>Multi-stage training</li> <li>Stage manager coordinating training phases</li> <li>Progressive training from base to xlarge models</li> <li>Iterative refinement with multiple fine-tuning rounds</li> <li>Base to xlarge progression strategy</li> <li>Pretrain-finetune-distill pipeline</li> <li>Contrastive learning</li> <li>Supervised contrastive loss for representation learning</li> <li>Triplet loss with hard negative mining</li> <li>Contrastive learning with data augmentation</li> </ul>"},{"location":"CHANGELOG/#optimization-components","title":"Optimization Components","text":"<ul> <li>Custom optimizers</li> <li>AdamW with decoupled weight decay</li> <li>LAMB for large batch training</li> <li>Lookahead optimizer with slow and fast weights</li> <li>Sharpness-Aware Minimization (SAM)</li> <li>Adafactor for memory-efficient optimization</li> <li>Learning rate schedulers</li> <li>Cosine annealing with warmup</li> <li>Polynomial decay</li> <li>Cyclic learning rate with triangular policy</li> <li>Inverse square root scheduler</li> <li>OneCycle learning rate policy</li> <li>Gradient management</li> <li>Gradient accumulation across micro-batches</li> <li>Gradient clipping by norm and value</li> <li>Gradient checkpointing for memory efficiency</li> <li>Gradient monitoring detecting vanishing/exploding gradients</li> </ul>"},{"location":"CHANGELOG/#loss-functions","title":"Loss Functions","text":"<ul> <li>Focal loss for class imbalance</li> <li>Label smoothing regularization</li> <li>Contrastive loss for metric learning</li> <li>Triplet loss with margin</li> <li>Custom cross-entropy with temperature</li> <li>Instruction-aware loss for prompted models</li> <li>Distillation loss combining hard and soft targets</li> <li>Multi-task loss with task weighting</li> </ul>"},{"location":"CHANGELOG/#training-callbacks","title":"Training Callbacks","text":"<ul> <li>Early stopping with patience and delta thresholds</li> <li>Model checkpoint saving best and periodic checkpoints</li> <li>TensorBoard logging for training visualization</li> <li>Weights &amp; Biases integration for experiment tracking</li> <li>MLflow logger for model registry</li> <li>Learning rate monitoring and scheduling</li> <li>Overfitting monitor detecting train-validation divergence</li> <li>Complexity regularizer limiting model capacity</li> <li>Test protection callback preventing test set access</li> <li>LoRA rank callback tracking adapter efficiency</li> <li>Memory monitor for GPU memory usage</li> <li>Platform-specific callbacks</li> <li>Colab callback handling session timeouts</li> <li>Kaggle callback optimizing for kernel limits</li> <li>Platform callback with automatic detection</li> <li>Quota callback tracking resource usage</li> <li>Session callback managing long-running jobs</li> </ul>"},{"location":"CHANGELOG/#platform-adaptive-training","title":"Platform-Adaptive Training","text":"<ul> <li>Platform detector identifying execution environment</li> <li>Smart selector choosing optimal configuration</li> <li>Platform-specific training configurations</li> <li>Colab free tier: gradient accumulation, mixed precision, checkpoint frequency</li> <li>Colab Pro: larger batch sizes, longer training, advanced features</li> <li>Kaggle GPU: P100/T4 optimization, TPU support</li> <li>Kaggle TPU: XLA compilation, TPU-specific batching</li> <li>Local GPU: full feature utilization, multi-GPU training</li> <li>Local CPU: INT8 inference, CPU-optimized operations</li> <li>Cache manager with platform-specific strategies</li> <li>Checkpoint manager with automatic save/resume</li> <li>Quota tracker monitoring GPU/TPU hours and quotas</li> <li>Storage sync for Google Drive and Kaggle Datasets</li> <li>Session manager handling disconnections and resumption</li> <li>Resource monitor for real-time resource tracking</li> </ul>"},{"location":"CHANGELOG/#overfitting-prevention-system","title":"Overfitting Prevention System","text":""},{"location":"CHANGELOG/#validation-framework","title":"Validation Framework","text":"<ul> <li>Test set validator with SHA-256 hash verification</li> <li>Data leakage detector using statistical independence tests</li> <li>Configuration validator ensuring safe training settings</li> <li>Hyperparameter validator with reasonable bounds</li> <li>Split validator ensuring proper data partitioning</li> <li>Model size validator based on dataset size guidelines</li> <li>LoRA configuration validator recommending safe ranks</li> <li>Ensemble validator checking diversity requirements</li> <li>Constraint validator enforcing overfitting prevention policies</li> </ul>"},{"location":"CHANGELOG/#real-time-monitoring","title":"Real-time Monitoring","text":"<ul> <li>Training monitor tracking loss and metrics</li> <li>Overfitting detector measuring train-validation gap</li> <li>Complexity monitor computing model capacity metrics</li> <li>Benchmark comparator against established baselines</li> <li>Metrics tracker for comprehensive metric logging</li> <li>Gradient monitor detecting training instabilities</li> <li>LoRA rank monitor analyzing adapter efficiency</li> <li>Ensemble diversity monitor ensuring complementary models</li> </ul>"},{"location":"CHANGELOG/#constraint-enforcement","title":"Constraint Enforcement","text":"<ul> <li>Model size constraints based on dataset size</li> <li>Small datasets (under 10K): maximum 100M parameters</li> <li>Medium datasets (10K-100K): maximum 500M parameters</li> <li>Large datasets (over 100K): unlimited with monitoring</li> <li>XLarge model constraints for billion-parameter models</li> <li>LLM constraints for multi-billion parameter models</li> <li>Ensemble constraints limiting model count and diversity requirements</li> <li>Training constraints on epochs and early stopping</li> <li>Parameter efficiency requirements enforcing PEFT usage</li> <li>Augmentation constraints preventing over-augmentation</li> <li>Constraint enforcer with automatic violation handling</li> </ul>"},{"location":"CHANGELOG/#access-control-and-guards","title":"Access Control and Guards","text":"<ul> <li>Test set guard preventing unauthorized access</li> <li>Validation guard ensuring proper validation strategy</li> <li>Experiment guard for reproducibility requirements</li> <li>Access control logging all test set interactions</li> <li>Parameter freeze guard preventing backbone updates</li> <li>Configuration guard validating before training</li> </ul>"},{"location":"CHANGELOG/#recommendation-system","title":"Recommendation System","text":"<ul> <li>Model recommender based on dataset characteristics</li> <li>Configuration recommender suggesting safe hyperparameters</li> <li>Prevention technique recommender for overfitting risks</li> <li>Ensemble recommender for model combination</li> <li>LoRA recommender suggesting optimal rank</li> <li>Distillation recommender for compression strategies</li> <li>Parameter efficiency recommender optimizing trainable parameters</li> <li>Dataset-specific recommendations for AG News</li> </ul>"},{"location":"CHANGELOG/#reporting-and-analytics","title":"Reporting and Analytics","text":"<ul> <li>Overfitting reporter generating comprehensive reports</li> <li>Risk scorer quantifying overfitting probability</li> <li>Comparison reporter analyzing train/validation/test metrics</li> <li>HTML report generator with visualizations</li> <li>Parameter efficiency reporter comparing methods</li> <li>Benchmark comparison against published results</li> <li>Statistical significance testing for result validation</li> </ul>"},{"location":"CHANGELOG/#evaluation-and-analysis","title":"Evaluation and Analysis","text":""},{"location":"CHANGELOG/#metrics-computation","title":"Metrics Computation","text":"<ul> <li>Classification metrics (accuracy, precision, recall, F1-score)</li> <li>Per-class metrics for fine-grained analysis</li> <li>Confusion matrix computation and visualization</li> <li>ROC-AUC and PR-AUC for probabilistic evaluation</li> <li>Calibration metrics (Expected Calibration Error, Maximum Calibration Error)</li> <li>Overfitting metrics (train-validation gap, generalization gap)</li> <li>Diversity metrics for ensemble evaluation</li> <li>Efficiency metrics (parameters, FLOPs, inference time, memory)</li> </ul>"},{"location":"CHANGELOG/#error-analysis","title":"Error Analysis","text":"<ul> <li>Misclassification analysis identifying error patterns</li> <li>Confidence distribution analysis across predictions</li> <li>Hard example identification for targeted improvement</li> <li>Failure case analysis with error categorization</li> <li>Per-class error breakdown</li> <li>Confusion pattern detection</li> <li>Error correlation across ensemble members</li> </ul>"},{"location":"CHANGELOG/#model-interpretability","title":"Model Interpretability","text":"<ul> <li>Attention visualization using BertViz</li> <li>Attention weight extraction and analysis</li> <li>SHAP value computation for feature importance</li> <li>LIME explanations for individual predictions</li> <li>Integrated gradients for attribution analysis</li> <li>Feature importance ranking</li> <li>Layer-wise relevance propagation</li> <li>Saliency maps for input importance</li> </ul>"},{"location":"CHANGELOG/#lora-specific-analysis","title":"LoRA-Specific Analysis","text":"<ul> <li>LoRA rank impact analysis across tasks</li> <li>Weight distribution visualization for A and B matrices</li> <li>Adapter efficiency comparison across methods</li> <li>Parameter efficiency metrics</li> <li>Rank ablation studies</li> <li>LoRA weight visualization</li> </ul>"},{"location":"CHANGELOG/#ensemble-analysis","title":"Ensemble Analysis","text":"<ul> <li>Diversity measurement using multiple metrics</li> <li>Component contribution analysis via ablation</li> <li>Disagreement analysis across ensemble members</li> <li>Ensemble confidence calibration</li> <li>Member correlation analysis</li> <li>Ensemble pruning analysis</li> </ul>"},{"location":"CHANGELOG/#visualization-tools","title":"Visualization Tools","text":"<ul> <li>Training curves (loss, accuracy, learning rate)</li> <li>Confusion matrix heatmaps</li> <li>Attention maps with head-level analysis</li> <li>Embedding visualizations using t-SNE and UMAP</li> <li>LoRA weight distribution plots</li> <li>Ensemble diversity plots</li> <li>Performance comparison charts</li> </ul>"},{"location":"CHANGELOG/#experiment-management","title":"Experiment Management","text":""},{"location":"CHANGELOG/#experiment-infrastructure","title":"Experiment Infrastructure","text":"<ul> <li>Experiment runner with configuration management</li> <li>Experiment tagger for organization and retrieval</li> <li>Result aggregator combining multiple runs</li> <li>Leaderboard generator ranking models</li> <li>Reproducibility utilities (seed setting, deterministic operations)</li> <li>Experiment tracking with metadata</li> <li>Version control integration</li> </ul>"},{"location":"CHANGELOG/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<ul> <li>Optuna integration with pruning algorithms</li> <li>Ray Tune distributed hyperparameter search</li> <li>Bayesian optimization with Gaussian processes</li> <li>Hyperband for efficient resource allocation</li> <li>LoRA rank search experiments</li> <li>Ensemble weight optimization</li> <li>Learning rate finder</li> <li>Batch size optimization</li> </ul>"},{"location":"CHANGELOG/#ablation-studies","title":"Ablation Studies","text":"<ul> <li>Model size ablation (base, large, xlarge, xxlarge)</li> <li>Data amount ablation (10%, 25%, 50%, 75%, 100%)</li> <li>LoRA rank ablation (4, 8, 16, 32, 64, 128)</li> <li>QLoRA bits ablation (4-bit vs 8-bit quantization)</li> <li>Regularization ablation (dropout, weight decay, R-Drop, Mixout)</li> <li>Augmentation impact analysis</li> <li>Ensemble size ablation (1, 3, 5, 7, 10 models)</li> <li>Ensemble component ablation (removing individual models)</li> <li>Prompt ablation (zero-shot, few-shot, instruction-tuned)</li> <li>Distillation temperature ablation (1.0, 2.0, 4.0, 8.0)</li> <li>Feature ablation (text only vs text with metadata)</li> </ul>"},{"location":"CHANGELOG/#sota-experiment-pipeline","title":"SOTA Experiment Pipeline","text":"<ul> <li>Phase 1: XLarge models with LoRA fine-tuning</li> <li>Phase 2: LLM models with QLoRA quantization</li> <li>Phase 3: LLM distillation to XLarge student models</li> <li>Phase 4: Ensemble of top XLarge models</li> <li>Phase 5: Ultimate SOTA combining all techniques</li> <li>Phase 6: Production-ready SOTA with optimization</li> <li>Single model SOTA experiments</li> <li>Ensemble SOTA experiments</li> <li>Full pipeline SOTA validation</li> <li>Production deployment experiments</li> <li>Prompt-based SOTA approaches</li> <li>Comprehensive approach comparison</li> </ul>"},{"location":"CHANGELOG/#baseline-experiments","title":"Baseline Experiments","text":"<ul> <li>Classical ML baselines (Naive Bayes, SVM, Random Forest, Logistic Regression)</li> <li>Neural baselines (LSTM, CNN, vanilla BERT)</li> <li>Transformer baselines (BERT base, RoBERTa base)</li> <li>Benchmark comparisons against published results</li> </ul>"},{"location":"CHANGELOG/#integration-with-tracking-platforms","title":"Integration with Tracking Platforms","text":"<ul> <li>Weights &amp; Biases integration</li> <li>Automatic experiment logging</li> <li>Hyperparameter tracking</li> <li>Artifact management</li> <li>Model versioning</li> <li>Custom dashboards</li> <li>MLflow integration</li> <li>Experiment tracking</li> <li>Model registry</li> <li>Model deployment</li> <li>Metric comparison</li> <li>TensorBoard logging</li> <li>Scalar metrics</li> <li>Image logging</li> <li>Embedding projector</li> <li>Hyperparameter tuning</li> <li>Custom scalars configuration</li> <li>Local monitoring</li> <li>File-based metrics storage</li> <li>Local TensorBoard server</li> <li>Local MLflow server</li> <li>SQLite-based tracking</li> </ul>"},{"location":"CHANGELOG/#api-and-serving","title":"API and Serving","text":""},{"location":"CHANGELOG/#restful-api","title":"RESTful API","text":"<ul> <li>FastAPI application with automatic OpenAPI documentation</li> <li>API routers</li> <li>Classification router for inference endpoints</li> <li>Training router for model training</li> <li>Models router for model management</li> <li>Data router for dataset operations</li> <li>Health router for system monitoring</li> <li>Metrics router for performance statistics</li> <li>Overfitting router for prevention system</li> <li>LLM router for large model operations</li> <li>Platform router for environment info</li> <li>Admin router for administrative tasks</li> <li>Request/Response schemas with Pydantic validation</li> <li>Error handling with detailed error messages</li> <li>CORS configuration for cross-origin requests</li> <li>Rate limiting preventing abuse</li> <li>Request validation and sanitization</li> <li>WebSocket handler for streaming predictions</li> <li>Server-sent events for real-time updates</li> </ul>"},{"location":"CHANGELOG/#authentication-and-security","title":"Authentication and Security","text":"<ul> <li>Token-based authentication with JWT</li> <li>API key management</li> <li>Role-based access control (RBAC)</li> <li>Rate limiting per user/IP</li> <li>Input validation and sanitization</li> <li>CORS policy enforcement</li> <li>Request logging and auditing</li> <li>Security headers configuration</li> </ul>"},{"location":"CHANGELOG/#middleware","title":"Middleware","text":"<ul> <li>Logging middleware tracking requests</li> <li>Metrics middleware collecting statistics</li> <li>Security middleware enforcing policies</li> <li>Error handling middleware</li> <li>Request ID middleware for tracing</li> <li>Compression middleware for responses</li> </ul>"},{"location":"CHANGELOG/#local-api","title":"Local API","text":"<ul> <li>Simplified API for offline deployment</li> <li>Batch API for processing multiple inputs</li> <li>Streaming API for real-time predictions</li> <li>File-based API for document classification</li> </ul>"},{"location":"CHANGELOG/#service-layer","title":"Service Layer","text":""},{"location":"CHANGELOG/#core-services","title":"Core Services","text":"<ul> <li>Prediction service handling inference requests</li> <li>Training service managing training jobs</li> <li>Data service for dataset operations</li> <li>Model management service for model lifecycle</li> <li>LLM service for large language model operations</li> <li>Service registry for dependency injection</li> <li>Base service with common functionality</li> </ul>"},{"location":"CHANGELOG/#local-services","title":"Local Services","text":"<ul> <li>Local cache service using diskcache</li> <li>Local queue service for background tasks</li> <li>File storage service for model artifacts</li> <li>SQLite-based tracking service</li> </ul>"},{"location":"CHANGELOG/#monitoring-services","title":"Monitoring Services","text":"<ul> <li>Monitoring router aggregating metrics</li> <li>TensorBoard service for visualization</li> <li>MLflow service for experiment tracking</li> <li>Weights &amp; Biases service integration</li> <li>Local metrics service for offline monitoring</li> <li>Logging service with structured logging</li> </ul>"},{"location":"CHANGELOG/#user-interfaces","title":"User Interfaces","text":""},{"location":"CHANGELOG/#streamlit-application","title":"Streamlit Application","text":"<ul> <li>20 interactive pages</li> <li>Home page with project overview</li> <li>Single prediction interface</li> <li>Batch analysis tool</li> <li>Model comparison dashboard</li> <li>Overfitting monitoring dashboard</li> <li>Model recommender system</li> <li>Parameter efficiency dashboard</li> <li>Interpretability viewer</li> <li>Performance dashboard</li> <li>Real-time demo</li> <li>Model selection wizard</li> <li>Documentation browser</li> <li>Prompt testing interface</li> <li>Local monitoring dashboard</li> <li>IDE setup guide</li> <li>Experiment tracker</li> <li>Platform information</li> <li>Quota dashboard</li> <li>Platform selector</li> <li>Auto-training UI</li> <li>Custom components</li> <li>Prediction component</li> <li>Overfitting monitor component</li> <li>LoRA config selector</li> <li>Ensemble builder</li> <li>Visualization component</li> <li>Model selector</li> <li>File uploader</li> <li>Result display</li> <li>Performance monitor</li> <li>Prompt builder</li> <li>IDE configurator</li> <li>Platform info component</li> <li>Quota monitor component</li> <li>Resource gauge</li> <li>Session management for state persistence</li> <li>Caching for performance optimization</li> <li>Custom theming and styling</li> <li>Helper utilities</li> </ul>"},{"location":"CHANGELOG/#gradio-application","title":"Gradio Application","text":"<ul> <li>Quick demo interface</li> <li>Model comparison tool</li> <li>Interactive prediction</li> <li>Visualization dashboard</li> </ul>"},{"location":"CHANGELOG/#command-line-interface","title":"Command-Line Interface","text":"<ul> <li>Main CLI with subcommands</li> <li>Rich formatting for output</li> <li>Progress bars for long operations</li> <li>Interactive prompts</li> <li>ASCII art for branding</li> <li>Comprehensive help messages</li> <li>Command aliases</li> </ul>"},{"location":"CHANGELOG/#documentation","title":"Documentation","text":""},{"location":"CHANGELOG/#top-level-documentation","title":"Top-Level Documentation","text":"<ul> <li>README.md with project overview and quick start</li> <li>ARCHITECTURE.md describing system design</li> <li>PERFORMANCE.md with benchmark results</li> <li>SECURITY.md covering security considerations</li> <li>TROUBLESHOOTING.md for common issues</li> <li>SOTA_MODELS_GUIDE.md for model selection</li> <li>OVERFITTING_PREVENTION.md for prevention strategies</li> <li>ROADMAP.md with future plans</li> <li>FREE_DEPLOYMENT_GUIDE.md for free-tier deployment</li> <li>PLATFORM_OPTIMIZATION_GUIDE.md for platform-specific optimization</li> <li>IDE_SETUP_GUIDE.md for multi-IDE support</li> <li>LOCAL_MONITORING_GUIDE.md for local monitoring setup</li> <li>QUICK_START.md for 5-minute getting started</li> <li>HEALTH_CHECK.md for system validation</li> <li>CHANGELOG.md (this file)</li> </ul>"},{"location":"CHANGELOG/#user-documentation","title":"User Documentation","text":"<ul> <li>Multi-level guides</li> <li>Level 1 Beginner: Installation, first model, evaluation, deployment</li> <li>Level 2 Intermediate: LoRA/QLoRA, ensemble, distillation, optimization</li> <li>Level 3 Advanced: SOTA pipeline, custom models, research workflow</li> <li>Platform guides</li> <li>Colab guide with free and Pro optimization</li> <li>Colab advanced features</li> <li>Kaggle guide with GPU and TPU support</li> <li>Kaggle TPU-specific guide</li> <li>Local deployment guide</li> <li>Gitpod cloud IDE setup</li> <li>Platform comparison matrix</li> <li>User guides</li> <li>Data preparation workflow</li> <li>Model training procedures</li> <li>Auto-training system</li> <li>LoRA configuration guide</li> <li>QLoRA setup guide</li> <li>Distillation guide</li> <li>Ensemble building guide</li> <li>Overfitting prevention practices</li> <li>Safe training procedures</li> <li>Evaluation methodology</li> <li>Local deployment instructions</li> <li>Quota management strategies</li> <li>Platform optimization techniques</li> <li>Prompt engineering guide</li> <li>Advanced techniques compendium</li> </ul>"},{"location":"CHANGELOG/#developer-documentation","title":"Developer Documentation","text":"<ul> <li>Architecture documentation</li> <li>Adding custom models guide</li> <li>Custom dataset integration</li> <li>Local API development</li> <li>Contributing guidelines</li> <li>Code organization principles</li> <li>Design patterns used</li> </ul>"},{"location":"CHANGELOG/#api-reference","title":"API Reference","text":"<ul> <li>REST API documentation</li> <li>Data API reference</li> <li>Models API reference</li> <li>Training API reference</li> <li>LoRA API reference</li> <li>Ensemble API reference</li> <li>Overfitting prevention API reference</li> <li>Platform API reference</li> <li>Quota API reference</li> <li>Evaluation API reference</li> </ul>"},{"location":"CHANGELOG/#ide-guides","title":"IDE Guides","text":"<ul> <li>Visual Studio Code setup with extensions and tasks</li> <li>PyCharm configuration with run configurations</li> <li>Jupyter Notebook/Lab setup with kernels</li> <li>Vim setup with CoC LSP</li> <li>Sublime Text project configuration</li> <li>IDE comparison and recommendations</li> </ul>"},{"location":"CHANGELOG/#tutorials","title":"Tutorials","text":"<ul> <li>Basic usage tutorial</li> <li>XLarge model training tutorial</li> <li>LLM fine-tuning tutorial</li> <li>Distillation tutorial</li> <li>SOTA pipeline tutorial</li> <li>Local training tutorial</li> <li>Free deployment tutorial</li> <li>Best practices guide</li> </ul>"},{"location":"CHANGELOG/#examples","title":"Examples","text":"<ul> <li>Hello world example</li> <li>Training baseline example</li> <li>SOTA pipeline example</li> <li>Custom model example</li> </ul>"},{"location":"CHANGELOG/#cheatsheets","title":"Cheatsheets","text":"<ul> <li>Model selection cheatsheet (PDF)</li> <li>Overfitting prevention checklist (PDF)</li> <li>Free deployment comparison (PDF)</li> <li>Platform comparison chart (PDF)</li> <li>Auto-training cheatsheet (PDF)</li> <li>Quota limits reference (PDF)</li> <li>CLI commands reference (PDF)</li> </ul>"},{"location":"CHANGELOG/#academic-documentation","title":"Academic Documentation","text":"<ul> <li>Architecture decision records (ADRs)</li> <li>Design patterns documentation</li> <li>System diagrams with PlantUML</li> <li>Best practices documentation</li> </ul>"},{"location":"CHANGELOG/#multi-ide-support","title":"Multi-IDE Support","text":""},{"location":"CHANGELOG/#ide-configurations","title":"IDE Configurations","text":"<ul> <li>Visual Studio Code</li> <li>Settings.json with Python configuration</li> <li>Launch.json with debugging configurations</li> <li>Tasks.json for build and test tasks</li> <li>Extensions.json recommending extensions</li> <li>Snippets for Python and YAML</li> <li>PyCharm</li> <li>Workspace configuration</li> <li>Inspection profiles</li> <li>Run configurations (train, test, API)</li> <li>Code style configuration</li> <li>Module settings</li> <li>Jupyter</li> <li>Notebook configuration</li> <li>Lab configuration</li> <li>Custom CSS styling</li> <li>Custom JavaScript extensions</li> <li>Nbextensions configuration</li> <li>User settings</li> <li>Workspace configuration</li> <li>Custom kernel configuration</li> <li>Vim</li> <li>Vimrc configuration</li> <li>CoC settings for LSP</li> <li>UltiSnips for code snippets</li> <li>Plugin recommendations</li> <li>Neovim</li> <li>Init.lua with Lua configuration</li> <li>Plugin management with Packer</li> <li>LSP configuration</li> <li>Keymaps for common tasks</li> <li>Custom commands</li> <li>Sublime Text</li> <li>Project file configuration</li> <li>Workspace settings</li> <li>Preferences for Python</li> <li>Code snippets</li> <li>Build systems for training and testing</li> <li>Cloud IDEs</li> <li>Gitpod configuration with Docker image</li> <li>GitHub Codespaces devcontainer</li> <li>Google Colab setup script</li> <li>Kaggle Kernels setup script</li> </ul>"},{"location":"CHANGELOG/#configuration-management","title":"Configuration Management","text":"<ul> <li>SOURCE_OF_TRUTH.yaml for canonical settings</li> <li>Automatic synchronization scripts</li> <li>IDE-specific README files</li> <li>Setup scripts per IDE</li> </ul>"},{"location":"CHANGELOG/#local-deployment-and-monitoring","title":"Local Deployment and Monitoring","text":""},{"location":"CHANGELOG/#docker-support","title":"Docker Support","text":"<ul> <li>Multi-stage Dockerfiles</li> <li>Base image with dependencies</li> <li>CPU-optimized image</li> <li>GPU-optimized image with CUDA</li> <li>Docker Compose orchestration</li> <li>API service</li> <li>TensorBoard service</li> <li>MLflow service</li> <li>Redis cache</li> <li>Nginx reverse proxy</li> <li>Docker ignore file</li> <li>Build optimization with layer caching</li> </ul>"},{"location":"CHANGELOG/#local-monitoring-stack","title":"Local Monitoring Stack","text":"<ul> <li>TensorBoard configuration</li> <li>Scalar metrics logging</li> <li>Image logging</li> <li>Embedding projector</li> <li>Custom scalars</li> <li>Hyperparameter tuning</li> <li>MLflow configuration</li> <li>Experiment tracking</li> <li>Model registry</li> <li>Artifact storage</li> <li>Metric comparison</li> <li>Dashboard customization</li> <li>Custom dashboards</li> <li>Training monitoring</li> <li>Overfitting detection</li> <li>Parameter efficiency tracking</li> <li>Platform metrics</li> <li>Quota monitoring</li> <li>Metrics collectors</li> <li>Custom metrics implementation</li> <li>Local metrics storage</li> <li>Model metrics tracking</li> <li>Training metrics collection</li> <li>Overfitting metrics</li> <li>Platform metrics</li> <li>Quota metrics</li> </ul>"},{"location":"CHANGELOG/#system-services","title":"System Services","text":"<ul> <li>Systemd service files</li> <li>API service</li> <li>Monitoring service</li> <li>Background worker service</li> <li>Nginx configuration</li> <li>Reverse proxy setup</li> <li>SSL/TLS termination</li> <li>Load balancing</li> <li>Static file serving</li> <li>Startup scripts</li> <li>TensorBoard launcher</li> <li>MLflow server launcher</li> <li>Weights &amp; Biases sync</li> <li>Platform monitoring</li> <li>Metrics export</li> <li>Quota export</li> <li>Report generation</li> </ul>"},{"location":"CHANGELOG/#caching-and-storage","title":"Caching and Storage","text":"<ul> <li>Local caching strategies</li> <li>Disk cache for models</li> <li>Memory cache for frequently accessed data</li> <li>LRU cache for limited memory</li> <li>SQLite database for tracking</li> <li>Experiment metadata</li> <li>Metrics history</li> <li>Model versions</li> <li>Quota tracking</li> <li>Backup and recovery</li> <li>Incremental backup strategy</li> <li>Local backup scripts</li> <li>Restore procedures</li> <li>Recovery plan documentation</li> </ul>"},{"location":"CHANGELOG/#testing-and-quality-assurance","title":"Testing and Quality Assurance","text":""},{"location":"CHANGELOG/#test-suite-organization","title":"Test Suite Organization","text":"<ul> <li>Unit tests (200+ tests)</li> <li>Data module tests</li> <li>Model module tests</li> <li>Training module tests</li> <li>Deployment module tests</li> <li>API tests</li> <li>Overfitting prevention tests</li> <li>Utility tests</li> <li>Integration tests (100+ tests)</li> <li>Full pipeline testing</li> <li>Auto-training flow</li> <li>Ensemble pipeline</li> <li>Inference pipeline</li> <li>Local API flow</li> <li>Prompt pipeline</li> <li>LLM integration</li> <li>Platform workflows</li> <li>Quota tracking flow</li> <li>Overfitting prevention flow</li> <li>Platform-specific tests</li> <li>Colab integration tests</li> <li>Kaggle integration tests</li> <li>Local environment tests</li> <li>Performance tests</li> <li>Model speed benchmarks</li> <li>Memory usage tests</li> <li>Accuracy benchmarks</li> <li>Local performance tests</li> <li>SLA compliance tests</li> <li>Throughput tests</li> <li>End-to-end tests</li> <li>Complete workflow testing</li> <li>User scenario tests</li> <li>Local deployment tests</li> <li>Free deployment tests</li> <li>Quickstart pipeline tests</li> <li>SOTA pipeline tests</li> <li>Auto-training on Colab</li> <li>Auto-training on Kaggle</li> <li>Quota enforcement tests</li> <li>Regression tests</li> <li>Model accuracy regression</li> <li>Ensemble diversity regression</li> <li>Inference speed regression</li> <li>Baseline comparison</li> <li>Chaos engineering tests</li> <li>Fault tolerance testing</li> <li>Corrupted configuration handling</li> <li>Out-of-memory handling</li> <li>Network failure resilience</li> <li>Compatibility tests</li> <li>PyTorch version compatibility</li> <li>Transformers version compatibility</li> <li>Cross-platform testing</li> <li>Python version matrix</li> </ul>"},{"location":"CHANGELOG/#test-infrastructure","title":"Test Infrastructure","text":"<ul> <li>Pytest configuration with markers</li> <li>Fixtures for test data</li> <li>Sample data fixtures</li> <li>Mock models</li> <li>Test configurations</li> <li>Local-specific fixtures</li> <li>Conftest with shared setup</li> <li>Test utilities and helpers</li> </ul>"},{"location":"CHANGELOG/#code-quality-tools","title":"Code Quality Tools","text":"<ul> <li>Black for code formatting (line length 100)</li> <li>isort for import sorting</li> <li>flake8 for linting with custom rules</li> <li>pylint for code analysis</li> <li>mypy for static type checking</li> <li>ruff for fast linting</li> <li>pre-commit hooks</li> <li>Black formatting</li> <li>isort import sorting</li> <li>flake8 linting</li> <li>mypy type checking</li> <li>Trailing whitespace removal</li> <li>YAML validation</li> <li>Large file prevention</li> <li>Commitlint for conventional commits</li> </ul>"},{"location":"CHANGELOG/#security-and-safety","title":"Security and Safety","text":"<ul> <li>Bandit for security scanning</li> <li>Safety for dependency vulnerability checking</li> <li>Secrets detection preventing credential leaks</li> <li>PII detection in data</li> <li>Data masking utilities</li> <li>Model checksum verification</li> <li>Dependency auditing</li> </ul>"},{"location":"CHANGELOG/#coverage-and-reporting","title":"Coverage and Reporting","text":"<ul> <li>pytest-cov for coverage tracking</li> <li>Coverage reports (term, HTML, XML)</li> <li>Branch coverage enabled</li> <li>Coverage thresholds enforced</li> <li>Coverage exclusions documented</li> </ul>"},{"location":"CHANGELOG/#configuration-management_1","title":"Configuration Management","text":""},{"location":"CHANGELOG/#configuration-structure","title":"Configuration Structure","text":"<ul> <li>300+ YAML configuration files</li> <li>Hierarchical organization</li> <li>API configurations</li> <li>Service configurations</li> <li>Environment configurations</li> <li>Feature flags</li> <li>Secrets templates</li> <li>Model configurations (60+ files)</li> <li>Training configurations (40+ files)</li> <li>Overfitting prevention configurations</li> <li>Data configurations</li> <li>Deployment configurations</li> <li>Quota configurations</li> <li>Experiment configurations</li> </ul>"},{"location":"CHANGELOG/#model-configurations","title":"Model Configurations","text":"<ul> <li>Recommended configurations</li> <li>Quick start configuration</li> <li>Balanced configuration</li> <li>SOTA accuracy configuration</li> <li>Tier 1 SOTA (XLarge with LoRA)</li> <li>Tier 2 LLM (QLoRA)</li> <li>Tier 3 Ensemble</li> <li>Tier 4 Distilled</li> <li>Tier 5 Free-optimized<ul> <li>Auto-selected for platforms</li> <li>Platform-specific optimizations</li> <li>Colab-friendly configurations</li> <li>CPU-friendly configurations</li> </ul> </li> <li>Single model configurations</li> <li>Transformer variants (30+ configs)</li> <li>LLM variants (15+ configs)</li> <li>Ensemble configurations</li> <li>Ensemble selection guide</li> <li>Presets (quick start, SOTA, balanced)</li> <li>Voting ensembles</li> <li>Stacking ensembles</li> <li>Blending ensembles</li> <li>Advanced ensembles</li> </ul>"},{"location":"CHANGELOG/#training-configurations","title":"Training Configurations","text":"<ul> <li>Standard training configurations</li> <li>Platform-adaptive configurations</li> <li>Colab free training</li> <li>Colab Pro training</li> <li>Kaggle GPU training</li> <li>Kaggle TPU training</li> <li>Local GPU training</li> <li>Local CPU training</li> <li>Efficient training (LoRA, QLoRA, Adapters, Prefix, Prompt, IA3, Combined)</li> <li>TPU optimization</li> <li>Advanced training (curriculum, adversarial, multitask, contrastive, distillation, meta-learning, instruction tuning, multi-stage)</li> <li>Regularization configurations (dropout, advanced regularization, data regularization, combined)</li> <li>Safe training configurations</li> </ul>"},{"location":"CHANGELOG/#configuration-tools","title":"Configuration Tools","text":"<ul> <li>Configuration loader with validation</li> <li>Configuration validator with schemas</li> <li>Configuration generator from templates</li> <li>Smart defaults system</li> <li>Configuration explainer</li> <li>Configuration comparator</li> <li>Configuration optimizer</li> <li>Sync manager for IDE configs</li> <li>Validation for all configs</li> </ul>"},{"location":"CHANGELOG/#platform-specific-features","title":"Platform-Specific Features","text":""},{"location":"CHANGELOG/#platform-detection","title":"Platform Detection","text":"<ul> <li>Automatic environment detection</li> <li>Google Colab (free and Pro)</li> <li>Kaggle Kernels (GPU and TPU)</li> <li>Local machine (CPU and GPU)</li> <li>Gitpod cloud IDE</li> <li>GitHub Codespaces</li> <li>HuggingFace Spaces</li> <li>Platform profiles with resource limits</li> <li>Smart selector choosing optimal configuration</li> </ul>"},{"location":"CHANGELOG/#quota-management","title":"Quota Management","text":"<ul> <li>Quota tracking system</li> <li>GPU hour tracking</li> <li>TPU hour tracking</li> <li>Session duration monitoring</li> <li>Resource usage logging</li> <li>Quota limits per platform</li> <li>Colab free: 12-15 GPU hours per week</li> <li>Colab Pro: 50-100 GPU hours per month</li> <li>Kaggle: 30 GPU hours + 30 TPU hours per week</li> <li>Platform quotas configuration</li> <li>Quota callbacks during training</li> <li>Quota dashboard in UI</li> <li>Usage history tracking</li> <li>Session logs</li> <li>Platform usage database</li> </ul>"},{"location":"CHANGELOG/#session-management","title":"Session Management","text":"<ul> <li>Session timeout handling</li> <li>Checkpoint auto-save before timeout</li> <li>Session recovery after disconnect</li> <li>Keep-alive utilities for Colab</li> <li>Progress persistence</li> <li>State synchronization</li> </ul>"},{"location":"CHANGELOG/#resource-monitoring","title":"Resource Monitoring","text":"<ul> <li>Real-time resource monitoring</li> <li>GPU memory tracking</li> <li>CPU usage monitoring</li> <li>Disk space monitoring</li> <li>Network bandwidth tracking</li> <li>Resource alerts and warnings</li> </ul>"},{"location":"CHANGELOG/#platform-optimization","title":"Platform Optimization","text":"<ul> <li>Colab optimizations</li> <li>Drive mounting and caching</li> <li>Session keep-alive</li> <li>Checkpoint frequency adjustment</li> <li>Memory-efficient training</li> <li>Kaggle optimizations</li> <li>Dataset caching</li> <li>TPU utilization</li> <li>Kernel time management</li> <li>Output size management</li> <li>Local optimizations</li> <li>Multi-GPU utilization</li> <li>CPU parallelization</li> <li>Memory management</li> <li>Disk I/O optimization</li> </ul>"},{"location":"CHANGELOG/#deployment-support","title":"Deployment Support","text":""},{"location":"CHANGELOG/#free-tier-deployment","title":"Free-Tier Deployment","text":"<ul> <li>Google Colab deployment</li> <li>Free tier (T4 GPU, 12GB RAM)</li> <li>Pro tier (V100/A100, 32GB RAM)</li> <li>Drive integration</li> <li>Session management</li> <li>Kaggle deployment</li> <li>GPU kernels (P100/T4, 16GB RAM)</li> <li>TPU kernels (TPU v3-8)</li> <li>Dataset integration</li> <li>Notebook scheduling</li> <li>HuggingFace Spaces</li> <li>Gradio app deployment</li> <li>Streamlit app deployment</li> <li>Model hosting</li> <li>Inference API</li> <li>Streamlit Cloud</li> <li>Free community tier</li> <li>Resource limitations</li> <li>GitHub integration</li> <li>GitHub Codespaces</li> <li>Development environment</li> <li>GPU support (paid)</li> <li>Integration with VS Code</li> <li>Gitpod</li> <li>Cloud IDE</li> <li>Prebuilt environments</li> <li>Docker-based workspace</li> </ul>"},{"location":"CHANGELOG/#local-deployment","title":"Local Deployment","text":"<ul> <li>Docker containerization</li> <li>Multi-stage builds</li> <li>CPU and GPU images</li> <li>Compose orchestration</li> <li>Systemd services</li> <li>API service</li> <li>Monitoring service</li> <li>Worker service</li> <li>Nginx reverse proxy</li> <li>SSL/TLS setup</li> <li>Load balancing</li> <li>Static file serving</li> <li>Process management</li> <li>Gunicorn for production</li> <li>Uvicorn for ASGI</li> <li>Supervisor for process control</li> </ul>"},{"location":"CHANGELOG/#model-optimization-for-deployment","title":"Model Optimization for Deployment","text":"<ul> <li>ONNX export for cross-platform inference</li> <li>Quantization (INT8, INT4) for reduced size</li> <li>Pruning for model compression</li> <li>Knowledge distillation to smaller models</li> <li>TensorRT optimization for NVIDIA GPUs</li> <li>OpenVINO optimization for Intel hardware</li> <li>Model caching for faster loading</li> <li>Batch inference for throughput</li> </ul>"},{"location":"CHANGELOG/#build-and-packaging","title":"Build and Packaging","text":""},{"location":"CHANGELOG/#python-packaging","title":"Python Packaging","text":"<ul> <li>setup.py with comprehensive metadata</li> <li>setup.cfg for declarative configuration</li> <li>pyproject.toml for modern packaging</li> <li>MANIFEST.in for additional files</li> <li>Version management in version.py</li> <li>Automatic version from git tags with setuptools-scm</li> </ul>"},{"location":"CHANGELOG/#requirements-management","title":"Requirements Management","text":"<ul> <li>Modular requirements files (15+ files)</li> <li>base.txt: Core dependencies</li> <li>ml.txt: Machine learning libraries</li> <li>llm.txt: Large language model support</li> <li>efficient.txt: Parameter-efficient fine-tuning</li> <li>data.txt: Data processing tools</li> <li>ui.txt: User interface libraries</li> <li>dev.txt: Development tools</li> <li>docs.txt: Documentation generation</li> <li>research.txt: Research and experimentation</li> <li>robustness.txt: Robustness testing</li> <li>local_prod.txt: Local production deployment</li> <li>all_local.txt: Complete local installation</li> <li>colab.txt: Google Colab specific</li> <li>kaggle.txt: Kaggle Kernels specific</li> <li>free_tier.txt: Free-tier platforms</li> <li>local_monitoring.txt: Local monitoring stack</li> <li>minimal.txt: Minimal installation</li> <li>platform_minimal.txt: Platform-specific minimal</li> <li>Locked requirements for reproducibility</li> <li>base.lock</li> <li>ml.lock</li> <li>llm.lock</li> <li>all.lock</li> </ul>"},{"location":"CHANGELOG/#build-automation","title":"Build Automation","text":"<ul> <li>Makefile with 70+ targets</li> <li>Installation targets</li> <li>Testing targets</li> <li>Linting and formatting targets</li> <li>Documentation building targets</li> <li>Deployment targets</li> <li>Cleaning targets</li> <li>Installation scripts</li> <li>install.sh for automated setup</li> <li>Platform-specific setup scripts</li> <li>Environment validation scripts</li> <li>Dependency verification</li> </ul>"},{"location":"CHANGELOG/#research-and-experimentation-tools","title":"Research and Experimentation Tools","text":""},{"location":"CHANGELOG/#benchmarking","title":"Benchmarking","text":"<ul> <li>Accuracy benchmarks</li> <li>Model comparison results</li> <li>XLarge model benchmarks</li> <li>LLM model benchmarks</li> <li>Ensemble results</li> <li>SOTA benchmarks</li> <li>Efficiency benchmarks</li> <li>Parameter efficiency comparison</li> <li>Memory usage profiling</li> <li>Training time measurements</li> <li>Inference speed testing</li> <li>Platform comparison</li> <li>Robustness benchmarks</li> <li>Adversarial robustness results</li> <li>Out-of-distribution detection</li> <li>Contrast set evaluation</li> <li>Overfitting benchmarks</li> <li>Train-validation gap analysis</li> <li>LoRA rank impact</li> <li>Prevention effectiveness</li> </ul>"},{"location":"CHANGELOG/#statistical-analysis","title":"Statistical Analysis","text":"<ul> <li>Significance testing (t-tests, Mann-Whitney U)</li> <li>Confidence interval computation</li> <li>Effect size calculation (Cohen's d)</li> <li>Multiple comparison correction (Bonferroni, Holm)</li> <li>Bootstrap resampling</li> <li>Cross-validation analysis</li> <li>Ablation study statistics</li> </ul>"},{"location":"CHANGELOG/#visualization","title":"Visualization","text":"<ul> <li>Training curves with smoothing</li> <li>Confusion matrix heatmaps</li> <li>ROC and PR curves</li> <li>Calibration plots</li> <li>Attention visualization</li> <li>Embedding projections (t-SNE, UMAP)</li> <li>LoRA weight distributions</li> <li>Ensemble diversity plots</li> <li>Performance comparison charts</li> </ul>"},{"location":"CHANGELOG/#profiling","title":"Profiling","text":"<ul> <li>Memory profiler tracking allocation</li> <li>Speed profiler identifying bottlenecks</li> <li>GPU profiler using NVIDIA tools</li> <li>Parameter counter for model analysis</li> <li>Local profiler for deployment testing</li> </ul>"},{"location":"CHANGELOG/#debugging-tools","title":"Debugging Tools","text":"<ul> <li>Model debugger for architecture inspection</li> <li>Overfitting debugger analyzing prevention</li> <li>LoRA debugger for adapter analysis</li> <li>Data validator ensuring quality</li> <li>Platform debugger for environment issues</li> <li>Quota debugger for resource tracking</li> <li>Local debugger for deployment issues</li> </ul>"},{"location":"CHANGELOG/#security-features","title":"Security Features","text":""},{"location":"CHANGELOG/#input-security","title":"Input Security","text":"<ul> <li>Input validation for all endpoints</li> <li>Request sanitization preventing injection</li> <li>File upload validation</li> <li>Size limits on inputs</li> <li>Type checking and schema validation</li> </ul>"},{"location":"CHANGELOG/#authentication","title":"Authentication","text":"<ul> <li>Token-based authentication with JWT</li> <li>API key management</li> <li>Local RBAC for role-based access</li> <li>Session management with expiration</li> </ul>"},{"location":"CHANGELOG/#data-privacy","title":"Data Privacy","text":"<ul> <li>PII detection in text data</li> <li>Data masking utilities</li> <li>Anonymization for logging</li> <li>Secure secret storage</li> <li>Environment variable management</li> </ul>"},{"location":"CHANGELOG/#model-security","title":"Model Security","text":"<ul> <li>Adversarial defense mechanisms</li> <li>Model checksum verification</li> <li>Input perturbation detection</li> <li>Output confidence filtering</li> <li>Rate limiting per user</li> </ul>"},{"location":"CHANGELOG/#development-tools","title":"Development Tools","text":""},{"location":"CHANGELOG/#automation","title":"Automation","text":"<ul> <li>Health check runner for validation</li> <li>Auto-fix runner for common issues</li> <li>Batch configuration generator</li> <li>Platform health monitoring</li> <li>Nightly tasks automation</li> </ul>"},{"location":"CHANGELOG/#cli-helpers","title":"CLI Helpers","text":"<ul> <li>Rich console for formatting</li> <li>Progress bars with rich</li> <li>Interactive prompts with questionary</li> <li>ASCII art for branding</li> <li>Colored output for readability</li> </ul>"},{"location":"CHANGELOG/#compatibility-tools","title":"Compatibility Tools","text":"<ul> <li>Compatibility checker for versions</li> <li>Version matrix tester</li> <li>Upgrade path finder</li> <li>Dependency conflict resolver</li> </ul>"},{"location":"CHANGELOG/#cost-tools","title":"Cost Tools","text":"<ul> <li>Cost estimator for cloud resources</li> <li>Cost comparator across platforms</li> <li>Free-tier optimization recommendations</li> </ul>"},{"location":"CHANGELOG/#performance-achievements","title":"Performance Achievements","text":""},{"location":"CHANGELOG/#accuracy-results-on-ag-news-dataset","title":"Accuracy Results on AG News Dataset","text":"<ul> <li>DeBERTa v3 base: 94.5% test accuracy (baseline)</li> <li>DeBERTa v3 large: 95.8% test accuracy</li> <li>DeBERTa v3 xlarge with LoRA (r=32): 96.7% test accuracy</li> <li>DeBERTa v2 xxlarge with QLoRA (4-bit): 97.1% test accuracy</li> <li>RoBERTa large with LoRA (r=16): 95.9% test accuracy</li> <li>ELECTRA large with LoRA (r=32): 95.7% test accuracy</li> <li>XLNet large with LoRA (r=32): 95.6% test accuracy</li> <li>LLaMA 2 7B with QLoRA (r=64): 95.4% test accuracy</li> <li>LLaMA 2 13B with QLoRA (r=64): 96.2% test accuracy</li> <li>Mistral 7B with QLoRA (r=64): 95.8% test accuracy</li> <li>Ensemble (5 XLarge models, soft voting): 97.8% test accuracy</li> <li>Ensemble (5 XLarge models, stacking): 97.9% test accuracy</li> <li>Ultimate SOTA (Ensemble + Knowledge Distillation): 98.0% test accuracy</li> </ul>"},{"location":"CHANGELOG/#parameter-efficiency","title":"Parameter Efficiency","text":"<ul> <li>LoRA reduces trainable parameters by 99% (from 900M to 9M for DeBERTa-xlarge with r=32)</li> <li>QLoRA enables 70B parameter models on 40GB GPU (vs 280GB required for full precision)</li> <li>Adapter methods: 0.5-2% trainable parameters of full model</li> <li>Prefix tuning: 0.1-0.5% trainable parameters</li> <li>Prompt tuning: under 0.1% trainable parameters</li> <li>IA3: under 0.01% trainable parameters</li> </ul>"},{"location":"CHANGELOG/#training-efficiency","title":"Training Efficiency","text":"<ul> <li>Mixed precision training: 2x speedup with FP16, 1.8x with BF16</li> <li>Gradient checkpointing: 40-50% memory reduction with 20% time overhead</li> <li>Gradient accumulation: enables effective batch size 256 on 16GB GPU</li> <li>LoRA training: 3x faster than full fine-tuning</li> <li>QLoRA training: enables 13B models on consumer GPUs (24GB VRAM)</li> </ul>"},{"location":"CHANGELOG/#inference-performance","title":"Inference Performance","text":"<ul> <li>Single model latency: 10-50ms on GPU, 50-200ms on CPU</li> <li>Ensemble latency: 50-200ms on GPU (parallel execution)</li> <li>Batch inference throughput: 100-500 samples/sec (CPU), 500-2000 samples/sec (GPU)</li> <li>ONNX optimized inference: 2-3x speedup over PyTorch</li> <li>INT8 quantized inference: 4x speedup with minimal accuracy loss (under 0.5%)</li> <li>Model loading time: under 5 seconds for LoRA adapters</li> </ul>"},{"location":"CHANGELOG/#resource-requirements","title":"Resource Requirements","text":"<ul> <li>Minimum for inference only: 4GB RAM, 2 CPU cores</li> <li>Recommended for development: 16GB RAM, 4 CPU cores, 8GB GPU</li> <li>SOTA training: 32GB RAM, 8 CPU cores, 24GB GPU</li> <li>Free tier compatibility: Colab (T4 12GB), Kaggle (P100 16GB)</li> </ul>"},{"location":"CHANGELOG/#generalization-performance","title":"Generalization Performance","text":"<ul> <li>Train-validation gap: under 0.5% for properly regularized models</li> <li>Cross-validation standard deviation: under 0.3% across 5 folds</li> <li>Performance on contrast sets: 90%+ accuracy (robustness test)</li> <li>Calibration error (ECE): under 5% for ensemble models</li> <li>Out-of-distribution detection AUROC: 85%+ using confidence thresholding</li> </ul>"},{"location":"CHANGELOG/#documentation-statistics","title":"Documentation Statistics","text":""},{"location":"CHANGELOG/#documentation-files","title":"Documentation Files","text":"<ul> <li>15 top-level documentation files</li> <li>100+ markdown documentation pages</li> <li>50+ tutorial notebooks</li> <li>300+ YAML configuration files</li> <li>1000+ docstrings in code</li> <li>API reference for all modules</li> <li>Comprehensive README files in each directory</li> </ul>"},{"location":"CHANGELOG/#code-statistics","title":"Code Statistics","text":"<ul> <li>50,000+ lines of Python code</li> <li>700+ files in project structure</li> <li>200+ unit tests</li> <li>100+ integration tests</li> <li>Type hints on 90%+ of functions</li> <li>Docstring coverage: 95%+</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":""},{"location":"CHANGELOG/#initial-release-fixes","title":"Initial Release Fixes","text":"<ul> <li>None (initial release baseline)</li> </ul>"},{"location":"CHANGELOG/#security","title":"Security","text":""},{"location":"CHANGELOG/#security-measures-implemented","title":"Security Measures Implemented","text":"<ul> <li>Input validation on all API endpoints preventing injection attacks</li> <li>Rate limiting (100 requests per minute per IP) preventing abuse</li> <li>Token-based authentication with JWT and configurable expiration</li> <li>CORS configuration restricting origins</li> <li>Secrets management using environment variables and templates</li> <li>PII detection and masking in data processing</li> <li>Model checksum verification ensuring integrity</li> <li>Dependency vulnerability scanning with safety and bandit</li> <li>SQL injection prevention through parameterized queries</li> <li>XSS protection through output encoding</li> <li>Secure headers configuration (HSTS, CSP, X-Frame-Options)</li> <li>File upload size limits (10MB default)</li> <li>Request timeout enforcement (30 seconds default)</li> <li>Logging of security events for auditing</li> </ul>"},{"location":"CHANGELOG/#known-issues-and-limitations","title":"Known Issues and Limitations","text":""},{"location":"CHANGELOG/#platform-limitations","title":"Platform Limitations","text":"<ul> <li>Flash Attention 2 only supported on Linux with CUDA 11.8+</li> <li>DeepSpeed not available on Windows platforms</li> <li>Some packages incompatible with Python 3.12 (maximum 3.11)</li> <li>ROCm (AMD GPU) support experimental and not fully tested</li> <li>Apple Silicon (M1/M2) support limited for some quantization features</li> <li>Google Colab free tier has session timeout (12 hours)</li> <li>Kaggle Kernels limited to 9 hours per session</li> <li>HuggingFace Spaces free tier CPU-only with 16GB RAM limit</li> </ul>"},{"location":"CHANGELOG/#model-limitations","title":"Model Limitations","text":"<ul> <li>Maximum sequence length: 512 tokens (standard models), 4096 tokens (Longformer)</li> <li>LLM inference requires significant memory (7B model minimum 4GB VRAM with QLoRA)</li> <li>Very large ensembles (10+ models) have slow inference (over 500ms latency)</li> <li>Quantization may cause 0.5-2% accuracy degradation</li> <li>Some models not compatible with ONNX export (e.g., Mixtral MoE)</li> </ul>"},{"location":"CHANGELOG/#data-limitations","title":"Data Limitations","text":"<ul> <li>AG News dataset limited to English language</li> <li>Four class categories (World, Sports, Business, Technology)</li> <li>Training set: 120,000 samples</li> <li>Test set: 7,600 samples</li> <li>No validation set provided (requires manual split)</li> <li>Text truncation needed for documents over 512 tokens</li> </ul>"},{"location":"CHANGELOG/#known-bugs","title":"Known Bugs","text":"<ul> <li>None identified in this release</li> </ul>"},{"location":"CHANGELOG/#future-improvements","title":"Future Improvements","text":"<ul> <li>Support for additional languages beyond English</li> <li>Integration with more experiment tracking platforms</li> <li>Enhanced AutoML capabilities</li> <li>Mobile deployment tools (TFLite, Core ML)</li> <li>Real-time learning pipelines</li> <li>Federated learning support</li> </ul>"},{"location":"CHANGELOG/#dependencies","title":"Dependencies","text":""},{"location":"CHANGELOG/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>Python: 3.8, 3.9, 3.10, or 3.11</li> <li>PyTorch: 2.1.0 to 2.2.x</li> <li>Transformers: 4.36.0 to 4.40.x</li> <li>Tokenizers: 0.15.0 to 0.15.x</li> <li>Datasets: 2.16.0 to 2.19.x</li> <li>Accelerate: 0.25.0 to 0.30.x</li> <li>PEFT: 0.7.0 to 0.11.x</li> </ul>"},{"location":"CHANGELOG/#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li>CUDA: 11.8 or 12.1 (for GPU training)</li> <li>cuDNN: 8.x (for GPU optimization)</li> <li>Flash Attention: 2.4.0+ (Linux only, for faster attention)</li> <li>DeepSpeed: 0.12.0+ (Linux only, for advanced training)</li> <li>bitsandbytes: 0.41.0+ (for quantization)</li> <li>ONNX: 1.15.0+ (for model export)</li> <li>TensorRT: 8.x (for NVIDIA inference optimization)</li> <li>OpenVINO: 2023.x (for Intel optimization)</li> </ul>"},{"location":"CHANGELOG/#development-dependencies","title":"Development Dependencies","text":"<ul> <li>pytest: 7.4.0+</li> <li>black: 23.12.0+</li> <li>mypy: 1.8.0+</li> <li>flake8: 7.0.0+</li> <li>pre-commit: 3.6.0+</li> </ul>"},{"location":"CHANGELOG/#total-dependencies-by-profile","title":"Total Dependencies by Profile","text":"<ul> <li>Base installation: 50+ packages</li> <li>ML profile: 150+ packages</li> <li>LLM profile: 200+ packages</li> <li>All dependencies: 400+ packages</li> </ul>"},{"location":"CHANGELOG/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>None (initial release)</li> </ul>"},{"location":"CHANGELOG/#deprecations","title":"Deprecations","text":"<ul> <li>None (initial release)</li> </ul>"},{"location":"CHANGELOG/#migration-guide","title":"Migration Guide","text":"<ul> <li>Not applicable for initial release</li> </ul>"},{"location":"CHANGELOG/#version-numbering-scheme","title":"Version Numbering Scheme","text":"<p>This project follows Semantic Versioning 2.0.0 (https://semver.org/):</p> <ul> <li>MAJOR version (X.0.0): Incompatible API changes breaking backward compatibility</li> <li>MINOR version (0.X.0): New functionality added in backward-compatible manner</li> <li>PATCH version (0.0.X): Backward-compatible bug fixes and minor improvements</li> </ul>"},{"location":"CHANGELOG/#pre-release-version-suffixes","title":"Pre-release Version Suffixes","text":"<ul> <li>Alpha (X.Y.Z-alpha.N): Early development stage, unstable, not feature-complete</li> <li>Beta (X.Y.Z-beta.N): Feature-complete, undergoing testing, may have bugs</li> <li>Release Candidate (X.Y.Z-rc.N): Final testing before release, minimal changes expected</li> </ul>"},{"location":"CHANGELOG/#version-increment-guidelines","title":"Version Increment Guidelines","text":"<ul> <li>Increment MAJOR when making incompatible API changes</li> <li>Increment MINOR when adding backward-compatible functionality</li> <li>Increment PATCH when making backward-compatible bug fixes</li> <li>Use pre-release suffixes for development versions</li> </ul>"},{"location":"CHANGELOG/#changelog-categories","title":"Changelog Categories","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<p>New features, capabilities, or documentation added to the project.</p>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<p>Changes to existing functionality, behavior, or documentation.</p>"},{"location":"CHANGELOG/#deprecated","title":"Deprecated","text":"<p>Features marked for removal in future versions with migration path provided.</p>"},{"location":"CHANGELOG/#removed","title":"Removed","text":"<p>Features or capabilities removed from the project.</p>"},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":"<p>Bug fixes, error corrections, or improvements to existing functionality.</p>"},{"location":"CHANGELOG/#security_1","title":"Security","text":"<p>Security-related changes, vulnerability fixes, or security improvements.</p>"},{"location":"CHANGELOG/#contributing-to-this-changelog","title":"Contributing to This Changelog","text":"<p>When making changes to the project, contributors should:</p> <ol> <li>Add entries to the [Unreleased] section under appropriate category</li> <li>Use clear, concise descriptions explaining the change and its impact</li> <li>Reference issue numbers using #issue_number format when applicable</li> <li>Follow conventional commit message format for consistency</li> <li>Update changelog with each significant commit or pull request</li> <li>Ensure changes are documented before release</li> </ol>"},{"location":"CHANGELOG/#release-process","title":"Release Process","text":"<p>Standard release workflow:</p> <ol> <li>Update version number in src/version.py</li> <li>Move [Unreleased] changes to new version section with release date</li> <li>Add comprehensive release notes summarizing changes</li> <li>Update comparison links at bottom of file</li> <li>Commit changelog with message \"chore: update changelog for vX.Y.Z\"</li> <li>Create annotated git tag: git tag -a vX.Y.Z -m \"Release vX.Y.Z\"</li> <li>Push commits and tags: git push origin main --tags</li> <li>Create GitHub release with changelog excerpt</li> <li>Build and upload package to PyPI</li> <li>Update documentation with new version</li> </ol>"},{"location":"CHANGELOG/#links-and-references","title":"Links and References","text":"<ul> <li>Repository: https://github.com/VoHaiDung/ag-news-text-classification</li> <li>Documentation: https://github.com/VoHaiDung/ag-news-text-classification#readme</li> <li>Issue Tracker: https://github.com/VoHaiDung/ag-news-text-classification/issues</li> <li>Discussions: https://github.com/VoHaiDung/ag-news-text-classification/discussions</li> <li>Keep a Changelog: https://keepachangelog.com/en/1.0.0/</li> <li>Semantic Versioning: https://semver.org/spec/v2.0.0.html</li> <li>Conventional Commits: https://www.conventionalcommits.org/</li> </ul>"},{"location":"CHANGELOG/#acknowledgments","title":"Acknowledgments","text":"<p>This project builds upon foundational research and open-source contributions:</p> <ul> <li>HuggingFace Transformers library for transformer model implementations</li> <li>PyTorch framework for deep learning infrastructure</li> <li>AG News dataset (Zhang et al., 2015) \"Character-level Convolutional Networks for Text Classification\"</li> <li>LoRA (Hu et al., 2021) \"LoRA: Low-Rank Adaptation of Large Language Models\"</li> <li>QLoRA (Dettmers et al., 2023) \"QLoRA: Efficient Finetuning of Quantized LLMs\"</li> <li>DeBERTa (He et al., 2020) \"DeBERTa: Decoding-enhanced BERT with Disentangled Attention\"</li> <li>DeBERTa v3 (He et al., 2021) \"DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training\"</li> <li>LLaMA (Touvron et al., 2023) \"LLaMA: Open and Efficient Foundation Language Models\"</li> <li>LLaMA 2 (Touvron et al., 2023) \"LLaMA 2: Open Foundation and Fine-Tuned Chat Models\"</li> <li>Mistral (Jiang et al., 2023) \"Mistral 7B\"</li> <li>RoBERTa (Liu et al., 2019) \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"</li> <li>ELECTRA (Clark et al., 2020) \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\"</li> </ul>"},{"location":"CHANGELOG/#citation","title":"Citation","text":"<p>If you use this project in your research or applications, please cite:</p> BibTeX<pre><code>@software{vo2025agnews,\n  author = {V\u00f5 H\u1ea3i D\u0169ng},\n  title = {AG News Text Classification: A Comprehensive Framework with Overfitting Prevention},\n  year = {2025},\n  url = {https://github.com/VoHaiDung/ag-news-text-classification},\n  version = {1.0.0},\n  license = {MIT}\n}\n</code></pre> <p>For specific components or techniques, please also cite the original research papers.</p>"},{"location":"CHANGELOG/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for complete details.</p> <p>The MIT License grants permission to use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the software, subject to including the copyright notice and permission notice in all copies or substantial portions.</p> <p>Maintained by: V\u00f5 H\u1ea3i D\u0169ng Email: vohaidung.work@gmail.com Last Updated: 2025-09-19  </p>"},{"location":"FREE_DEPLOYMENT_GUIDE/","title":"FREE_DEPLOYMENT_GUIDE","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"FREE_DEPLOYMENT_GUIDE/#overview","title":"Overview","text":"<p>Comprehensive guide for FREE_DEPLOYMENT_GUIDE.</p>"},{"location":"FREE_DEPLOYMENT_GUIDE/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"FREE_DEPLOYMENT_GUIDE/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"FREE_DEPLOYMENT_GUIDE/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"HEALTH_CHECK/","title":"HEALTH_CHECK","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"HEALTH_CHECK/#overview","title":"Overview","text":"<p>Comprehensive guide for HEALTH_CHECK.</p>"},{"location":"HEALTH_CHECK/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"HEALTH_CHECK/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"HEALTH_CHECK/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"IDE_SETUP_GUIDE/","title":"IDE_SETUP_GUIDE","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"IDE_SETUP_GUIDE/#overview","title":"Overview","text":"<p>Comprehensive guide for IDE_SETUP_GUIDE.</p>"},{"location":"IDE_SETUP_GUIDE/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"IDE_SETUP_GUIDE/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"IDE_SETUP_GUIDE/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"LOCAL_MONITORING_GUIDE/","title":"LOCAL_MONITORING_GUIDE","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"LOCAL_MONITORING_GUIDE/#overview","title":"Overview","text":"<p>Comprehensive guide for LOCAL_MONITORING_GUIDE.</p>"},{"location":"LOCAL_MONITORING_GUIDE/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"LOCAL_MONITORING_GUIDE/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"LOCAL_MONITORING_GUIDE/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"OVERFITTING_PREVENTION/","title":"OVERFITTING_PREVENTION","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"OVERFITTING_PREVENTION/#overview","title":"Overview","text":"<p>Comprehensive guide for OVERFITTING_PREVENTION.</p>"},{"location":"OVERFITTING_PREVENTION/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"OVERFITTING_PREVENTION/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"OVERFITTING_PREVENTION/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"PERFORMANCE/","title":"PERFORMANCE","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"PERFORMANCE/#overview","title":"Overview","text":"<p>Comprehensive guide for PERFORMANCE.</p>"},{"location":"PERFORMANCE/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"PERFORMANCE/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"PERFORMANCE/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"PLATFORM_OPTIMIZATION_GUIDE/","title":"PLATFORM_OPTIMIZATION_GUIDE","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"PLATFORM_OPTIMIZATION_GUIDE/#overview","title":"Overview","text":"<p>Comprehensive guide for PLATFORM_OPTIMIZATION_GUIDE.</p>"},{"location":"PLATFORM_OPTIMIZATION_GUIDE/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"PLATFORM_OPTIMIZATION_GUIDE/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"PLATFORM_OPTIMIZATION_GUIDE/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"QUICK_START/","title":"QUICK_START","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"QUICK_START/#overview","title":"Overview","text":"<p>Comprehensive guide for QUICK_START.</p>"},{"location":"QUICK_START/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"QUICK_START/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"QUICK_START/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"ROADMAP/","title":"ROADMAP","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"ROADMAP/#overview","title":"Overview","text":"<p>Comprehensive guide for ROADMAP.</p>"},{"location":"ROADMAP/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"ROADMAP/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"ROADMAP/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"SECURITY/","title":"SECURITY","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"SECURITY/#overview","title":"Overview","text":"<p>Comprehensive guide for SECURITY.</p>"},{"location":"SECURITY/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"SECURITY/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"SECURITY/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"SOTA_MODELS_GUIDE/","title":"SOTA_MODELS_GUIDE","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"SOTA_MODELS_GUIDE/#overview","title":"Overview","text":"<p>Comprehensive guide for SOTA_MODELS_GUIDE.</p>"},{"location":"SOTA_MODELS_GUIDE/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"SOTA_MODELS_GUIDE/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"SOTA_MODELS_GUIDE/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"TROUBLESHOOTING/","title":"TROUBLESHOOTING","text":"<p>This documentation is under development for AG News Text Classification.</p>"},{"location":"TROUBLESHOOTING/#overview","title":"Overview","text":"<p>Comprehensive guide for TROUBLESHOOTING.</p>"},{"location":"TROUBLESHOOTING/#contents","title":"Contents","text":"<p>Documentation content will be added here.</p>"},{"location":"TROUBLESHOOTING/#author","title":"Author","text":"<p>V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"TROUBLESHOOTING/#contact","title":"Contact","text":"<p>For questions or support, contact: vohaidung.work@gmail.com</p>"},{"location":"getting_started/choosing_platform/","title":"Platform Selection Guide","text":"<p>Choose the best platform for your needs in AG News Text Classification.</p>"},{"location":"getting_started/choosing_platform/#available-platforms","title":"Available Platforms","text":""},{"location":"getting_started/choosing_platform/#google-colab","title":"Google Colab","text":"<ul> <li>Free GPU access (T4)</li> <li>Easy to start</li> <li>Session limits (12 hours)</li> <li>Recommended for: Quick experiments, learning</li> </ul>"},{"location":"getting_started/choosing_platform/#kaggle","title":"Kaggle","text":"<ul> <li>Free GPU/TPU access</li> <li>Longer sessions (9-12 hours)</li> <li>Dataset integration</li> <li>Recommended for: Training, competitions</li> </ul>"},{"location":"getting_started/choosing_platform/#local","title":"Local","text":"<ul> <li>Full control</li> <li>No time limits</li> <li>Hardware dependent</li> <li>Recommended for: Development, production</li> </ul>"},{"location":"getting_started/choosing_platform/#platform-comparison","title":"Platform Comparison","text":"<p>See PLATFORM_OPTIMIZATION_GUIDE.md for detailed comparison.</p> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"getting_started/free_deployment/","title":"Free Deployment Guide","text":"<p>Deploy AG News Text Classification without monthly costs.</p>"},{"location":"getting_started/free_deployment/#deployment-options","title":"Deployment Options","text":""},{"location":"getting_started/free_deployment/#hugging-face-spaces","title":"Hugging Face Spaces","text":"<ul> <li>Free hosting</li> <li>Automatic deployment</li> <li>Community visibility</li> </ul>"},{"location":"getting_started/free_deployment/#streamlit-cloud","title":"Streamlit Cloud","text":"<ul> <li>Free tier available</li> <li>Easy deployment</li> <li>Streamlit integration</li> </ul>"},{"location":"getting_started/free_deployment/#local-deployment","title":"Local Deployment","text":"<ul> <li>No cost</li> <li>Full control</li> <li>Self-hosted</li> </ul>"},{"location":"getting_started/free_deployment/#setup-instructions","title":"Setup Instructions","text":"<p>See FREE_DEPLOYMENT_GUIDE.md for detailed instructions.</p> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"getting_started/installation/","title":"Installation Guide","text":"<p>Comprehensive installation instructions for AG News Text Classification.</p>"},{"location":"getting_started/installation/#quick-installation","title":"Quick Installation","text":"Bash<pre><code>git clone https://github.com/VoHaiDung/ag-news-text-classification.git\ncd ag-news-text-classification\npip install -r requirements/base.txt\n</code></pre>"},{"location":"getting_started/installation/#platform-specific-installation","title":"Platform-Specific Installation","text":""},{"location":"getting_started/installation/#google-colab","title":"Google Colab","text":"<p>See Colab Guide for setup instructions.</p>"},{"location":"getting_started/installation/#kaggle","title":"Kaggle","text":"<p>See Kaggle Guide for setup instructions.</p>"},{"location":"getting_started/installation/#local-setup","title":"Local Setup","text":"<p>See Local Guide for detailed local installation.</p>"},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>PyTorch 2.0+</li> <li>Transformers 4.30+</li> </ul>"},{"location":"getting_started/installation/#verification","title":"Verification","text":"Python<pre><code>python scripts/setup/verify_installation.py\n</code></pre> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"ide_guides/jupyter_guide/","title":"Jupyter Setup Guide","text":"<p>Setup Jupyter for AG News Text Classification development.</p> <p>See IDE_SETUP_GUIDE.md for details.</p> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"ide_guides/pycharm_guide/","title":"PyCharm Setup Guide","text":"<p>Setup PyCharm for AG News Text Classification development.</p> <p>See IDE_SETUP_GUIDE.md for details.</p> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"ide_guides/vscode_guide/","title":"VS Code Setup Guide","text":"<p>Setup Visual Studio Code for AG News Text Classification development.</p> <p>See IDE_SETUP_GUIDE.md for details.</p> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"platform_guides/colab_guide/","title":"Google Colab Guide","text":"<p>Setup and usage guide for Google Colab platform.</p> <p>See PLATFORM_OPTIMIZATION_GUIDE.md for details.</p> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"platform_guides/kaggle_guide/","title":"Kaggle Guide","text":"<p>Setup and usage guide for Kaggle platform.</p> <p>See PLATFORM_OPTIMIZATION_GUIDE.md for details.</p> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"platform_guides/local_guide/","title":"Local Setup Guide","text":"<p>Setup and usage guide for local development.</p> <p>See PLATFORM_OPTIMIZATION_GUIDE.md for details.</p> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"user_guide/data_preparation/","title":"Data Preparation Guide","text":"<p>Prepare data for training and evaluation in AG News Text Classification.</p>"},{"location":"user_guide/data_preparation/#data-loading","title":"Data Loading","text":"Python<pre><code>from src.data.datasets.ag_news import AGNewsDataset\n\ndataset = AGNewsDataset()\ntrain_data, val_data, test_data = dataset.load_splits()\n</code></pre>"},{"location":"user_guide/data_preparation/#preprocessing","title":"Preprocessing","text":"Python<pre><code>from src.data.preprocessing.text_cleaner import TextCleaner\n\ncleaner = TextCleaner()\ncleaned_texts = cleaner.clean(texts)\n</code></pre>"},{"location":"user_guide/data_preparation/#data-augmentation","title":"Data Augmentation","text":"Python<pre><code>from src.data.augmentation.back_translation import BackTranslator\n\naugmenter = BackTranslator()\naugmented_data = augmenter.augment(train_data)\n</code></pre> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"user_guide/evaluation/","title":"Model Evaluation Guide","text":"<p>Evaluate trained models on AG News dataset.</p>"},{"location":"user_guide/evaluation/#basic-evaluation","title":"Basic Evaluation","text":"Python<pre><code>python scripts/evaluation/evaluate_all_models.py\n</code></pre>"},{"location":"user_guide/evaluation/#metrics","title":"Metrics","text":"<ul> <li>Accuracy</li> <li>F1 Score</li> <li>Precision</li> <li>Recall</li> </ul>"},{"location":"user_guide/evaluation/#overfitting-check","title":"Overfitting Check","text":"Python<pre><code>python scripts/overfitting_prevention/check_data_leakage.py\n</code></pre> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"},{"location":"user_guide/model_training/","title":"Model Training Guide","text":"<p>Train models on AG News dataset.</p>"},{"location":"user_guide/model_training/#basic-training","title":"Basic Training","text":"Python<pre><code>python scripts/training/train_single_model.py \\\n  --config configs/models/recommended/tier_1_sota/deberta_v3_xlarge_lora.yaml\n</code></pre>"},{"location":"user_guide/model_training/#advanced-training","title":"Advanced Training","text":""},{"location":"user_guide/model_training/#with-lora","title":"With LoRA","text":"Python<pre><code>python scripts/training/single_model/train_xlarge_lora.py\n</code></pre>"},{"location":"user_guide/model_training/#with-qlora","title":"With QLoRA","text":"Python<pre><code>python scripts/training/single_model/train_xxlarge_qlora.py\n</code></pre>"},{"location":"user_guide/model_training/#monitoring","title":"Monitoring","text":"Bash<pre><code>tensorboard --logdir outputs/logs/tensorboard\n</code></pre> <p>Author: V\u00f5 H\u1ea3i D\u0169ng</p>"}]}